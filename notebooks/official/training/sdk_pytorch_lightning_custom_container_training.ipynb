{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2022 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Vertex AI Training with PyTorch Lightning\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/training/pytorch_lightning_custom_container_training.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/training/pytorch_lightning_custom_container_training.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/training/pytorch_lightning_custom_container_training.ipynb\" target='_blank'>\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "     </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f879013c4231"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This tutorial demonstrates how to use the Vertex AI SDK for Python to train a ResNet model using custom containers and PyTorch Lightning. The model training code is from the CIFAR-10 training example on PyTorch Lightning's documentation page: \n",
        "https://pytorch-lightning.readthedocs.io/en/stable/notebooks/lightning_examples/cifar10-baseline.html\n",
        "\n",
        "Two training approaches are taken: 1) Multiple GPU training on a single machine 2) Multiple machine training wtih a single GPU on each machine.\n",
        "\n",
        "Learn more about [Custom Training Overview](https://cloud.google.com/vertex-ai/docs/training/overview)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32202bef4592"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this notebook, you learn how to take an existing example of a model trained using PyTorch Lighting, and use Vertex AI to distribute training across GPUs and multiple machines.\n",
        "\n",
        "This tutorial uses the following Google Cloud ML services:\n",
        "\n",
        "- `Vertex AI Training`\n",
        "- `Vertex AI TensorBoard`\n",
        "\n",
        "The steps performed include:\n",
        "\n",
        "    * Install and import libraries to test model training locally\n",
        "    * Initialize the Vertex AI SDK\n",
        "    * Create a custom container for training\n",
        "    * Create a Vertex AI TensorBoard\n",
        "    * Modify the code for pass in arguments, log to the TensorBoard, and save the model to Cloud Storage\n",
        "    * Run a Vertex AI training job on a single machine with GPUs\n",
        "    * Run a Vertex AI training job on multiple machines with single GPUs attached"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "684bab680572"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "Here's the description from the website: The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "The dataset will be loaded using the Lightning Bolts datamodules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "### Costs \n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* Vertex AI\n",
        "* Cloud Storage\n",
        "\n",
        "Learn about [Vertex AI\n",
        "pricing](https://cloud.google.com/vertex-ai/pricing) and [Cloud Storage\n",
        "pricing](https://cloud.google.com/storage/pricing), and use the [Pricing\n",
        "Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "### Install additional packages\n",
        "\n",
        "Install additional package dependencies not installed in your notebook environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyy5Lbnzg5fi"
      },
      "outputs": [],
      "source": [
        "USER_FLAG = \"\"\n",
        "\n",
        "! pip3 install {USER_FLAG} --upgrade \"torch>=1.6, <1.9\"\n",
        "! pip3 install {USER_FLAG} --upgrade \"lightning-bolts\"\n",
        "! pip3 install {USER_FLAG} --upgrade git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "! pip3 install {USER_FLAG} --upgrade \"torchmetrics>=0.3\"\n",
        "! pip3 install {USER_FLAG} --upgrade \"torchvision\"\n",
        "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform\n",
        "! pip3 install {USER_FLAG} --upgrade ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58707a750154"
      },
      "source": [
        "### Colab only: Uncomment the following cell to restart the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f200f10a1da3"
      },
      "outputs": [],
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "**If you don't know your project ID**, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Set the region\n",
        "\n",
        "**Optional**: Update the 'REGION' variable to specify the region that you want to use. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsN5NJKSu-GU"
      },
      "outputs": [],
      "source": [
        "REGION = \"us-central1\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b45b2839f8b9"
      },
      "source": [
        "#### UUID\n",
        "\n",
        "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a uuid for each instance session, and append the uuid onto the name of resources you create in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e80050370d51"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "\n",
        "# Generate a uuid of a specifed length(default=8)\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "\n",
        "UUID = generate_uuid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "To authenticate your Google Cloud account, follow the instructions for your Jupyter environment:\n",
        "\n",
        "**1. Vertex AI Workbench**\n",
        "<br>You are already authenticated.\n",
        "\n",
        "**2. Local JupyterLab instance**\n",
        "<br>Uncomment and run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**3. Colab**\n",
        "<br>Uncomment and run the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13224697bfb"
      },
      "source": [
        "**4. Service account or other**\n",
        "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgPO1eR3CYjk"
      },
      "source": [
        "### Create a Cloud Storage bucket\n",
        "\n",
        "Create a storage bucket to store intermediate artifacts such as datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGDU7TWdts_"
      },
      "outputs": [],
      "source": [
        "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # noqa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_bucket"
      },
      "source": [
        "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIq7R4HZCfIc"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION -p $PROJECT_ID $BUCKET_URI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Import libraries and define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRUOFELefqf1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from pl_bolts.datamodules import CIFAR10DataModule\n",
        "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torchmetrics.functional import accuracy\n",
        "\n",
        "seed_everything(7)\n",
        "\n",
        "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
        "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
        "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
        "NUM_WORKERS = int(os.cpu_count() / 2)\n",
        "\n",
        "print(PATH_DATASETS)\n",
        "print(AVAIL_GPUS)\n",
        "print(BATCH_SIZE)\n",
        "print(NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoEqT2Y4DJmf"
      },
      "source": [
        "### Define training functions for local testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bca47ea0e3c3"
      },
      "outputs": [],
      "source": [
        "train_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.RandomCrop(32, padding=4),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        cifar10_normalization(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        cifar10_normalization(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cifar10_dm = CIFAR10DataModule(\n",
        "    data_dir=PATH_DATASETS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    train_transforms=train_transforms,\n",
        "    test_transforms=test_transforms,\n",
        "    val_transforms=test_transforms,\n",
        ")\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
        "    model.conv1 = nn.Conv2d(\n",
        "        3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
        "    )\n",
        "    model.maxpool = nn.Identity()\n",
        "    return model\n",
        "\n",
        "\n",
        "class LitResnet(LightningModule):\n",
        "    def __init__(self, lr=0.05):\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "        self.model = create_model()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, batch, stage=None):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy(preds, y)\n",
        "\n",
        "        if stage:\n",
        "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
        "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.evaluate(batch, \"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self.evaluate(batch, \"test\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=self.hparams.lr,\n",
        "            momentum=0.9,\n",
        "            weight_decay=5e-4,\n",
        "        )\n",
        "        steps_per_epoch = 45000 // BATCH_SIZE\n",
        "        scheduler_dict = {\n",
        "            \"scheduler\": OneCycleLR(\n",
        "                optimizer,\n",
        "                0.1,\n",
        "                epochs=self.trainer.max_epochs,\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "            ),\n",
        "            \"interval\": \"step\",\n",
        "        }\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71da4176a4cc"
      },
      "source": [
        "#### Train the model locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af65cc43993c"
      },
      "outputs": [],
      "source": [
        "model = LitResnet(lr=0.05)\n",
        "model.datamodule = cifar10_dm\n",
        "\n",
        "trainer = Trainer(\n",
        "    max_epochs=5,\n",
        "    gpus=AVAIL_GPUS,\n",
        "    logger=TensorBoardLogger(\"lightning_logs/\", name=\"resnet\"),\n",
        "    callbacks=[LearningRateMonitor(logging_interval=\"step\")],\n",
        "    strategy=\"dp\",\n",
        ")\n",
        "\n",
        "trainer.fit(model, cifar10_dm)\n",
        "trainer.test(model, datamodule=cifar10_dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eb55916219c"
      },
      "source": [
        "## Vertex AI Training using the Vertex AI SDK and a custom container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e61fc5c1b9e0"
      },
      "source": [
        "### Build the custom container"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be10c827e494"
      },
      "source": [
        "#### Run these steps once to setup artifact registry and authorize docker to use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "667b03930fc3"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project $PROJECT_ID\n",
        "! gcloud services enable artifactregistry.googleapis.com\n",
        "! sudo usermod -a -G docker ${USER}\n",
        "! gcloud auth configure-docker us-central1-docker.pkg.dev --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2647eb53957f"
      },
      "outputs": [],
      "source": [
        "REPOSITORY = \"gpu-training-repository\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37973b9d3ca3"
      },
      "outputs": [],
      "source": [
        "! gcloud artifacts repositories create $REPOSITORY --repository-format=docker \\\n",
        "--location=$REGION --description=\"Vertex GPU training repository\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2cfd36ceacd"
      },
      "source": [
        "#### Make a trainer directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05fa93664880"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.mkdir(\"trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27f1c07650b0"
      },
      "source": [
        "#### Build the container\n",
        "This code extends the original example and adds argument parsing, TensorBoard logging, ability to choose the training strategy, and model saving to Cloud Storage "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89f63d109973"
      },
      "outputs": [],
      "source": [
        "%%writefile trainer/task.py\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from pl_bolts.datamodules import CIFAR10DataModule\n",
        "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.optim.swa_utils import AveragedModel, update_bn\n",
        "from torchmetrics.functional import accuracy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from pl_bolts.datamodules import CIFAR10DataModule\n",
        "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
        "from pytorch_lightning import LightningModule, Trainer, seed_everything\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.optim.swa_utils import AveragedModel, update_bn\n",
        "from torchmetrics.functional import accuracy\n",
        "\n",
        "# Arg parsing and shutil for folder creation\n",
        "import argparse\n",
        "import shutil\n",
        "\n",
        "seed_everything(7)\n",
        "\n",
        "PATH_DATASETS = os.environ.get(\"PATH_DATASETS\", \".\")\n",
        "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
        "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
        "NUM_WORKERS = int(os.cpu_count() / 2)\n",
        "\n",
        "print (PATH_DATASETS)\n",
        "print (AVAIL_GPUS)\n",
        "print (BATCH_SIZE)\n",
        "print (NUM_WORKERS)\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.RandomCrop(32, padding=4),\n",
        "        torchvision.transforms.RandomHorizontalFlip(),\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        cifar10_normalization(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        cifar10_normalization(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "cifar10_dm = CIFAR10DataModule(\n",
        "    data_dir=PATH_DATASETS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    train_transforms=train_transforms,\n",
        "    test_transforms=test_transforms,\n",
        "    val_transforms=test_transforms,\n",
        ")\n",
        "\n",
        "# Added code to read args\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--epochs', dest='epochs',\n",
        "                        default=10, type=int,\n",
        "                        help='Number of epochs.')\n",
        "    parser.add_argument('--distribute', dest='distribute', type=str, default='dp',\n",
        "                        help='Distributed training strategy.')\n",
        "    parser.add_argument('--num-nodes', dest='num_nodes',\n",
        "                        default=1, type=int,\n",
        "                        help='Number of nodes')\n",
        "    parser.add_argument(\n",
        "          '--model-dir', dest='model_dir', default=os.getenv('AIP_MODEL_DIR'), type=str,\n",
        "          help='a Cloud Storage URI of a directory intended for saving model artifacts')\n",
        "    parser.add_argument(\n",
        "          '--tensorboard-log-dir', dest='tensorboard_log_dir', default=os.getenv('AIP_TENSORBOARD_LOG_DIR'), type=str,\n",
        "          help='a Cloud Storage URI of a directory intended for saving TensorBoard')\n",
        "    parser.add_argument(\n",
        "          '--checkpoint-dir', dest='checkpoint_dir', default=os.getenv('AIP_CHECKPOINT_DIR'), type=str,\n",
        "          help='a Cloud Storage URI of a directory intended for saving checkpoints')\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "# Cunction to make model directory if it doesn't exist\n",
        "def makedirs(model_dir):\n",
        "    if os.path.exists(model_dir) and os.path.isdir(model_dir):\n",
        "        shutil.rmtree(model_dir)\n",
        "    os.makedirs(model_dir)\n",
        "    return\n",
        "\n",
        "def create_model():\n",
        "    model = torchvision.models.resnet18(pretrained=False, num_classes=10)\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    return model\n",
        "\n",
        "class LitResnet(LightningModule):\n",
        "    def __init__(self, lr=0.05):\n",
        "        super().__init__()\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "        self.model = create_model()\n",
        "\n",
        "    # TensorBoard logging at epoch end\n",
        "    def training_epoch_end(self,outputs):\n",
        "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_logs = {'loss': avg_loss}\n",
        "\n",
        "        epoch_dictionary={'loss': avg_loss,'log': tensorboard_logs}\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return F.log_softmax(out, dim=1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, batch, stage=None):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = F.nll_loss(logits, y)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        acc = accuracy(preds, y)\n",
        "\n",
        "        if stage:\n",
        "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
        "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        self.evaluate(batch, \"val\")\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        self.evaluate(batch, \"test\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=self.hparams.lr,\n",
        "            momentum=0.9,\n",
        "            weight_decay=5e-4,\n",
        "        )\n",
        "        steps_per_epoch = 45000 // BATCH_SIZE\n",
        "        scheduler_dict = {\n",
        "            \"scheduler\": OneCycleLR(\n",
        "                optimizer,\n",
        "                0.1,\n",
        "                epochs=self.trainer.max_epochs,\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "            ),\n",
        "            \"interval\": \"step\",\n",
        "        }\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}\n",
        "\n",
        "def main():   \n",
        "\n",
        "    # Parse args\n",
        "    args = parse_args()\n",
        "    print (f\"Args={args}\")\n",
        "    print (f\"model directory={args.epochs}\")\n",
        "    print (f\"model directory={args.model_dir}\")\n",
        "    print (f\"distribute strategy={args.distribute}\")\n",
        "\n",
        "    # model, ensorboard, and checkpoint directories set\n",
        "    local_model_dir = './tmp/model'\n",
        "    local_tensorboard_log_dir = './tmp/logs'\n",
        "    local_checkpoint_dir = './tmp/checkpoints'\n",
        "\n",
        "    model_dir = args.model_dir or local_model_dir\n",
        "    tensorboard_log_dir = args.tensorboard_log_dir or local_tensorboard_log_dir\n",
        "    checkpoint_dir = args.checkpoint_dir or local_checkpoint_dir\n",
        "\n",
        "    print (\"Model directory\" + model_dir)\n",
        "    print (\"TensorBoard directory\" + tensorboard_log_dir)\n",
        "    print (\"Checkpoint directory\" + checkpoint_dir)\n",
        "\n",
        "    gs_prefix = 'gs://'\n",
        "    gcsfuse_prefix = '/gcs/'\n",
        "    if model_dir and model_dir.startswith(gs_prefix):\n",
        "        model_dir = model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
        "        if not os.path.isdir(model_dir):\n",
        "            os.makedirs(model_dir)\n",
        "    if tensorboard_log_dir and tensorboard_log_dir.startswith(gs_prefix):\n",
        "        tensorboard_log_dir = tensorboard_log_dir.replace(gs_prefix, gcsfuse_prefix)\n",
        "        if not os.path.isdir(tensorboard_log_dir):\n",
        "            os.makedirs(tensorboard_log_dir)\n",
        "    if checkpoint_dir and checkpoint_dir.startswith(gs_prefix):\n",
        "        checkpoint_dir = checkpoint_dir.replace(gs_prefix, gcsfuse_prefix)\n",
        "        if not os.path.isdir(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "\n",
        "    model = LitResnet(lr=0.05)\n",
        "    model.datamodule = cifar10_dm\n",
        "\n",
        "    trainer = Trainer(\n",
        "        progress_bar_refresh_rate=10,\n",
        "        gpus=AVAIL_GPUS, \n",
        "        logger=TensorBoardLogger(tensorboard_log_dir, \"resnet\"), \n",
        "        callbacks=[LearningRateMonitor(logging_interval=\"step\")],\n",
        "        # Changes to use args, change default checkpoint dir, and set number of nodes\n",
        "        max_epochs=args.epochs,\n",
        "        strategy=args.distribute,\n",
        "        default_root_dir=checkpoint_dir,\n",
        "        num_nodes=args.num_nodes,\n",
        "    )\n",
        "\n",
        "    trainer.fit(model, cifar10_dm)\n",
        "    trainer.test(model, datamodule=cifar10_dm)\n",
        "\n",
        "    #Save model step\n",
        "    model_name = \"pylightning_resnet_state_dict.pth\"\n",
        "\n",
        "    model_save_path = os.path.join(model_dir, model_name)\n",
        "    if trainer.global_rank == 0:\n",
        "        makedirs(model_dir)\n",
        "        print(\"Saving model to {}\".format(model_save_path))\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3682bd84fda"
      },
      "source": [
        "#### Configure the container name and path to artifact registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c70a580cc6d4"
      },
      "outputs": [],
      "source": [
        "content_name = \"pytorch-lightning-gpu-training\"\n",
        "hostname = f\"{REGION}-docker.pkg.dev\"\n",
        "image_name_train = content_name\n",
        "tag = \"latest\"\n",
        "\n",
        "custom_container_image_uri_train = (\n",
        "    f\"{hostname}/{PROJECT_ID}/{REPOSITORY}/{image_name_train}:{tag}\"  # noqa\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a45380a286c8"
      },
      "source": [
        "#### Create the requirements.txt and Dockerfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cc1a3438482"
      },
      "outputs": [],
      "source": [
        "%%writefile trainer/requirements.txt\n",
        "torch>=1.6, <1.9\n",
        "lightning-bolts\n",
        "pytorch-lightning=1.3\n",
        "torchmetrics>=0.3\n",
        "torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51a958343b27"
      },
      "outputs": [],
      "source": [
        "%%writefile trainer/Dockerfile\n",
        "FROM pytorch/pytorch:1.8.1-cuda11.1-cudnn8-runtime\n",
        "\n",
        "COPY . /trainer\n",
        "\n",
        "WORKDIR /trainer\n",
        "\n",
        "RUN pip install -r requirements.txt\n",
        "\n",
        "ENTRYPOINT [\"python\", \"task.py\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a7e8f8f534f"
      },
      "source": [
        "#### Create an empty __init__.py file required to be in the container"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "459b9d5a8965"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "with open(os.path.join(\"trainer\", \"__init__.py\"), \"w\") as fp:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b36b457aa5d"
      },
      "source": [
        "#### Build the container, train the model within the container image locally, and push to Artifact Registry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e58c75158872"
      },
      "outputs": [],
      "source": [
        "! cd trainer && docker build -t $custom_container_image_uri_train -f Dockerfile ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bde55966086a"
      },
      "outputs": [],
      "source": [
        "! docker run --rm $custom_container_image_uri_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34c407c3be0e"
      },
      "outputs": [],
      "source": [
        "! docker push $custom_container_image_uri_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "526115eabf35"
      },
      "outputs": [],
      "source": [
        "! gcloud artifacts repositories describe $REPOSITORY --location=$REGION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f026b57d265e"
      },
      "source": [
        "### Initialize Vertex SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0deece1086e"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(\n",
        "    project=PROJECT_ID,\n",
        "    staging_bucket=BUCKET_URI,\n",
        "    location=REGION,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ec68e279ed1"
      },
      "source": [
        "### Create a Vertex AI TensorBoard Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d945f0e32b02"
      },
      "outputs": [],
      "source": [
        "tensorboard = aiplatform.Tensorboard.create(\n",
        "    display_name=content_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb13cc82acc5"
      },
      "source": [
        "#### Option: Use a previously created Vertex AI TensorBoard instance\n",
        "\n",
        "```\n",
        "tensorboard_name = \"Your TensorBoard Resource Name or TensorBoard ID\"\n",
        "tensorboard = aiplatform.Tensorboard(tensorboard_name=tensorboard_name)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e38c06b9377f"
      },
      "source": [
        "#### Set the parameters for the training. The model/TensorBoard/checkpoint directory uses the Vertex defaults. Uncomment to set your own"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8494564137b0"
      },
      "outputs": [],
      "source": [
        "gcs_output_uri_prefix = f\"{BUCKET_URI}/{content_name}-{UUID}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c999fe5df46"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 30\n",
        "TRAIN_STRATEGY = \"dp\"  # Distributed Parallel for single machine multiple GPU\n",
        "MODEL_DIR = f\"{BUCKET_URI}/{content_name}/model\"\n",
        "TB_DIR = f\"{BUCKET_URI}/{content_name}/logs\"\n",
        "CHKPT_DIR = f\"{BUCKET_URI}/{content_name}/checkpoints\"\n",
        "NUM_NODES = 1\n",
        "\n",
        "machine_type = \"n1-standard-4\"\n",
        "accelerator_count = 2\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "\n",
        "CMDARGS = [\n",
        "    \"--epochs=\" + str(EPOCHS),\n",
        "    \"--distribute=\" + TRAIN_STRATEGY,\n",
        "    \"--num-nodes=\" + str(NUM_NODES),\n",
        "    \"--model-dir=\" + MODEL_DIR,\n",
        "    \"--checkpoint-dir=\" + CHKPT_DIR,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db9de8eec4c4"
      },
      "outputs": [],
      "source": [
        "custom_container_training_job = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=content_name + \"-MultGPU-dp-\" + UUID,\n",
        "    container_uri=custom_container_image_uri_train,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa5f67712789"
      },
      "outputs": [],
      "source": [
        "custom_container_training_job.run(\n",
        "    args=CMDARGS,\n",
        "    replica_count=NUM_NODES,\n",
        "    base_output_dir=gcs_output_uri_prefix,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    tensorboard=tensorboard.resource_name,\n",
        "    sync=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29b14f2289f3"
      },
      "outputs": [],
      "source": [
        "print(f\"Custom Training Job Name: {custom_container_training_job.resource_name}\")\n",
        "print(f\"GCS Output URI Prefix: {gcs_output_uri_prefix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e38c06b9377f"
      },
      "source": [
        "#### Delete the training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d681091fd23d"
      },
      "outputs": [],
      "source": [
        "custom_container_training_job.delete()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f2b541299e9"
      },
      "source": [
        "## Run training on multiple machines w/ 1 GPU on each"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b15bd8c318d8"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 30\n",
        "TRAIN_STRATEGY = \"ddp\"  # Distributed Parallel for single machine multiple GPU\n",
        "MODEL_DIR = f\"{BUCKET_URI}/{content_name}-ddp/model\"\n",
        "TB_DIR = f\"{BUCKET_URI}/{content_name}-ddp/logs\"\n",
        "CHKPT_DIR = f\"{BUCKET_URI}/{content_name}-ddp/checkpoints\"\n",
        "NUM_NODES = 2\n",
        "\n",
        "machine_type = \"n1-standard-4\"\n",
        "accelerator_count = 1\n",
        "accelerator_type = \"NVIDIA_TESLA_V100\"\n",
        "\n",
        "CMDARGS = [\n",
        "    \"--epochs=\" + str(EPOCHS),\n",
        "    \"--distribute=\" + TRAIN_STRATEGY,\n",
        "    \"--num-nodes=\" + str(NUM_NODES),\n",
        "    \"--model-dir=\" + MODEL_DIR,\n",
        "    \"--checkpoint-dir=\" + CHKPT_DIR,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4ca2f0e0f00"
      },
      "outputs": [],
      "source": [
        "custom_container_training_job_dist = aiplatform.CustomContainerTrainingJob(\n",
        "    display_name=content_name + \"-MultiCPU-1GPU-ddp-\" + UUID,\n",
        "    container_uri=custom_container_image_uri_train,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fbe3a5549a1"
      },
      "outputs": [],
      "source": [
        "custom_container_training_job_dist.run(\n",
        "    args=CMDARGS,\n",
        "    replica_count=NUM_NODES,\n",
        "    base_output_dir=gcs_output_uri_prefix,\n",
        "    machine_type=machine_type,\n",
        "    accelerator_type=accelerator_type,\n",
        "    accelerator_count=accelerator_count,\n",
        "    service_account=SERVICE_ACCOUNT,\n",
        "    tensorboard=tensorboard.resource_name,\n",
        "    sync=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29b14f2289f3"
      },
      "outputs": [],
      "source": [
        "print(f\"Custom Training Job Name: {custom_container_training_job_dist.resource_name}\")\n",
        "print(f\"GCS Output URI Prefix: {gcs_output_uri_prefix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "## Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx_vKniMq9ZX"
      },
      "outputs": [],
      "source": [
        "# Warning: Setting this to true will delete everything in your bucket\n",
        "delete_bucket = False\n",
        "\n",
        "# Delete TensorBoard\n",
        "TB_NAME = tensorboard.resource_name\n",
        "! gcloud beta ai tensorboards delete $TB_NAME --quiet\n",
        "\n",
        "# Delete the training job\n",
        "custom_container_training_job_dist.delete()\n",
        "\n",
        "CONTENT_DIR = f\"{BUCKET_URI}/{content_name}*\"\n",
        "# Delete Cloud Storage objects that were created\n",
        "! gsutil -m rm -r $CONTENT_DIR\n",
        "\n",
        "if delete_bucket and \"BUCKET_URI\" in globals():\n",
        "    ! gsutil -m rm -r $BUCKET_URI"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sdk_pytorch_lightning_custom_container_training.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
