{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a2820e-2c3b-44db-a52c-3f7c6c4d6a20",
   "metadata": {
    "id": "18ebbd838e32"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c4f2d5-71a3-4881-a48f-3c593894f40b",
   "metadata": {
    "id": "50c4f2d5-71a3-4881-a48f-3c593894f40b"
   },
   "source": [
    "# Serving a stable diffusion model on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c26c8-4e19-42bd-adee-c925718bf2e0",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22460d37-17c7-4a01-88b2-e5bacafdf971",
   "metadata": {
    "id": "22460d37-17c7-4a01-88b2-e5bacafdf971",
    "outputId": "05acf8b2-f4b6-4959-dcbe-1a7369ad56d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "torchserve\n",
    "torch-model-archiver\n",
    "torch-workflow-archiver\n",
    "google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89d2ab8-2f15-42c3-b31d-002a7737be30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torchserve\n",
      "  Downloading torchserve-0.7.0-py3-none-any.whl (19.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch-model-archiver\n",
      "  Downloading torch_model_archiver-0.7.0-py3-none-any.whl (14 kB)\n",
      "Collecting torch-workflow-archiver\n",
      "  Downloading torch_workflow_archiver-0.2.6-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.21.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchserve->-r requirements.txt (line 1)) (23.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from torchserve->-r requirements.txt (line 1)) (0.38.4)\n",
      "Collecting future\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m184.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from torchserve->-r requirements.txt (line 1)) (9.4.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from torchserve->-r requirements.txt (line 1)) (5.9.3)\n",
      "Collecting enum-compat\n",
      "  Downloading enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 4)) (3.19.6)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 4)) (2.7.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 4)) (1.22.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 4)) (1.34.0)\n",
      "Collecting packaging\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m140.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 4)) (1.8.5.post1)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 4)) (3.4.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (1.58.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (2.16.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (2.28.2)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (1.48.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (1.51.1)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (2.4.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->-r requirements.txt (line 4)) (0.12.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->torchserve->-r requirements.txt (line 1)) (3.0.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (4.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 4)) (0.4.8)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492025 sha256=dfdeb04b26f5484240c3b76ab405626cfcb226fc19f0d58b60f70e098f44454e\n",
      "  Stored in directory: /var/tmp/pip-ephem-wheel-cache-pii5uczt/wheels/52/2a/fc/520209cfa6448febd490720a0b09036cb367628f7c4e9cc172\n",
      "Successfully built future\n",
      "Installing collected packages: torch-workflow-archiver, enum-compat, packaging, future, torchserve, torch-model-archiver\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "Successfully installed enum-compat-0.0.3 future-0.18.3 packaging-21.3 torch-model-archiver-0.7.0 torch-workflow-archiver-0.2.6 torchserve-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870dd180-d439-406c-b18d-771c426b6a54",
   "metadata": {
    "id": "870dd180-d439-406c-b18d-771c426b6a54"
   },
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8c6feef-c3b7-46a9-9a94-ab10fdb631ec",
   "metadata": {
    "id": "b8c6feef-c3b7-46a9-9a94-ab10fdb631ec"
   },
   "outputs": [],
   "source": [
    "!mkdir model_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafbb0e0-40e6-43a0-a38e-edc54323da51",
   "metadata": {
    "id": "eafbb0e0-40e6-43a0-a38e-edc54323da51"
   },
   "source": [
    "## Create the custom TorchServe handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94567a87-9d74-4c87-a749-306ddaf01b61",
   "metadata": {
    "id": "94567a87-9d74-4c87-a749-306ddaf01b61",
    "outputId": "9bbfea0e-3792-4e83-afb4-177671ca3f9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_artifacts/handler.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_artifacts/handler.py\n",
    "\n",
    "\"\"\"Customized handler for stable diffusion 2.\"\"\"\n",
    "import base64\n",
    "import logging\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "from diffusers import EulerDiscreteScheduler\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "model_id = 'stabilityai/stable-diffusion-2'\n",
    "\n",
    "\n",
    "class ModelHandler(BaseHandler):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.initialized = False\n",
    "    self.map_location = None\n",
    "    self.device = None\n",
    "    self.use_gpu = True\n",
    "    self.store_avg = True\n",
    "    self.pipe = None\n",
    "\n",
    "  def initialize(self, context):\n",
    "    \"\"\"Initializes the pipe.\"\"\"\n",
    "    properties = context.system_properties\n",
    "    gpu_id = properties.get('gpu_id')\n",
    "\n",
    "    self.map_location, self.device, self.use_gpu = \\\n",
    "      ('cuda', torch.device('cuda:' + str(gpu_id)),\n",
    "       True) if torch.cuda.is_available() else \\\n",
    "        ('cpu', torch.device('cpu'), False)\n",
    "\n",
    "    # Use the Euler scheduler here instead\n",
    "    scheduler = EulerDiscreteScheduler.from_pretrained(model_id,\n",
    "                                                       subfolder='scheduler')\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id,\n",
    "                                                   scheduler=scheduler,\n",
    "                                                   torch_dtype=torch.float16)\n",
    "    pipe = pipe.to('cuda')\n",
    "    # Uncomment the following line to reduce the GPU memory usage.\n",
    "    # pipe.enable_attention_slicing()\n",
    "    self.pipe = pipe\n",
    "\n",
    "    self.initialized = True\n",
    "\n",
    "  def preprocess(self, requests):\n",
    "    \"\"\"Noting to do here.\"\"\"\n",
    "    logger.info('requests: %s', requests)\n",
    "    return requests\n",
    "\n",
    "  def inference(self, preprocessed_data, *args, **kwargs):\n",
    "    \"\"\"Run the inference.\"\"\"\n",
    "    images = []\n",
    "    for pd in preprocessed_data:\n",
    "      prompt = pd['prompt']\n",
    "      images.extend(self.pipe(prompt).images)\n",
    "    return images\n",
    "\n",
    "  def postprocess(self, output_batch):\n",
    "    \"\"\"Converts the images to base64 string.\"\"\"\n",
    "    postprocessed_data = []\n",
    "    for op in output_batch:\n",
    "      fp = BytesIO()\n",
    "      op.save(fp, format='JPEG')\n",
    "      postprocessed_data.append(base64.b64encode(fp.getvalue()).decode('utf-8'))\n",
    "      fp.close()\n",
    "    return postprocessed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba496ba-0004-4ba6-a984-3c54a03f036e",
   "metadata": {
    "id": "bba496ba-0004-4ba6-a984-3c54a03f036e"
   },
   "source": [
    "## Create TorchServe model archive file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b3ee32-ddfc-4633-83f7-912b4ca211bd",
   "metadata": {
    "id": "e7b3ee32-ddfc-4633-83f7-912b4ca211bd"
   },
   "outputs": [],
   "source": [
    "!torch-model-archiver \\\n",
    "  -f \\\n",
    "  --model-name model \\\n",
    "  --version 1.0 \\\n",
    "  --handler model_artifacts/handler.py \\\n",
    "  --export-path model_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1789667-b3bd-4135-80ae-cebf3ef36d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handler.py  model.mar\n"
     ]
    }
   ],
   "source": [
    "!ls model_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af4464d-b585-4f4f-9259-84c56db59624",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"speech-erschmid\"\n",
    "GCS_PATH = f\"gs://{BUCKET_NAME}/\" # change this to a gcs path\n",
    "FULL_GCS_PATH = f\"{GCS_PATH}model_artifacts\"\n",
    "LOCATION = \"us-west1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6afa77-7378-4616-82b8-e18133b9a0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model_artifacts/handler.py [Content-Type=text/x-python]...\n",
      "Copying file://model_artifacts/model.mar [Content-Type=application/octet-stream]...\n",
      "/ [2 files][  3.3 KiB/  3.3 KiB]                                                \n",
      "Operation completed over 2 objects/3.3 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r model_artifacts $GCS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16902ae-2fc7-482e-bce9-ade8f480e872",
   "metadata": {},
   "source": [
    "**Note** You need to upload the model that you trained with Dreambooth to the same Google Cloud Storage location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aac36cab-b74e-4a9e-a5f2-502b6bff5268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/stable_diffusion_weights/output/model_index.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/model-dog.ckpt [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/args.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/text_encoder/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "/ [4 files][  4.4 GiB/  4.4 GiB]  117.6 MiB/s                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/text_encoder/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/scheduler/scheduler_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/0/logs/dreambooth/events.out.tfevents.1675122371.stable-diffusion.18308.0 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/unet/diffusion_pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/unet/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/vae/diffusion_pytorch_model.bin [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/vae/config.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/tokenizer/merges.txt [Content-Type=text/plain]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/tokenizer/special_tokens_map.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/tokenizer/tokenizer_config.json [Content-Type=application/json]...\n",
      "Copying file:///home/jupyter/stable_diffusion_weights/output/tokenizer/vocab.json [Content-Type=application/json]...\n",
      "| [15 files][  8.0 GiB/  8.0 GiB]  114.1 MiB/s                                  \n",
      "Operation completed over 15 objects/8.0 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r /home/jupyter/stable_diffusion_weights/output $FULL_GCS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ca67c39-9d14-4c67-a780-6865beda8bd5",
   "metadata": {
    "id": "7ca67c39-9d14-4c67-a780-6865beda8bd5",
    "outputId": "6e517fc4-436b-415d-b4b6-53803ab0cf3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM_PREDICTOR_IMAGE_URI = us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.1-12:latest\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"video-erschmid\"  # <---CHANGE THIS TO YOUR PROJECT\n",
    "CUSTOM_PREDICTOR_IMAGE_URI = f\"us-docker.pkg.dev/vertex-ai/prediction/pytorch-gpu.1-12:latest\"\n",
    "APP_NAME = \"my-stable-diffusion\"\n",
    "print(f\"CUSTOM_PREDICTOR_IMAGE_URI = {CUSTOM_PREDICTOR_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2766ac7-a016-42e7-bd64-6f9feadbc467",
   "metadata": {
    "id": "c2766ac7-a016-42e7-bd64-6f9feadbc467"
   },
   "source": [
    "## Deploy to Vertex AI endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9151a761-9079-4f1f-96e3-9f80b9a3d427",
   "metadata": {
    "id": "9151a761-9079-4f1f-96e3-9f80b9a3d427"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "993e6ac0-8dea-4edd-b17b-051564c3ec62",
   "metadata": {
    "id": "993e6ac0-8dea-4edd-b17b-051564c3ec62"
   },
   "outputs": [],
   "source": [
    "VERSION = 1\n",
    "model_display_name = \"stable_diffusion_2\"\n",
    "model_description = \"stable_diffusion_2 container\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e33903b5-24b0-4629-9af3-8dcca8d326c6",
   "metadata": {
    "id": "e33903b5-24b0-4629-9af3-8dcca8d326c6",
    "outputId": "884b5c86-a3e5-47c0-c344-68fb5b415d65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/147301782967/locations/us-west1/models/7406503838745624576/operations/3441447205683068928\n",
      "Model created. Resource name: projects/147301782967/locations/us-west1/models/7406503838745624576@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/147301782967/locations/us-west1/models/7406503838745624576@1')\n",
      "stable_diffusion_2\n",
      "projects/147301782967/locations/us-west1/models/7406503838745624576\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=model_display_name,\n",
    "    description=model_description,\n",
    "    serving_container_image_uri=CUSTOM_PREDICTOR_IMAGE_URI,\n",
    "    artifact_uri=FULL_GCS_PATH ,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b87521ad-7425-4d4f-8b80-1fabbfb91ec4",
   "metadata": {
    "id": "b87521ad-7425-4d4f-8b80-1fabbfb91ec4",
    "outputId": "ae7fed39-0755-4157-ee2a-6874aeb11417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/147301782967/locations/us-west1/endpoints/7728229737125904384/operations/7159731648030834688\n",
      "Endpoint created. Resource name: projects/147301782967/locations/us-west1/endpoints/7728229737125904384\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/147301782967/locations/us-west1/endpoints/7728229737125904384')\n"
     ]
    }
   ],
   "source": [
    "endpoint_display_name = f\"{APP_NAME}-endpoint\"\n",
    "endpoint = aiplatform.Endpoint.create(display_name=endpoint_display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c693352b-e1a8-4013-aaad-1d882f145472",
   "metadata": {
    "id": "c693352b-e1a8-4013-aaad-1d882f145472",
    "outputId": "ad476893-204c-4e45-be1f-f5845454eb04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/147301782967/locations/us-west1/endpoints/7728229737125904384\n",
      "Deploy Endpoint model backing LRO: projects/147301782967/locations/us-west1/endpoints/7728229737125904384/operations/9088961138405670912\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_31380/638337043.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0maccelerator_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtraffic_percentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraffic_percentage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   3314\u001b[0m             \u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3315\u001b[0m             \u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3316\u001b[0;31m             \u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3317\u001b[0m         )\n\u001b[1;32m   3318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_spec, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   3477\u001b[0m             \u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m             \u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m             \u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3480\u001b[0m         )\n\u001b[1;32m   3481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model, endpoint_resource_traffic_split, network, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_spec, metadata, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         )\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m     def undeploy(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \"\"\"\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mpolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_or_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             raise concurrent.futures.TimeoutError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             )\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;34m\"Retrying due to {}, sleeping {:.1f}s ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_exc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         )\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sleep generator stopped yielding sleep values.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "traffic_percentage = 100\n",
    "machine_type = \"n1-standard-4\"\n",
    "accelerator_type = \"NVIDIA_TESLA_T4\"\n",
    "accelerator_count = 1\n",
    "deployed_model_display_name = model_display_name\n",
    "min_replica_count = 1\n",
    "max_replica_count = 1\n",
    "sync = True\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=deployed_model_display_name,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    traffic_percentage=traffic_percentage,\n",
    "    sync=sync,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3b43f-ef1f-448f-89ee-b6bbc4dbceac",
   "metadata": {
    "id": "a6b3b43f-ef1f-448f-89ee-b6bbc4dbceac"
   },
   "source": [
    "## Getting a new image from the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa1c80b1-de37-40c4-ba67-e28a2b5ed89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [{ \"prompt\": \"A dog with a baseball jersey.\" }]\n",
    "response = endpoint.predict(instances=instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92b61646-3c70-4f73-9d31-81fb2cd46396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from IPython import display\n",
    "\n",
    "with open('img5.jpg', 'wb') as g:\n",
    "    g.write(base64.b64decode(response.predictions[0]))\n",
    "    \n",
    "display.Image('img5.jpg')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
