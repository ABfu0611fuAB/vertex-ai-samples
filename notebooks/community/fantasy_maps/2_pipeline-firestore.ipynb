{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce211e25-1262-49d7-9dbc-6748718e81c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title ###### Licensed to the Apache Software Foundation (ASF), Version 2.0 (the \"License\")\n",
    "\n",
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements. See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership. The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License. You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied. See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c8bcb",
   "metadata": {},
   "source": [
    "# Creating a Vertex Pipeline to extract training data\n",
    "\n",
    "This notebook (the second in a five-part series) creates a Vertex AI pipeline that scrapes images from an online source (e.g. Reddit) and stores the image metadata in Firestore. Here, you will build a pipeline that \n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "1. Creating a pipeline component to collect images from Reddit\n",
    "1. Creating a pipeline component to store images in Cloud Storage\n",
    "1. Creating a pipeline component to store metadata about the images in Firestore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82dcd1e-611e-428d-9984-8c89fd181fec",
   "metadata": {},
   "source": [
    "### Set IAM permissions\n",
    "\n",
    "When you run a notebook on Vertex Workbench, the notebook runs in a Compute Engine context that has its own service account. You will need to give your service account IAM permissions to access Secret Manager before you can use it (in a pipeline).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c9b41-cc68-4587-bac1-ef1c4799e88a",
   "metadata": {},
   "source": [
    "### Enable the Cloud resources\n",
    "\n",
    "For this notebook, you must have a Google Cloud project with the following resources:\n",
    "\n",
    "+ A Cloud Storage bucket\n",
    "+ The following APIs enabled:\n",
    "  - Cloud Firestore\n",
    "  - Vertex AI\n",
    "  - Storage\n",
    "  - Secret Manager\n",
    "  \n",
    "If you completed the [first](1_firestore.ipynb) notebook in this series, you should have these APIs already enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841f4afa-2219-4274-81ae-5d1e37cd638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  fantasymaps-334622\n"
     ]
    }
   ],
   "source": [
    "# Get your GCP project id from gcloud\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d0df4f24-5c51-45be-8aad-8fff58b6bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"fantasy-maps\" # Google Cloud Storage bucket\n",
    "COLLECTION_NAME = \"FantasyMapsTest\" # Firestore collection name\n",
    "LOCATION = \"us-central1\"\n",
    "GCS_PREFIX = \"ScrapedData\"\n",
    "SUBREDDIT_NAME = \"battlemaps\"\n",
    "LIMIT=300\n",
    "MODEL_ID = \"4304645197347684352\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67b63f",
   "metadata": {},
   "source": [
    "### Install the required Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "3b04cdb7-fea6-46ec-a647-b334de316587",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rfd requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "84639262-bb90-43b9-bdf2-a047277c29ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "google-cloud-secret-manager\n",
    "google-cloud-aiplatform\n",
    "google-cloud-pipeline-components>=1.0.30\n",
    "kfp\n",
    "praw\n",
    "pandas\n",
    "spacy\n",
    "pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bafcfe57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Requirement already satisfied: google-cloud-secret-manager in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (2.10.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/jupyter/.local/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.3.0)\n",
      "Collecting google-cloud-pipeline-components>=1.0.30\n",
      "  Downloading google_cloud_pipeline_components-1.0.30-py3-none-any.whl (817 kB)\n",
      "     |████████████████████████████████| 817 kB 4.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: kfp in /home/jupyter/.local/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.8.12)\n",
      "Requirement already satisfied: praw in /home/jupyter/.local/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (7.5.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.3.4)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (3.4.3)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (8.4.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (1.31.5)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (0.12.3)\n",
      "Requirement already satisfied: proto-plus>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-secret-manager->-r requirements.txt (line 1)) (1.19.8)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (21.3)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (2.30.1)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 2)) (1.43.0)\n",
      "Requirement already satisfied: google-cloud-notebooks>=0.4.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-pipeline-components>=1.0.30->-r requirements.txt (line 3)) (1.2.1)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.20.0-py2.py3-none-any.whl (2.3 MB)\n",
      "     |████████████████████████████████| 2.3 MB 67.6 MB/s            \n",
      "\u001b[?25hCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)\n",
      "     |████████████████████████████████| 409 kB 85.8 MB/s            \n",
      "\u001b[?25hCollecting proto-plus>=1.15.0\n",
      "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
      "     |████████████████████████████████| 47 kB 5.8 MB/s             \n",
      "\u001b[?25hCollecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.7.0-py2.py3-none-any.whl (235 kB)\n",
      "     |████████████████████████████████| 235 kB 64.1 MB/s            \n",
      "\u001b[?25hCollecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.0-py3-none-any.whl (120 kB)\n",
      "     |████████████████████████████████| 120 kB 81.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: jsonschema<4,>=3.0.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.8.9)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.9.1)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.11.0)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.14 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.1.14)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.12.8)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (8.0.3)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.13)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "     |████████████████████████████████| 1.0 MB 42.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: google-auth<2,>=1.6.1 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.35.0)\n",
      "Requirement already satisfied: pydantic<2,>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.8.2)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.2.13)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (3.10.0.2)\n",
      "Requirement already satisfied: kubernetes<19,>=8.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (18.20.0)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (0.1.10)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/jupyter/.local/lib/python3.7/site-packages (from kfp->-r requirements.txt (line 4)) (1.7.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /home/jupyter/.local/lib/python3.7/site-packages (from praw->-r requirements.txt (line 5)) (2.3.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in /home/jupyter/.local/lib/python3.7/site-packages (from praw->-r requirements.txt (line 5)) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.7/site-packages (from praw->-r requirements.txt (line 5)) (1.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (2021.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 6)) (1.19.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.26.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (4.62.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (1.0.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (59.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (0.10.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (8.1.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy->-r requirements.txt (line 7)) (2.4.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py<2,>=0.9->kfp->-r requirements.txt (line 4)) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy->-r requirements.txt (line 7)) (3.6.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9,>=7.1.2->kfp->-r requirements.txt (line 4)) (4.8.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp->-r requirements.txt (line 4)) (1.13.3)\n",
      "Requirement already satisfied: termcolor in /home/jupyter/.local/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager->-r requirements.txt (line 1)) (1.53.0)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
      "     |████████████████████████████████| 115 kB 47.2 MB/s            \n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.57.0-py2.py3-none-any.whl (217 kB)\n",
      "     |████████████████████████████████| 217 kB 79.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager->-r requirements.txt (line 1)) (1.42.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-secret-manager->-r requirements.txt (line 1)) (1.42.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp->-r requirements.txt (line 4)) (0.19.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->kfp->-r requirements.txt (line 4)) (0.1.0)\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "     |████████████████████████████████| 62 kB 510 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (4.2.4)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (1.7.2)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "     |████████████████████████████████| 233 kB 59.5 MB/s            \n",
      "\u001b[?25hCollecting grpc-google-iam-v1<0.13dev,>=0.12.3\n",
      "  Downloading grpc_google_iam_v1-0.12.4-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp->-r requirements.txt (line 4)) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp->-r requirements.txt (line 4)) (21.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp->-r requirements.txt (line 4)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp->-r requirements.txt (line 4)) (1.26.7)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<19,>=8.0.0->kfp->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.4.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy->-r requirements.txt (line 7)) (6.2.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 7)) (3.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp->-r requirements.txt (line 4)) (0.37.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 7)) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 7)) (0.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy->-r requirements.txt (line 7)) (2.0.1)\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1\n",
      "  Downloading google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (1.1.2)\n",
      "Collecting grpcio<2.0dev,>=1.33.2\n",
      "  Downloading grpcio-1.51.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "     |████████████████████████████████| 4.8 MB 51.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp->-r requirements.txt (line 4)) (3.1.1)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform->-r requirements.txt (line 2)) (2.21)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Installing collected packages: protobuf, grpcio, googleapis-common-protos, google-api-core, proto-plus, grpc-google-iam-v1, google-cloud-core, google-cloud-resource-manager, google-api-python-client, google-cloud-aiplatform, google-cloud-pipeline-components\n",
      "  Attempting uninstall: protobuf\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: grpcio\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: googleapis-common-protos 1.53.0\n",
      "    Uninstalling googleapis-common-protos-1.53.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.53.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.31.5\n",
      "    Uninstalling google-api-core-1.31.5:\n",
      "      Successfully uninstalled google-api-core-1.31.5\n",
      "  Attempting uninstall: proto-plus\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: proto-plus 1.19.8\n",
      "    Uninstalling proto-plus-1.19.8:\n",
      "      Successfully uninstalled proto-plus-1.19.8\n",
      "  Attempting uninstall: grpc-google-iam-v1\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: grpc-google-iam-v1 0.12.3\n",
      "    Uninstalling grpc-google-iam-v1-0.12.3:\n",
      "      Successfully uninstalled grpc-google-iam-v1-0.12.3\n",
      "  Attempting uninstall: google-cloud-core\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "    Found existing installation: google-cloud-core 1.7.2\n",
      "    Uninstalling google-cloud-core-1.7.2:\n",
      "      Successfully uninstalled google-cloud-core-1.7.2\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 1.12.8\n",
      "    Uninstalling google-api-python-client-1.12.8:\n",
      "      Successfully uninstalled google-api-python-client-1.12.8\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.3.0\n",
      "    Uninstalling google-cloud-aiplatform-1.3.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.3.0\n",
      "  Attempting uninstall: google-cloud-pipeline-components\n",
      "    Found existing installation: google-cloud-pipeline-components 0.1.5\n",
      "    Uninstalling google-cloud-pipeline-components-0.1.5:\n",
      "      Successfully uninstalled google-cloud-pipeline-components-0.1.5\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-cloud-recommendations-ai 0.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.10.2 which is incompatible.\n",
      "fantasy-maps 0.1.0 requires google-cloud-aiplatform==1.3.0, but you have google-cloud-aiplatform 1.20.0 which is incompatible.\u001b[0m\n",
      "Successfully installed google-api-core-2.10.2 google-api-python-client-2.31.0 google-cloud-aiplatform-1.20.0 google-cloud-core-2.3.2 google-cloud-pipeline-components-1.0.30 google-cloud-resource-manager-1.6.3 googleapis-common-protos-1.57.0 grpc-google-iam-v1-0.12.4 grpcio-1.51.1 proto-plus-1.22.1 protobuf-3.20.3\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7087667-c013-4166-b44f-b703ac011f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "     |████████████████████████████████| 12.8 MB 5.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/jupyter/.local/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (59.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /home/jupyter/.local/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.10.0.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.62.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.26.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.19.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (6.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.8.2)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oogle-cloud-datastore (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383ecef",
   "metadata": {},
   "source": [
    "## Create a custom Reddit pipelines component\n",
    "\n",
    "The pipeline and all it components need to be compiled into a runnable format. We use the Kubeflow Pipelines (`kfp`) SDK to create this uploadable pipelines file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99aef58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f2f591-92a3-4c46-82d4-082c6961d080",
   "metadata": {},
   "source": [
    "Now we can define the pipeline. For this component, we are going to store the `pandas.DataFrame` that we compose from the Redit posts as a CSV file on Cloud Storage. We'll pass the URI of this Storage file onto the next piece of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fdc68927",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"praw\",\n",
    "                                \"google-cloud-secret-manager\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"numpy\",\n",
    "                                \"pandas\",\n",
    "                                \"spacy\"])\n",
    "def reddit(\n",
    "    secret_name: str,\n",
    "    subreddit_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    project_id: str,\n",
    "    limit: int,\n",
    ") -> str:\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import praw\n",
    "    import re\n",
    "    \n",
    "    from google.cloud import storage\n",
    "\n",
    "    def get_reddit_credentials(project_id):\n",
    "        \"\"\"Gets the Reddit API key out of Secrets Manager\n",
    "    \n",
    "        Arguments:\n",
    "            project_id (str): the current project ID\n",
    "\n",
    "        Returns:\n",
    "            JSON object (dict)\n",
    "        \"\"\"\n",
    "        from google.cloud import secretmanager\n",
    "        import json\n",
    "\n",
    "        client = secretmanager.SecretManagerServiceClient()\n",
    "\n",
    "        secret_resource_name = f\"projects/{project_id}/secrets/{secret_name}/versions/1\"\n",
    "        response = client.access_secret_version(request={\"name\": secret_resource_name})\n",
    "        payload = response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "        return json.loads(payload)\n",
    "    \n",
    "    def get_reddit_posts(reddit_credentials, subreddit_name, limit):\n",
    "        \"\"\"Gets posts from a subreddit.\n",
    "\n",
    "        Arguments:\n",
    "            reddit_credentials (dict): a dictionary with client_id, secret, and user_agent\n",
    "            subreddit_name (str): the name of the subreddit to scrape posts from\n",
    "            limit (int): the maximum number of posts to grab\n",
    "\n",
    "        Returns:\n",
    "            List of Reddit API objects\n",
    "        \"\"\"\n",
    "        import praw\n",
    "\n",
    "        reddit = praw.Reddit(client_id=reddit_credentials[\"client_id\"], \n",
    "                     client_secret=reddit_credentials[\"secret\"],\n",
    "                     user_agent=reddit_credentials[\"user_agent\"])\n",
    "\n",
    "        return reddit.subreddit(subreddit_name).hot(limit=limit)\n",
    "\n",
    "    def convert_posts_to_dataframe(posts, columns):\n",
    "        \"\"\"Converts a sequence of Reddit API post objects into a pandas.DataFrame.\n",
    "        \n",
    "        Arguments:\n",
    "            posts (list(praw.Post)): the posts from Reddit\n",
    "            columns (list(str)): the column headings for the Dataframe\n",
    "        \n",
    "        Returns:\n",
    "            A pandas.Dataframe\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        filtered_posts = [[s.title, s.selftext, s.id, s.url] for s in posts]\n",
    "        filtered_posts = np.array(filtered_posts)\n",
    "        reddit_posts_df = pd.DataFrame(filtered_posts,\n",
    "                                   columns=columns)\n",
    "\n",
    "        return reddit_posts_df\n",
    "    \n",
    "    COLUMNS = ['Title', 'Post', 'ID', 'URL']\n",
    "    \n",
    "    # Get the data from Reddit\n",
    "    credentials = get_reddit_credentials(project_id=project_id)\n",
    "    posts = get_reddit_posts(reddit_credentials=credentials, subreddit_name=subreddit_name,\n",
    "                             limit=limit)\n",
    "    \n",
    "    reddit_posts_df = convert_posts_to_dataframe(posts=posts, columns=COLUMNS)\n",
    "    \n",
    "    # Remove all of the posts that don't meet our criteria\n",
    "    import re\n",
    "    jpg_df = reddit_posts_df[(reddit_posts_df[\"URL\"].str.contains(\"jpg\")) &\n",
    "                             (reddit_posts_df[\"Title\"].str.contains(pat = \"\\d+x\\d\"))]\n",
    "    \n",
    "    # Save the dataframe as CSV in Storage\n",
    "    csv_str = jpg_df.to_csv()\n",
    "    \n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    csv_file_uri = f\"{gcs_prefix_name}/_reddit-scraped-{subreddit_name}-{timestamp}.csv\"\n",
    "    \n",
    "    file_blob = bucket.blob(csv_file_uri)\n",
    "    file_blob.upload_from_string(csv_str)\n",
    "    \n",
    "    return csv_file_uri\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0794401a-c153-4407-a04f-030e70c4b3db",
   "metadata": {},
   "source": [
    "## Create the Cloud Storage component\n",
    "\n",
    "In this next component, we need to store any unique map images that we have picked up from the scraping. However, we need to validate that these images are useful training data before we process them and store their metadata in Firestore.\n",
    "\n",
    "To validate the images, we will do the following:\n",
    "\n",
    "1. Ensure that we don't already have the image in Firestore\n",
    "2. Use a pre-trained, earlier version of our model to infer the existence of gridlines on the image\n",
    "\n",
    "We'll do the first step in validation in the `storage` component. The second step (using an existing model to validate the usefulness of the images) will require using batch predictions on Vertex AI; we'll create another component to handle that part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "eb58d5ca-67dd-4abe-8aba-e6cee61e16d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@component(packages_to_install=[\"spacy\",\n",
    "                                \"google-cloud-firestore\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"pandas\",\n",
    "                                \"jsonlines\"])\n",
    "def storage(\n",
    "    project_id: str,\n",
    "    location_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    collection_name: str,\n",
    "    csv_input_file: str,\n",
    ") -> NamedTuple(\n",
    "    \"outputs\",\n",
    "    [\n",
    "        (\"batch_predict_file_uri\", str),\n",
    "        (\"posts_csv_file\", str),\n",
    "    ]\n",
    "):\n",
    "    \n",
    "    from google.cloud import firestore\n",
    "    from google.cloud import storage\n",
    "    \n",
    "    import base64\n",
    "    from datetime import datetime\n",
    "    from io import BytesIO\n",
    "    import jsonlines\n",
    "    import pandas as pd\n",
    "    \n",
    "    import spacy\n",
    "    spacy.cli.download(\"en_core_web_sm\")\n",
    "    spacy.prefer_gpu()\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    def make_nice_filename(name):\n",
    "        \"\"\"Converts Reddit post title into a meaningful(ish) filename.\n",
    "\n",
    "        Arguments:\n",
    "            name (str): title of the post\n",
    "\n",
    "        Returns:\n",
    "            String. Format is `<adj.>-<nouns>.<cols>x<rows>.jpg`\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        dims = re.findall(\"\\d+x\\d+\", name)\n",
    "        if len(dims) is 0:\n",
    "            return \"\"\n",
    "\n",
    "        dims = dims[0].split(\"x\")\n",
    "        if len(dims) is not 2:\n",
    "            return \"\"\n",
    "\n",
    "        tokens = get_tokens(name)\n",
    "        new_name = name.lower()[:30]\n",
    "\n",
    "        if len(tokens) > 0:\n",
    "            tokens = tokens[:6] # Arbitrarily keep new names to six words or less\n",
    "            new_name = \"_\".join(tokens)\n",
    "\n",
    "        return f\"{new_name}.{dims[0]}x{dims[1]}.jpg\"\n",
    "\n",
    "    def get_tokens(title):\n",
    "        \"\"\"Analyzes a post for nouns, proper nouns, and adjectives.\n",
    "\n",
    "        Arguments:\n",
    "            title (str): title of the post\n",
    "\n",
    "        Returns:\n",
    "            List of string. Words to use in a filename.    \n",
    "        \"\"\"\n",
    "        import spacy\n",
    "\n",
    "        POS = [\"PROPN\", \"NOUN\", \"ADJ\"]\n",
    "        words = []\n",
    "\n",
    "        tokens = nlp(title)\n",
    "        for t in tokens:\n",
    "            pos = t.pos_\n",
    "\n",
    "            if pos in POS:\n",
    "                words.append(t.text.lower())\n",
    "\n",
    "        return words\n",
    "    def convert_image_to_hash(content):\n",
    "        \"\"\"Convert image data to hash value (str).\n",
    "\n",
    "        Arguments:\n",
    "            content (byte array): the image\n",
    "\n",
    "        Return:\n",
    "            The image hash value as a string.\n",
    "        \"\"\"\n",
    "        import hashlib\n",
    "\n",
    "        sha1 = hashlib.sha1()\n",
    "        jpg_hash = sha1.update(content)\n",
    "        jpg_hash = sha1.hexdigest()\n",
    "\n",
    "        return jpg_hash\n",
    "    \n",
    "    def download_image(url):\n",
    "        \"\"\"Download an image from the internet to local file system.\n",
    "\n",
    "        Arguments:\n",
    "            url (str): the image to download\n",
    "\n",
    "        Returns:\n",
    "            Bool. Indicates whether downloading the image was successful.\n",
    "        \"\"\"\n",
    "        import requests\n",
    "\n",
    "        r = requests.get(url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            r.raw.decode_content = True\n",
    "            \n",
    "            hsh = convert_image_to_hash(r.content)\n",
    "            return (r.content, hsh)\n",
    "    \n",
    "    # Begin pipeline\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "\n",
    "    firestore_client = firestore.Client(project=project_id)\n",
    "    collection_ref = firestore_client.collection(collection_name)\n",
    "\n",
    "    blob = bucket.blob(csv_input_file)\n",
    "    csv_bytes = blob.download_as_string()\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    jpg_df = pd.read_csv(csv_buffer)\n",
    "    batch_prediction_inputs = []\n",
    "    \n",
    "    for i, row, in jpg_df.iterrows():\n",
    "        url = row[\"URL\"]\n",
    "        title = row[\"Title\"]\n",
    "        \n",
    "        content, hsh = download_image(url)\n",
    "\n",
    "        # Check whether we already have this image\n",
    "        doc_ref = collection_ref.document(hsh)\n",
    "        doc_ref = doc_ref.get()\n",
    "        if not doc_ref.exists:\n",
    "            continue\n",
    "        \n",
    "        file_name = make_nice_filename(title)\n",
    "                \n",
    "        img_gcs_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/{file_name}\"\n",
    "        blob_name = f\"{gcs_prefix_name}/{file_name}\"\n",
    "\n",
    "        file_blob = bucket.blob(blob_name)\n",
    "        image_buffer = BytesIO(content)\n",
    "\n",
    "        # Get image grid metadata\n",
    "        file_blob.upload_from_file(image_buffer)\n",
    "        jpg_df.at[i, \"gcsURI\"] = img_gcs_uri\n",
    "        jpg_df.at[i, \"UID\"] = hsh\n",
    "        \n",
    "        batch_prediction_inputs.append({\n",
    "            \"content\": img_gcs_uri,\n",
    "            \"mimeType\": \"image/jpeg\",\n",
    "        })\n",
    "\n",
    "    # Save the dataframe as CSV in Storage (again)\n",
    "    csv_str = jpg_df.to_csv()\n",
    "    file_blob = bucket.blob(csv_input_file)\n",
    "    file_blob.upload_from_string(csv_str)\n",
    "    \n",
    "    # Create batch prediction input file\n",
    "    bpi = BytesIO()\n",
    "    writer = jsonlines.Writer(bpi)\n",
    "    writer.write_all(batch_prediction_inputs)\n",
    "    writer.close()\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    bpi_gcs_path = f\"{gcs_prefix_name}/_batch_prediction_input_{timestamp}.jsonl\"\n",
    "    batch_prediction_input_file = f\"gs://{gcs_bucket_name}/{bpi_gcs_path}\"\n",
    "    \n",
    "    bpi_str = str(bpi.getvalue(), encoding=\"UTF8\")\n",
    "    bpi_blob = bucket.blob(bpi_gcs_path)\n",
    "    bpi_blob.upload_from_string(bpi_str)\n",
    "    \n",
    "    return (batch_prediction_input_file, csv_input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52768187-c334-451e-819d-e0d7fd1f5161",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create the Firestore component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "50ff240e-a81e-4504-8803-4ecbf2f8962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@component(packages_to_install=[\"Pillow\",\n",
    "                                \"google-cloud-firestore\",\n",
    "                                \"google-cloud-storage\",\n",
    "                                \"numpy\",\n",
    "                                \"pandas\"])\n",
    "def firestore(\n",
    "    subreddit_name: str,\n",
    "    collection_name: str,\n",
    "    gcs_bucket_name: str,\n",
    "    gcs_prefix_name: str,\n",
    "    csv_input_file: str,\n",
    "    project_id: str,\n",
    ") -> NamedTuple(\n",
    "    \"Outputs\",\n",
    "    [\n",
    "        (\"batch_predict_file_uri\", str),\n",
    "        (\"bp_inputs_count\", int),\n",
    "    ]\n",
    "):\n",
    "    \n",
    "    from datetime import datetime\n",
    "    import hashlib\n",
    "    from io import BytesIO\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    import re\n",
    "    import requests\n",
    "    import shutil\n",
    "\n",
    "    from google.cloud import firestore\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def make_nice_filename(name, *, rows=None, cols=None):\n",
    "        regex = \"[\\s|\\(|\\\"|\\)]\"\n",
    "        new_name = re.sub(regex, \"_\", name)\n",
    "        new_name = new_name.lower()[:30]\n",
    "        new_name = new_name.replace(\"__\", \"_\")\n",
    "        \n",
    "        if rows is not None and cols is not None:\n",
    "            new_name += f\".{cols}x{rows}\"\n",
    "        return f\"{new_name}.jpg\"\n",
    "\n",
    "\n",
    "    def create_vtt_json(content, title):\n",
    "        img = Image.open(BytesIO(content))\n",
    "        w, h = img.size\n",
    "\n",
    "        dims = re.findall(\"\\d+x\\d+\", title)\n",
    "        if len(dims) is 0:\n",
    "            return None\n",
    "\n",
    "        dims = dims[0].split(\"x\")\n",
    "\n",
    "        if len(dims) is not 2:\n",
    "            return None\n",
    "\n",
    "        cols = int(dims[0])\n",
    "        rows = int(dims[1])\n",
    "\n",
    "        cell_w = w / rows\n",
    "        cell_h = h / cols\n",
    "        if cell_w != cell_h:\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            \"cols\": cols,\n",
    "            \"rows\": rows,\n",
    "            \"imageWidth\": w,\n",
    "            \"imageHeight\": h,\n",
    "            \"cellOffsetX\": 0,\n",
    "            'cellOffsetY': 0, \n",
    "            'cellWidth': cell_w, \n",
    "            'cellHeight': cell_h, \n",
    "        }\n",
    "\n",
    "    def compute_bboxes(vtt_data):\n",
    "        bboxes = []\n",
    "\n",
    "        cols = vtt_data[\"cols\"]\n",
    "        rows = vtt_data[\"rows\"]\n",
    "\n",
    "        for x in range(1, cols):\n",
    "            for y in range(1, rows):\n",
    "               x_min_tmp = vtt_data[\"cellOffsetX\"] + (vtt_data[\"cellWidth\"] * x) - 2\n",
    "               x_max_tmp = x_min_tmp + vtt_data[\"cellWidth\"] + 4\n",
    "               y_min_tmp = vtt_data[\"cellOffsetY\"] + (vtt_data[\"cellHeight\"] * y) - 2\n",
    "               y_max_tmp = y_min_tmp + vtt_data[\"cellHeight\"] + 4\n",
    "\n",
    "               x_min_train = x_min_tmp / vtt_data[\"imageWidth\"]\n",
    "               x_max_train = x_max_tmp / vtt_data[\"imageWidth\"]\n",
    "               y_min_train = y_min_tmp / vtt_data[\"imageHeight\"]\n",
    "               y_max_train = y_max_tmp / vtt_data[\"imageHeight\"]\n",
    "\n",
    "               bboxes.append({\n",
    "                   \"xMin\": x_min_train,\n",
    "                   \"yMin\": y_min_train,\n",
    "                   \"xMax\": x_max_train,\n",
    "                   \"yMax\": y_max_train,\n",
    "                   \"displayName\": \"cell\"\n",
    "               })\n",
    "\n",
    "        return bboxes\n",
    "\n",
    "    storage_client = storage.Client(project=project_id)\n",
    "    bucket = storage_client.bucket(gcs_bucket_name)\n",
    "\n",
    "    firestore_client = firestore.Client(project=project_id)\n",
    "    collection_ref = firestore_client.collection(collection_name)\n",
    "\n",
    "    blob = bucket.blob(csv_input_file)\n",
    "    csv_bytes = blob.download_as_string()\n",
    "    csv_buffer = BytesIO(csv_bytes)\n",
    "\n",
    "    jpg_df = pd.read_csv(csv_buffer)\n",
    "\n",
    "    hashes = [None] * len(jpg_df.index)\n",
    "    jpg_df.insert(1, \"HashId\", hashes, True)\n",
    "    jpg_df.insert(6, \"GcsURI\", hashes, True)\n",
    "\n",
    "    # Concatenate string of batch prediction inputs\n",
    "    bp_inputs = \"\"\n",
    "    bp_inputs_count = 0\n",
    "    \n",
    "    # Iterate over JPG URIs, download them in batches, convert to sha values\n",
    "    for i, r in jpg_df.iterrows():\n",
    "        jpg_url = r[\"URL\"]\n",
    "        title = r[\"Title\"]\n",
    "\n",
    "        req = requests.get(jpg_url, stream=True)\n",
    "        if req.status_code == 200:\n",
    "            req.raw.decode_content = True\n",
    "            sha1 = hashlib.sha1()\n",
    "            jpg_hash = sha1.update(req.content)\n",
    "            jpg_hash = sha1.hexdigest()\n",
    "\n",
    "            jpg_df[\"HashId\"][i] = jpg_hash\n",
    "            #print(f\"Index {i}, hash {jpg_hash}\")\n",
    "            hashes.append(jpg_hash)\n",
    "\n",
    "            # Try to fetch each document from Firestore. If it does not exist,\n",
    "            # overwrite and download the image.\n",
    "            doc_ref = collection_ref.document(jpg_hash)\n",
    "            doc = doc_ref.get()\n",
    "            if not doc.exists:\n",
    "\n",
    "                img_data = create_vtt_json(req.content, title)\n",
    "                \n",
    "                if img_data is not None:\n",
    "                    file_name = make_nice_filename(title,\n",
    "                                                   rows=img_data[\"rows\"],\n",
    "                                                   cols=img_data[\"cols\"])\n",
    "                else:\n",
    "                    file_name = make_nice_filename(title)\n",
    "                \n",
    "                img_gcs_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/{file_name}\"\n",
    "                blob_name = f\"{gcs_prefix_name}/{file_name}\"\n",
    "\n",
    "                file_blob = bucket.blob(blob_name)\n",
    "                image_buffer = BytesIO(req.content)\n",
    "\n",
    "                # Get image grid metadata\n",
    "                #img_data = create_vtt_json(req.content, title)\n",
    "                print(img_data)\n",
    "\n",
    "                file_blob.upload_from_file(BytesIO(req.content))\n",
    "\n",
    "                data = {\n",
    "                    u\"filename\": file_name,\n",
    "                    u\"gcsURI\": img_gcs_uri,\n",
    "                    u\"source\": gcs_prefix_name,\n",
    "                    u\"userId\": \"None\",\n",
    "                    u\"sourceUrl\": jpg_url,\n",
    "                }\n",
    "\n",
    "                if img_data is not None:\n",
    "                    bboxes = compute_bboxes(img_data)\n",
    "                    data[\"vtt\"] = img_data\n",
    "                    data[\"computedBBoxes\"] = bboxes\n",
    "\n",
    "                    doc_ref.set(data)\n",
    "                    print(f\"Set data: {data}\")\n",
    "                    bp_inputs += json.dumps({ \"content\": img_gcs_uri, \"mimeType\": \"image/jpeg\"})\n",
    "                    bp_inputs += \"\\n\"\n",
    "                    bp_inputs_count = bp_inputs_count + 1\n",
    "\n",
    "    # No fresh JPGs in this scraping; return empty string\n",
    "    if bp_inputs is \"\":\n",
    "        print(\"no inputs\")\n",
    "        return (\"\", 0)\n",
    "\n",
    "    print(f\"First ten: {jpg_df.head(10)}\")\n",
    "\n",
    "    # Save the batch_predict file\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\") \n",
    "    batch_predict_file_uri = f\"gs://{gcs_bucket_name}/{gcs_prefix_name}/bp_input_{timestamp}.jsonl\"\n",
    "\n",
    "    bp_blob_name = f\"{gcs_prefix_name}/bp_input_{timestamp}.jsonl\"\n",
    "    bp_blob = bucket.blob(bp_blob_name)\n",
    "\n",
    "    bp_blob.upload_from_string(bp_inputs)\n",
    "\n",
    "    return (batch_predict_file_uri, bp_inputs_count)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb86e8e",
   "metadata": {},
   "source": [
    "## Build a simple pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a33687d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"reddit-scraper-pipeline\",\n",
    "    description=\"Gets data from a subreddit\",\n",
    "    pipeline_root=f\"gs://{BUCKET}/pipeline_root\",\n",
    ")\n",
    "def reddit_pipeline(\n",
    "    collection_name: str = COLLECTION_NAME,\n",
    "    secret_name: str = \"reddit-api-key\",\n",
    "    subreddit_name: str = SUBREDDIT_NAME,\n",
    "    gcs_bucket: str = BUCKET,\n",
    "    gcs_prefix: str = GCS_PREFIX,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    location: str = LOCATION,\n",
    "    limit: int = LIMIT,\n",
    "    model_id: str = MODEL_ID,\n",
    "):\n",
    "    \n",
    "    # Get the images from Reddit\n",
    "    reddit_op = reddit(\n",
    "        secret_name=secret_name,\n",
    "        subreddit_name=subreddit_name,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        gcs_prefix_name=gcs_prefix,\n",
    "        project_id=project_id,\n",
    "        limit=limit,\n",
    "    )\n",
    "    \n",
    "    reddit_csv_file = reddit_op.output\n",
    "    \n",
    "    # Store the new images on Cloud Storage\n",
    "    storage_op = storage(\n",
    "        project_id=project_id,\n",
    "        location_name=location,\n",
    "        gcs_bucket_name=gcs_bucket,\n",
    "        gcs_prefix_name=gcs_prefix,\n",
    "        collection_name=collection_name,\n",
    "        csv_input_file=reddit_csv_file,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1f32db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=reddit_pipeline, package_path=\"artifacts/reddit_scraper_pipeline_job.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "ff782324",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_client = AIPlatformClient(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e59c5-f05a-4959-919c-1b3f3d2a2e7d",
   "metadata": {},
   "source": [
    "When we run the pipeline, we don't want it to cache the pipeline, since caching the pipeline will likely result in producing the exact same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "cf97982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/reddit-scraper-pipeline-20221216211250?project=fantasymaps-334622\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    job_spec_path=\"artifacts/reddit_scraper_pipeline_job.json\",\n",
    "    enable_caching=True  # Change to False when needing to generate new values per job run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f101e-80df-41a3-a433-35bc559fd973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
