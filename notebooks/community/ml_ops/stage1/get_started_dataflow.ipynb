{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 1 : data management: get started with Dataflow\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage1/get_started_dataflow.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage1/get_started_dataflow.ipynb\">\n",
    "      Open in Google Cloud Notebooks\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 1 : data management: get started with Dataflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:gsod,lrg",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). The version of the dataset you use only the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage1,get_started_dataflow",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Dataflow` for training with `Vertex AI`.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Dataflow`\n",
    "- `BigQuery Datasets`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Offline preprocessing of data:\n",
    "    - Serially - w/o dataflow\n",
    "    - Parallel - with dataflow\n",
    "- Upstream preprocessing of data:\n",
    "    - tabular data\n",
    "    - image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommendation:mlops,stage1,dataflow",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Recommendations\n",
    "\n",
    "When doing E2E MLOps on Google Cloud, the following best practices for preprocessing and feeding data during training of custom models:\n",
    "\n",
    "#### Preprocessing\n",
    "\n",
    "Data is preprocessed either:\n",
    "\n",
    "- Offline: The data is preprocessed and stored prior to training.\n",
    "    - Small datasets: reprocessed and stored when new data.\n",
    "- Upstream: The data is preprocessed upstream from the model while the data is feed for training.\n",
    "    - Training on a CPU.\n",
    "- Downstream: The data is preprocessed downstream in the model while the data is feed for training.\n",
    "    - Training on a HW accelerator (e.g., GPU/TPU).\n",
    "\n",
    "#### Model Feeding\n",
    "\n",
    "Data is feed for model feeding either:\n",
    "\n",
    "- In-memory: small dataset.\n",
    "- From disk: large dataset, quick training.\n",
    "- `Dataflow` from disk: massive dataset, extended training.\n",
    "\n",
    "#### AutoML\n",
    "\n",
    "For AutoML training, preprocessing and model feeding are automatically handled.\n",
    "\n",
    "Alternately for AutoML tabular model training, you can reconfigure the otherwise default preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = '--user'\n",
    "else:\n",
    "    USER_FLAG = ''\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_dataflow",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Install the latest GA version of *Dataflow* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dataflow",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U apache-beam[gcp] $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Install the latest GA version of *BigQuery* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U google-cloud-bigquery $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_tfdv",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Install the latest GA version of *TensorFlow Data Validation* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tfdv",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U tensorflow-data-validation $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Install the latest GA version of *TensorFlow Transform* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U tensorflow-transform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_beam",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import Apache Beam\n",
    "\n",
    "Import the Apache Beam package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_beam",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import BigQuery\n",
    "\n",
    "Import the BigQuery package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_pandas",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import pandas\n",
    "\n",
    "Import the pandas package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_pandas",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_numpy",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import pandas\n",
    "\n",
    "Import the numpy package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_numpy",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tfdv",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import TFDV\n",
    "\n",
    "Import the TensorFlow Data Validation (TFDV) package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tfdv",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import TensorFlow Transform\n",
    "\n",
    "Import the TensorFlow Transform (TFT) package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create BigQuery client\n",
    "\n",
    "Create the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "offline_preprocess:bq",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Offline preprocessing data with BigQuery table using pandas dataframe\n",
    "\n",
    "- Offline: The BigQuery table is preprocessed in-memory and stored prior to training.\n",
    "\n",
    "    - Extract the tabular data into a pandas dataframe.\n",
    "    - Preprocess the data, per column, within the dataframe.\n",
    "    - Write the preprocessed dataframe to a new BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:gsod,bq,lrg",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
    "BQ_TABLE = 'bigquery-public-data.samples.gsod'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_to_dataframe:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Read the BigQuery dataset into a pandas dataframe\n",
    "\n",
    "Next, you read a sample of the dataset into a pandas dataframe using BigQuery `list_rows()` and `to_dataframe()` method, as follows:\n",
    "\n",
    "- `list_rows()`: Performs a query on the specified table and returns a row iterator to the query results. Optionally specify:\n",
    " - `selected_fields`: Subset of fields (columns) to return.\n",
    " - `max_results`: The maximum number of rows to return. Same as SQL LIMIT command.\n",
    "\n",
    "\n",
    "- `rows.to_dataframe()`: Invokes the row iterator and reads in the data into a pandas dataframe.\n",
    "\n",
    "Learn more about [Loading BigQuery table into a dataframe](https://cloud.google.com/bigquery/docs/bigquery-storage-python-pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq_to_dataframe:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Download a table.\n",
    "table = bigquery.TableReference.from_string(\n",
    "    \"bigquery-public-data.samples.gsod\"\n",
    ")\n",
    "\n",
    "rows = bqclient.list_rows(\n",
    "    table,\n",
    "    max_results=500,\n",
    "    selected_fields=[\n",
    "        bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "dataframe = rows.to_dataframe()\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataframe_transform:gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "categories = dataframe['station_number'].unique()\n",
    "\n",
    "one_hot = pd.get_dummies(categories)\n",
    "dataframe['station_number'] = one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataframe_to_bq:transformed,gsod",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    # Specify a (partial) schema. All columns are always written to the\n",
    "    # table. The schema is used to assist in data type definitions.\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"station_number\", \"FLOAT\"),  # <-- after one hot encoding\n",
    "        bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "    ],\n",
    "    # Optionally, set the write disposition. BigQuery appends loaded rows\n",
    "    # to an existing table by default, but with WRITE_TRUNCATE write\n",
    "    # disposition it replaces the table with the loaded data.\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")\n",
    "\n",
    "NEW_BQ_TABLE = f'{PROJECT_ID}.samples.{DATASET_ALIAS}_transformed'\n",
    "\n",
    "job = bqclient.load_table_from_dataframe(\n",
    "    dataframe, NEW_BQ_TABLE, job_config=job_config\n",
    ")  # Make an API request.\n",
    "job.result()  # Wait for the job to complete.\n",
    "\n",
    "table = bqclient.get_table(NEW_BQ_TABLE)  # Make an API request.\n",
    "print(\n",
    "    \"Loaded {} rows and {} columns to {}\".format(\n",
    "        table.num_rows, len(table.schema), NEW_BQ_TABLE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upstream_preprocess:image",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Upstream preprocessing data with tf.data.Dataset generator\n",
    "\n",
    "### Image data\n",
    "\n",
    "- Upstream: The data is preprocessed upstream from the model while the data is feed for training.\n",
    "\n",
    "    - Define preprocessing function:\n",
    "        - Input: unprocessed batch of tensors\n",
    "        - Output: preprocessed batch of tensors\n",
    "    - Use tf.data.Dataset `map()` method to map the preprocessing function to the generator output.\n",
    "\n",
    "In this example:\n",
    "\n",
    "- Load CIFAR10 dataset into memory as numpy arrays.\n",
    "- Create a tf.data.Dataset generator for the in-memory CIFAR10 dataset. *Note*: The pixel data is casted to FLOAT32 to be compatiable with the preprocessing function which outputs the pixel data as FLOAT32.\n",
    "- Define a preprocessing function to rescale the pixel data by 1/255.0\n",
    "- Map the preprocessing function to the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upstream_preprocess:image",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train))\n",
    "\n",
    "print(\"Before preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "def preprocess_fn(inputs, labels):\n",
    "    inputs /= 255.0\n",
    "    return tf.cast(inputs, tf.float32), labels\n",
    "\n",
    "tf_dataset = tf_dataset.map(preprocess_fn)\n",
    "\n",
    "print(\"After preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upstream_preprocess:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Upstream preprocessing data with tf.data.Dataset generator\n",
    "\n",
    "### Tabular data\n",
    "\n",
    "- Upstream: The data is preprocessed upstream from the model while the data is feed for training.\n",
    "\n",
    "    - Define preprocessing function:\n",
    "        - Input: unprocessed batch of tensors\n",
    "        - Output: preprocessed batch of tensors\n",
    "    - Use tf.data.Dataset `map()` method to map the preprocessing function to the generator output.\n",
    "\n",
    "In this example:\n",
    "\n",
    "- Blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upstream_preprocess:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((x_train.astype(np.float32), y_train))\n",
    "\n",
    "print(\"Before preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break\n",
    "\n",
    "def preprocessing_fn(inputs, labels):\n",
    "    inputs = tft.scale_to_0_1(inputs)\n",
    "    return tf.cast(inputs, tf.float32), labels\n",
    "\n",
    "tf_dataset = tf_dataset.map(preprocessing_fn)\n",
    "\n",
    "print(\"After preprocessing\")\n",
    "for batch in tf_dataset:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "offline_preprocess:dataflow",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Offline preprocessing with Dataflow\n",
    "\n",
    "- Generate data chema from BigQuery table.\n",
    "- Define Beam pipeline to:\n",
    "    - Split data from BigQuery table into train and eval datasets.\n",
    "    - Encode datasets as TFRecords, using the data schema.\n",
    "    - Save the TFRecords as compressed files to Cloud Storage\n",
    "- Run the Beam pipeline using Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tfdv_schema",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "###  Generate the raw data schema\n",
    "\n",
    "Generate the data schema on the dataset with the TensorFlow Data Validation (TFDV) package. Use the `infer_schema()` method, with the following parameters:\n",
    "\n",
    "- `statistics`: The statistics generated by TFDV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfdv_schema:write",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "SCHEMA_LOCATION = os.path.join(BUCKET_NAME, \"schema.txt\")\n",
    "tfdv.write_schema_text(output_path=SCHEMA_LOCATION, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataflow:split,bq",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Preprocess data with Dataflow\n",
    "\n",
    "#### Dataset splitting\n",
    "\n",
    "Next, you preprocess the data using Dataflow. In this example, you query the BigQuery table and split the examples into training and evaluation datasets. For expendiency, the number of examples from the dataset is limited to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataflow:split,bq",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "def parse_bq_record(bq_record):\n",
    "    \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
    "    output = {}\n",
    "    for key in bq_record:\n",
    "        output[key] = [bq_record[key]]\n",
    "    return output\n",
    "\n",
    "\n",
    "def split_dataset(bq_row, num_partitions, ratio):\n",
    "    \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
    "    import json\n",
    "\n",
    "    assert num_partitions == len(ratio)\n",
    "    bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
    "    total = 0\n",
    "    for i, part in enumerate(ratio):\n",
    "        total += part\n",
    "        if bucket < total:\n",
    "            return i\n",
    "    return len(ratio) - 1\n",
    "\n",
    "def run_pipeline(args):\n",
    "    \"\"\"Runs a Beam pipeline to split the dataset\"\"\"\n",
    "\n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **args)\n",
    "\n",
    "    raw_data_query = args[\"raw_data_query\"]\n",
    "    write_raw_data = args[\"write_raw_data\"]\n",
    "    exported_data_prefix = args[\"exported_data_prefix\"]\n",
    "    temp_location = args[\"temp_location\"]\n",
    "    project = args[\"project\"]\n",
    "\n",
    "    source_raw_schema = tfdv.load_schema_text(SCHEMA_LOCATION)\n",
    "    raw_feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(\n",
    "        source_raw_schema\n",
    "    ).feature_spec\n",
    "\n",
    "    raw_metadata = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
    "        tft.tf_metadata.schema_utils.schema_from_feature_spec(raw_feature_spec)\n",
    "    )\n",
    "\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        with tft_beam.Context(temp_location):\n",
    "\n",
    "            # Read raw BigQuery data.\n",
    "            raw_train_data, raw_eval_data = (\n",
    "                pipeline\n",
    "                | \"Read Raw Data\"\n",
    "                >> beam.io.ReadFromBigQuery(\n",
    "                    query=raw_data_query,\n",
    "                    project=project,\n",
    "                    use_standard_sql=True,\n",
    "                )\n",
    "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
    "                | \"Split\" >> beam.Partition(split_dataset, 2, ratio=[8, 2])\n",
    "            )\n",
    "\n",
    "            _ = (\n",
    "                raw_train_data\n",
    "                | \"Write Raw Train Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(\n",
    "                        exported_data_prefix, \"train/\"\n",
    "                    ),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            _ = (\n",
    "                raw_eval_data\n",
    "                | \"Write Raw Eval Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(\n",
    "                        exported_data_prefix, \"eval/\"\n",
    "                    ),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "EXPORTED_DATA_PREFIX = os.path.join(BUCKET_NAME, 'exported_data')\n",
    "\n",
    "QUERY_STRING = \"SELECT {0},{1} FROM {2} LIMIT 500\".format(\"station_number,year,month,day\", \"mean_temp\", IMPORT_FILE[5:])\n",
    "JOB_NAME = \"gsod\" + TIMESTAMP\n",
    "\n",
    "args = {\n",
    "    'runner': 'DirectRunner',\n",
    "    'raw_data_query': QUERY_STRING,\n",
    "    'write_raw_data': True,\n",
    "    'exported_data_prefix': EXPORTED_DATA_PREFIX,\n",
    "    'temp_location': os.path.join(BUCKET_NAME, 'temp'),\n",
    "    'project': PROJECT_ID\n",
    "}\n",
    "\n",
    "print('Data preprocessing started...')\n",
    "run_pipeline(args)\n",
    "print('Data preprocessing completed.')\n",
    "\n",
    "! gsutil ls $EXPORTED_DATA_PREFIX/train\n",
    "! gsutil ls $EXPORTED_DATA_PREFIX/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- AutoML Training Job\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if 'dataset' in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if 'model' in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if 'endpoint' in globals():\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline trainig job\n",
    "    try:\n",
    "        if 'dag' in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom trainig job\n",
    "    try:\n",
    "        if 'job' in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if 'batch_predict_job' in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if 'hpt_job' in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if 'BUCKET_NAME' in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vars": {
   "AIP": "AI Platform",
   "AUTOML": "AutoML",
   "AUTOML_URL": "https://cloud.google.com/vertex-ai/docs/start/automl-users",
   "BQ": "BigQuery",
   "COLAB": "Colab",
   "DATASET_ALIAS": "gsod",
   "DATASET_NAME": "NOAA historical weather data",
   "DOCKER": "Docker",
   "EDGE": "Edge",
   "EMAIL": "cdpe@gmail.com",
   "FEATURE_COLUMNS": "station_number,year,month,day",
   "GAPIC": "client library",
   "GAPICP": "SDK",
   "GCONSOLE": "Cloud Console",
   "GCP": "Google Cloud",
   "GCS": "Cloud Storage",
   "GNOTEBOOK": "Google Cloud Notebook",
   "LABEL_COLUMN": "mean_temp",
   "NOTEBOOK": "ml_ops_stage1/get_started_dataflow.ipynb",
   "REGION": "us-central1",
   "REPO": "vertex-ai-samples/tree/master/notebooks/official/automl",
   "SDKP": "SDK for Python",
   "STAGE": "1 : data management: get started with Dataflow",
   "TENSORFLOW": "TensorFlow",
   "TFLite": "TFLite",
   "TFServing": "TF Serving",
   "TIME_COLUMN": "date",
   "TIME_SERIES_ID_COLUMN": "county",
   "TITLE": "E2E ML on GCP: MLOps stage 1 : data management: get started with Dataflow",
   "VERTEX": "Vertex",
   "VERTEX_AI": "Vertex AI",
   "YEAR": "2021",
   "null": "null",
   "uCAIP": "Vertex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
