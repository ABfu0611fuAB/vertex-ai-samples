{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 2 : experimentation\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage2/mlops_experimentation.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage2/mlops_experimentation.ipynb\">\n",
    "      Open in Google Cloud Notebooks\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 2 : experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:bq,chicago,lbn",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [Chicago Taxi](https://www.kaggle.com/chicago/chicago-taxi-trips-bq). The version of the dataset you will use in this tutorial is stored in a public BigQuery table. The trained model predicts whether someone would leave a tip for a taxi fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you create a MLOps stage 2: experimentation process.\n",
    "\n",
    "This tutorial uses the following Vertex AI:\n",
    "\n",
    "- `Vertex Datasets`\n",
    "- `Vertex AutoML`\n",
    "- `Vertex Training`\n",
    "- `Vertex TensorBoard`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Review the `Dataset` resource created during stage 1.\n",
    "- Train an AutoML tabular binary classifier model in the background.\n",
    "- Construct a custom training job for the `Dataset` resource.\n",
    "- ?? Hyperparameter Tuning\n",
    "- Train the custom model.\n",
    "- Evaluate the custom model.\n",
    "- ?? Tensorboard\n",
    "- Wait for AutoML training job to complete.\n",
    "- Evaluate the AutoML model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "## Installation\n",
    "\n",
    "Install the latest version of Vertex SDK for Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Google Cloud Notebook\n",
    "if os.path.exists(\"/opt/deeplearning/metadata/env_version\"):\n",
    "    USER_FLAG = '--user'\n",
    "else:\n",
    "    USER_FLAG = ''\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform $USER_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tensorflow",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if os.environ[\"IS_TESTING\"]:\n",
    "    ! pip3 install --upgrade tensorflow $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Install the latest GA version of *TensorFlow Transform* library as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! pip3 install -U tensorflow-transform $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tf",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import TensorFlow\n",
    "\n",
    "Import the TensorFlow package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tf",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import TensorFlow Transform\n",
    "\n",
    "Import the TensorFlow Transform (TFT) package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tft",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Initialize Vertex SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk,region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "find_dataset:bq",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Retrieve the dataset from stage 1\n",
    "\n",
    "Next, retrieve the dataset you created during stage 1 with the helper function `find_dataset()`. This helper function finds all the datasets whose display name matches the specified prefix and import format (e.g., bq). Finally it sorts the matches by create time and returns the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "find_dataset:bq",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "def find_dataset(display_name_prefix, import_format):\n",
    "    matches=[]\n",
    "    datasets = aip.TabularDataset.list()\n",
    "    for dataset in datasets:\n",
    "        if dataset.display_name.startswith(display_name_prefix):\n",
    "            try:\n",
    "                if \"bq\" == import_format and dataset.to_dict()['metadata']['inputConfig']['bigquerySource']:\n",
    "                    matches.append(dataset)\n",
    "                if \"csv\" == import_format and dataset.to_dict()['metadata']['inputConfig']['gcsSource']:\n",
    "                    matches.append(dataset)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    create_time = None\n",
    "    for match in matches:\n",
    "        if (create_time == None or\n",
    "            match.create_time > create_time):\n",
    "            create_time = match.create_time\n",
    "            dataset = match\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = find_dataset(\"Chicago Taxi\", \"bq\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_automl_pipeline:tabular,lbn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Create and run training pipeline\n",
    "\n",
    "To train an AutoML model, you perform two steps: 1) create a training pipeline, and 2) run the pipeline.\n",
    "\n",
    "#### Create training pipeline\n",
    "\n",
    "An AutoML training pipeline is created with the `AutoMLTabularTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the `TrainingJob` resource.\n",
    "- `optimization_prediction_type`: The type task to train the model for.\n",
    "  - `classification`: A tabuar classification model.\n",
    "  - `regression`: A tabular regression model.\n",
    "- `column_transformations`: (Optional): Transformations to apply to the input columns\n",
    "- `optimization_objective`: The optimization objective to minimize or maximize.\n",
    "  - binary classification:\n",
    "    - `minimize-log-loss`\n",
    "    - `maximize-au-roc`\n",
    "    - `maximize-au-prc`\n",
    "    - `maximize-precision-at-recall`\n",
    "    - `maximize-recall-at-precision`\n",
    "  - multi-class classification:\n",
    "    - `minimize-log-loss`\n",
    "  - regression:\n",
    "    - `minimize-rmse`\n",
    "    - `minimize-mae`\n",
    "    - `minimize-rmsle`\n",
    "\n",
    "The instantiated object is the DAG (directed acyclic graph) for the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_automl_pipeline:tabular,lbn",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "dag = aip.AutoMLTabularTrainingJob(\n",
    "    display_name=\"chicago_\" + TIMESTAMP,\n",
    "    optimization_prediction_type=\"classification\",\n",
    "    optimization_objective=\"minimize-log-loss\"\n",
    ")\n",
    "\n",
    "print(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_automl_pipeline:async,tabular",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "#### Run the training pipeline\n",
    "\n",
    "Next, you run the DAG to start the training job by invoking the method `run`, with the following parameters:\n",
    "\n",
    "- `dataset`: The `Dataset` resource to train the model.\n",
    "- `model_display_name`: The human readable name for the trained model.\n",
    "- `training_fraction_split`: The percentage of the dataset to use for training.\n",
    "- `test_fraction_split`: The percentage of the dataset to use for test (holdout data).\n",
    "- `validation_fraction_split`: The percentage of the dataset to use for validation.\n",
    "- `target_column`: The name of the column to train as the label.\n",
    "- `budget_milli_node_hours`: (optional) Maximum training time specified in unit of millihours (1000 = hour).\n",
    "- `disable_early_stopping`: If `True`, training maybe completed before using the entire budget if the service believes it cannot further improve on the model objective measurements.\n",
    "\n",
    "The `run` method when completed returns the `Model` resource.\n",
    "\n",
    "The execution of the training pipeline will take upto 180 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_automl_pipeline:async,tabular",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "async_model = dag.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name=\"chicago_\" + TIMESTAMP,\n",
    "    training_fraction_split=0.8,\n",
    "    validation_fraction_split=0.1,\n",
    "    test_fraction_split=0.1,\n",
    "    budget_milli_node_hours=8000,\n",
    "    disable_early_stopping=False,\n",
    "    target_column=\"tip_bin\",\n",
    "    sync=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "find_dataset:csv",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Retrieve the dataset from stage 1\n",
    "\n",
    "Next, retrieve the dataset you created during stage 1 with the helper function `find_dataset()`. This helper function finds all the datasets whose display name matches the specified prefix and import format (e.g., bq). Finally it sorts the matches by create time and returns the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "find_dataset:csv",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "def find_dataset(display_name_prefix, import_format):\n",
    "    matches=[]\n",
    "    datasets = aip.TabularDataset.list()\n",
    "    for dataset in datasets:\n",
    "        if dataset.display_name.startswith(display_name_prefix):\n",
    "            try:\n",
    "                if \"bq\" == import_format and dataset.to_dict()['metadata']['inputConfig']['bigquerySource']:\n",
    "                    matches.append(dataset)\n",
    "                if \"csv\" == import_format and dataset.to_dict()['metadata']['inputConfig']['gcsSource']:\n",
    "                    matches.append(dataset)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    create_time = None\n",
    "    for match in matches:\n",
    "        if (create_time == None or\n",
    "            match.create_time > create_time):\n",
    "            create_time = match.create_time\n",
    "            dataset = match\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = find_dataset(\"Chicago Taxi\", \"csv\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_dataset_user_metadata",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Load dataset's user metadata\n",
    "\n",
    "Load the user metadata for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_dataset_user_metadata",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with tf.io.gfile.GFile('gs://' + dataset.labels['user_metadata'] + \"/metadata.jsonl\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start_experiment",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create experiment for tracking training related metadata\n",
    "\n",
    "Setup tracking the parameters (inputs) and metrics (results) for each experiment:\n",
    "\n",
    "- `aip.init()` - Create an experiment instance\n",
    "- `aip.start_run()` - Track a specific run within the experiment.\n",
    "\n",
    "Learn more about [Vertex ML Metadata](https://cloud.google.com/vertex-ai/docs/ml-metadata/introduction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "start_experiment",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"chicago-\" + TIMESTAMP\n",
    "aip.init(experiment=EXPERIMENT_NAME)\n",
    "aip.start_run('run-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_input_layer:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create the input layer for custom model\n",
    "\n",
    "Next, you create the input layer for your custom tabular model, based on the data types of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_input_layer:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "def create_model_inputs(numeric_features=None, categorical_features=None):\n",
    "    inputs = {}\n",
    "    for feature_name in numeric_features:\n",
    "        inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.float32)\n",
    "    for feature_name in categorical_features:\n",
    "        inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.int64)\n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_input_layer:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "input_layers = create_model_inputs(numeric_features=metadata['numeric_features'],\n",
    "                                   categorical_features=metadata['categorical_features']\n",
    "                                  )\n",
    "\n",
    "print(input_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_binary_classifier:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create the binary classifier custom model\n",
    "\n",
    "Next, you create your binary classifier custom tabular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_binary_classifier:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate, experimental\n",
    "\n",
    "def create_binary_classifier(input_layers, tft_output, hyperparams, numeric_features, categorical_features):\n",
    "    layers = []\n",
    "    for feature_name in input_layers:\n",
    "        if feature_name in categorical_features:\n",
    "            vocab_size = tft_output.vocabulary_size_by_name(feature_name)\n",
    "            onehot_layer = experimental.preprocessing.CategoryEncoding(\n",
    "                max_tokens=vocab_size,\n",
    "                output_mode=\"binary\",\n",
    "                name=f\"{feature_name}_onehot\",\n",
    "            )(input_layers[feature_name])\n",
    "            layers.append(onehot_layer)\n",
    "        elif feature_name in numeric_features:\n",
    "            numeric_layer = tf.expand_dims(input_layers[feature_name], -1)\n",
    "            layers.append(numeric_layer)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    joined = Concatenate(name=\"combines_inputs\")(layers)\n",
    "    feedforward_output = Sequential(\n",
    "        [\n",
    "            Dense(units, activation=\"relu\")\n",
    "            for units in hyperparams[\"hidden_units\"]\n",
    "        ],\n",
    "        name=\"feedforward_network\",\n",
    "    )(joined)\n",
    "    logits = Dense(units=1, name=\"logits\")(feedforward_output)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[logits])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "make_binary_classifier:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "TRANSFORM_ARTIFACTS_DIR = metadata['transform_artifacts_dir']\n",
    "tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "\n",
    "hyperparams = {'hidden_units': [ 128, 64 ] }\n",
    "aip.log_params(hyperparams)\n",
    "\n",
    "model = create_binary_classifier(input_layers, tft_output, hyperparams,\n",
    "                                 numeric_features=metadata['numeric_features'],\n",
    "                                 categorical_features=metadata['categorical_features']\n",
    "                                )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize_model",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Visualize the model archirecture\n",
    "\n",
    "Next, visualize the architecture of the custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "visualize_model",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    show_shapes=True,\n",
    "    show_dtype=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "construct_training_package",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Construct the training package\n",
    "\n",
    "#### Package layout\n",
    "\n",
    "Before you start training, you will look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "  - other Python scripts\n",
    "\n",
    "The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n",
    "\n",
    "The file `trainer/task.py` is the Python script for executing the custom training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "construct_training_package",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Chicago Taxi tabular binary classifier\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transform_feature_spec",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Get feature specification for the preprocessed data\n",
    "\n",
    "Next, create the feature specification for the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "transform_feature_spec",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "print(transform_feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "read_tfrecords_func",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Load the transformed data into a tf.data.Dataset\n",
    "\n",
    "Next, you load the gzip TFRecords on Cloud Storage storage into a `tf.data.Dataset` generator. These functions are re-used when training the custom model using `Vertex Training`, so you save them to the python training package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "read_tfrecords_func",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/data.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, feature_spec, label_column, batch_size=200):\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "    Args:\n",
    "      file_pattern: input tfrecord file pattern.\n",
    "      feature_spec: a dictionary of feature specifications.\n",
    "      batch_size: representing the number of consecutive elements of returned\n",
    "        dataset to combine in a single batch\n",
    "    Returns:\n",
    "      A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=feature_spec,\n",
    "        label_key=label_column,\n",
    "        reader=_gzip_reader_fn,\n",
    "        num_epochs=1,\n",
    "        drop_final_batch=True,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "read_tfrecords",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from custom.trainer import data\n",
    "\n",
    "TRANSFORMED_DATA_PREFIX = metadata['transformed_data_prefix']\n",
    "LABEL_COLUMN = metadata['label_column']\n",
    "\n",
    "train_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/train/data-*.gz'\n",
    "val_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/val/data-*.gz'\n",
    "test_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/test/data-*.gz'\n",
    "\n",
    "for input_features, target in data.get_dataset(\n",
    "    train_data_file_pattern, transform_feature_spec, LABEL_COLUMN, batch_size=3).take(1):\n",
    "    for key in input_features:\n",
    "        print(f\"{key} {input_features[key].dtype}: {input_features[key].numpy().tolist()}\")\n",
    "    print(f\"target: {target.numpy().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_model_func",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "blah\n",
    "\n",
    "### Create training script\n",
    "\n",
    "blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model_func",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "%%writefile custom/trainer/train.py\n",
    "\n",
    "from custom.trainer import data\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "def compile(model, hyperparams):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams[\"learning_rate\"])\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
    "\n",
    "    model.compile(optimizer=optimizer,loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    hyperparams,\n",
    "    train_data_dir,\n",
    "    val_data_dir,\n",
    "    label_column,\n",
    "    transformed_feature_spec,\n",
    "    log_dir\n",
    "):\n",
    "\n",
    "    train_dataset = data.get_dataset(\n",
    "        train_data_dir,\n",
    "        transformed_feature_spec,\n",
    "        label_column,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    val_dataset = data.get_dataset(\n",
    "        val_data_dir,\n",
    "        transformed_feature_spec,\n",
    "        label_column,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    logging.info(\"Model training started...\")\n",
    "    history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=hyperparams[\"num_epochs\"],\n",
    "            validation_data=val_dataset\n",
    "    )\n",
    "\n",
    "    logging.info(\"Model training completed.\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_model_local",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Train the model locally\n",
    "\n",
    "Next, test the training package locally, by training with just a few epochs:\n",
    "\n",
    "- `num_epochs`: The number of epochs to pass to the training package.\n",
    "- `compile()`: Compile the model for training.\n",
    "- `train(): Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model_local",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from custom.trainer import train\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "hyperparams[\"learning_rate\"] = 0.001\n",
    "hyperparams[\"num_epochs\"] = 5\n",
    "hyperparams[\"batch_size\"] = 512\n",
    "\n",
    "aip.log_params(hyperparams)\n",
    "train.compile(model, hyperparams)\n",
    "train.train(model, hyperparams, train_data_file_pattern, val_data_file_pattern, LABEL_COLUMN, transform_feature_spec, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_model:async",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "model = async_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "automl_job_wait:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Wait for completion of AutoML training job\n",
    "\n",
    "Next, wait for the AutoML training job to complete. Alternatively, one can set the parameter `sync` to `True` in the `run()` method to block until the AutoML training job is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "automl_job_wait:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluate_the_model:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "## Review model evaluation scores\n",
    "After your model has finished training, you can review the evaluation scores for it.\n",
    "\n",
    "First, you need to get a reference to the new model. As with datasets, you can either use the reference to the model variable you created when you deployed the model or you can list all of the models in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evaluate_the_model:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "# Get model resource ID\n",
    "models = aip.Model.list(filter='display_name=chicago_' + TIMESTAMP)\n",
    "\n",
    "# Get a reference to the Model Service client\n",
    "client_options = {\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"}\n",
    "model_service_client = aip.gapic.ModelServiceClient(\n",
    "    client_options=client_options\n",
    ")\n",
    "\n",
    "model_evaluations = model_service_client.list_model_evaluations(parent=models[0].resource_name)\n",
    "model_evaluation = list(model_evaluations)[0]\n",
    "print(model_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- AutoML Training Job\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if 'dataset' in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if 'model' in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if 'endpoint' in globals():\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline trainig job\n",
    "    try:\n",
    "        if 'dag' in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom trainig job\n",
    "    try:\n",
    "        if 'job' in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if 'batch_predict_job' in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if 'hpt_job' in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if 'BUCKET_NAME' in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vars": {
   "AIP": "AI Platform",
   "AUTOML": "AutoML",
   "AUTOML_URL": "https://cloud.google.com/vertex-ai/docs/start/automl-users",
   "BQ": "BigQuery",
   "COLAB": "Colab",
   "DATASET_ALIAS": "chicago",
   "DATASET_NAME": "Chicago Taxi",
   "DATA_TYPE": "tabular",
   "DOCKER": "Docker",
   "EDGE": "Edge",
   "EMAIL": "cdpe@gmail.com",
   "FEATURE_COLUMNS": "{'id': 'dataset:bq,chicago,lbn', 'parameters': [{'DATASET_ALIAS': 'chicago', 'DATASET_NAME': 'Chicago Taxi', 'FEATURE_COLUMNS': ['trip_month', 'trip_day', 'trip_day_of_week', 'trip_hour', 'trip_seconds', 'trip_miles', 'payment_type', 'pickup_grid', 'dropoff_grid', 'euclidean', 'loc_cross'], 'LABEL_COLUMN': 'tip_bin'}], 'type': 'text'}",
   "GAPIC": "client library",
   "GAPICP": "SDK",
   "GCONSOLE": "Cloud Console",
   "GCP": "Google Cloud",
   "GCS": "Cloud Storage",
   "GNOTEBOOK": "Google Cloud Notebook",
   "IMPORT_FORMAT": "bq",
   "LABEL_COLUMN": "tip_bin",
   "MODEL_TYPE": "tabular binary classifier",
   "NOTEBOOK": "ml_ops_stage2/mlops_experimentation.ipynb",
   "REGION": "us-central1",
   "REPO": "vertex-ai-samples/tree/master/notebooks/official/automl",
   "SDKP": "SDK for Python",
   "STAGE": "2 : experimentation",
   "TENSORFLOW": "TensorFlow",
   "TFLite": "TFLite",
   "TFServing": "TF Serving",
   "TITLE": "E2E ML on GCP: MLOps stage 2 : experimentation",
   "TRAINING_BUDGET": "8000",
   "TRAINING_TIME": "180",
   "VERTEX": "Vertex",
   "VERTEX_AI": "Vertex AI",
   "YEAR": "2021",
   "null": "null",
   "uCAIP": "Vertex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
