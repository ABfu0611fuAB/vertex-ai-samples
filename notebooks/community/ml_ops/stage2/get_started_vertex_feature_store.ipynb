{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 2 : experimentation: get started with Feature Store\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_feature_store.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_feature_store.ipynb\">\n",
    "      Open in Google Cloud Notebooks\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 2 : experimentation: get started with Feature Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:movies,lbn,avro",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the Movie Recommendations. The version of the dataset you will use in this tutorial is stored in a public Cloud Storage bucket, in Avro format.\n",
    "\n",
    "The dataset predicts whether a persons will watch a movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,get_started_vertex_feature_store",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Vertex AI Feature Store` for when training and prediction with `Vertex AI`.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Feature Store`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Creating a Vertex AI `Featurestore` resource.\n",
    "    - Creating `EntityType` resources for the `Featurestore` resource.\n",
    "    - Creating `Feature` resources for each `EntityType` resource.\n",
    "- Import feature values (entity data items) into `Featurestore` resource.\n",
    "    - From a Cloud Storage location.\n",
    "    - From a pandas DataFrame.\n",
    "- Perform online serving from a `Featurestore` resource.\n",
    "- Perform batch serving from a `Featurestore` resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install *one time* the packages for executing the MLOps notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "ONCE_ONLY = False\n",
    "if ONCE_ONLY:\n",
    "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
    "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
    "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
    "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
    "    ! pip3 install --upgrade kfp $USER_FLAG\n",
    "    ! pip3 install --upgrade torchvision $USER_FLAG\n",
    "    ! pip3 install --upgrade rpy2 $USER_FLAG\n",
    "    ! pip3 install --upgrade python-tabulate $USER_FLAG\n",
    "    ! pip3 install -U opencv-python-headless==4.5.2.52 $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import BigQuery\n",
    "\n",
    "Import the BigQuery package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create BigQuery client\n",
    "\n",
    "Create the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_bq",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_intro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Introduction to Vertex AI Feature Store\n",
    "\n",
    "Let's assume you have a recommendation model that predicts a coupon to print on the back of a cash register receipt. Now, if that model was trained only on single transaction instances (what was bought and how much), then (in the past) you used an Apriori algorithm.\n",
    "\n",
    "But now we have historical data on the customer (say it's indexed by credit card number). Like total purchases to date, average purchase per transaction, frequency of purchase by product category, etc. We use this \"enriched data\" to train a recommender system.\n",
    "\n",
    "Now it's time to do a live prediction. You get a transaction from the cash register, but all it has is the credit card number and this transaction. It does not have the enriched data the model needs. During serving, the credit card number is used as an index to Feature Store to get the enriched data needed for the model.\n",
    "\n",
    "Next problem. Let's say the enriched data the model was trained on was timestamp June 1. This transaction is June 15. Assume that the user has made other transactions between June 1 and 15, and the enriched data has been continuously updated in Feature Store. But the model was trained on June 1st data. FeatureStore knows the version number and serves the June 1 version to the model (not the current June 15); otherwise, if you used June 15 data you have training-serving skew.\n",
    "\n",
    "Next problem, data drift. Things change, suddenly one day everybody is buying toilet paper! There is a significant change in the distribution of the current stored enriched data from the distribution that the deployed model was trained on. FeatureStore can detect changes/thresholds in distribution changes and trigger a notification for retraining the model.\n",
    "\n",
    "Learn more about [Vertex AI Feature Store API](https://cloud.google.com/vertex-ai/docs/featurestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_datamodel:movies",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Vertex AI Feature Store data model\n",
    "\n",
    "Vertex AI Feature Store organizes data with the following 3 important hierarchical concepts:\n",
    "\n",
    "        Featurestore -> EntityType -> Feature\n",
    "\n",
    "- `Featurestore`: the place to store your features\n",
    "- `EntityType`: under a `Featurestore`, an `EntityType` describes an object to be modeled, real one or virtual one.\n",
    "- `Feature`: under an `EntityType`, a `Feature` describes an attribute of the `EntityType`\n",
    "\n",
    "Learn more about [Vertex AI Feature Store data model](https://cloud.google.com/vertex-ai/docs/featurestore/concepts).\n",
    "\n",
    "In the movie prediction dataset, you create a `Featurestore` resource called movies. This `Featurestore` resource has 2 entity types:\n",
    "- `users`: The entity type has the `age`, `gender`, and `like genres` features.\n",
    "- `movies`: The entity type has the `genres` and `average rating` features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Create a `Featurestore` resource\n",
    "\n",
    "First, you create a `Featurestore` for the dataset using the `Featurestore.create()` method, with the following parameters:\n",
    "\n",
    "- `featurestore_id`: The name of the feature store.\n",
    "- `online_store_fixed_node_count`: Configuration settings for online serving from the feature store.\n",
    "- `project`: The project ID.\n",
    "- `location`: The location (region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Represents featurestore resource path.\n",
    "FEATURESTORE_NAME = \"movies\"\n",
    "\n",
    "featurestore = aip.Featurestore.create(\n",
    "    featurestore_id=FEATURESTORE_NAME,\n",
    "    online_store_fixed_node_count=1,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "print(featurestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_list",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### List your `Featurestore` resources\n",
    "\n",
    "You can get a list of all your `Featurestore` resources in your project using the `Featurestore.list()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_list",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "for featurestore in aip.Featurestore.list():\n",
    "    print(featurestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_get",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Get a `Featurestore` resource\n",
    "\n",
    "You can get a specifed `Featurestore` resource in your project using the `Featurestore()` initializer, with the following parameters:\n",
    "\n",
    "- `featurestore_name`: The name for the `Featurestore` resource.\n",
    "- `project`: The project ID.\n",
    "- `location`: The location (region)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_get",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "featurestore = featurestore = aip.Featurestore(\n",
    "    featurestore_name=FEATURESTORE_NAME,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION\n",
    ")\n",
    "print(featurestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create:entity_type",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Create entity types for your `Featurestore` resource\n",
    "\n",
    "Next, you create the `EntityType` resources for your `Featurestore` resource using the `create_entity_type()` method, with the following parameters:\n",
    "\n",
    "- `entity_type_id`: The name of the `EntityType` resource.\n",
    "- `description`:  A description of the entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create:entity_type",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "for name, description in [(\"users\", \"Users descrip\"), (\"movies\", \"Movies descrip\")]:\n",
    "    entity_type = featurestore.create_entity_type(\n",
    "        entity_type_id=name,\n",
    "        description=description\n",
    "    )\n",
    "    print(entity_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create:feature",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Add `Feature` resources for your `EntityType` resources\n",
    "\n",
    "Next, you create the `Feature` resources for each of the `EntityType` resources in your `Featurestore` resource using the `create_feature()` method, with the following parameters:\n",
    "\n",
    "- `feature_id`: The name of the `Feature` resource.\n",
    "- `description`: A description of the feature.\n",
    "- `value_type`: The data type for the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create:feature,movies",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "def create_features(featurestore_name, entity_name, features):\n",
    "    entity_type = aip.EntityType(\n",
    "        entity_type_name=entity_name,\n",
    "        featurestore_id=featurestore_name\n",
    "    )\n",
    "\n",
    "    for feature in features:\n",
    "        feature = entity_type.create_feature(\n",
    "            feature_id=feature[0],\n",
    "            description=feature[1],\n",
    "            value_type=feature[2]\n",
    "        )\n",
    "        print(feature)\n",
    "\n",
    "create_features(\n",
    "    FEATURESTORE_NAME,\n",
    "    \"users\",\n",
    "    [\n",
    "        (\"age\", \"Age descrip\", \"INT64\"),\n",
    "        (\"gender\", \"Gender descrip\", \"STRING\"),\n",
    "        (\"liked_genres\", \"Genres descrip\", \"STRING_ARRAY\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "create_features(\n",
    "    FEATURESTORE_NAME,\n",
    "    \"movies\",\n",
    "    [\n",
    "        (\"title\", \"Title descrip\", \"STRING\"),\n",
    "        (\"genres\", \"Genres descrip\", \"STRING\"),\n",
    "        (\"average_rating\", \"Ave descrip\", \"DOUBLE\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_list",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### List your `Featurestore` resources\n",
    "\n",
    "You can get a list of all your `Featurestore` resources in your project using the `Featurestore.list()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_list",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "for featurestore in aip.Featurestore.list():\n",
    "    print(featurestore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_list:filter",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Search `Feature` resources using a filter\n",
    "\n",
    "You can narrow your search of `Feature` resources using the method `list_features()` and specifying a `filter` filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_list:filter",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Search by data type\n",
    "features = []\n",
    "for entity_type in featurestore.list_entity_types():\n",
    "    features += entity_type.list_features(filter=\"value_type=DOUBLE\")\n",
    "print(\"By data type\")\n",
    "for feature in features:\n",
    "    print(features)\n",
    "\n",
    "# Search by data type\n",
    "features = []\n",
    "for entity_type in featurestore.list_entity_types():\n",
    "    _ = entity_type.list_features()\n",
    "    for feature in _:\n",
    "        if feature.name == 'title':\n",
    "            print(type(feature))\n",
    "            features += [feature]\n",
    "\n",
    "print(\"By Name\")\n",
    "for feature in features:\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_search:query",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Search `Feature` resources using a query\n",
    "\n",
    "You can narrow your search of `Feature` resources using the method `search()` and specifying a `query` filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_search:query",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "features = aip.Feature.search(query=\"value_type=DOUBLE\")\n",
    "print(\"By data type\")\n",
    "for feature in features:\n",
    "    print(features)\n",
    "\n",
    "aip.Feature.search(query=\"feature_id=title\")\n",
    "print(\"By Name\")\n",
    "for feature in features:\n",
    "    print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:movies,lbn,avro",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"gs://cloud-samples-data/vertex-ai/feature-store/datasets/movie_prediction.csv\"\n",
    "FS_ENTITIES = {'users': 'gs://cloud-samples-data/vertex-ai/feature-store/datasets/users.avro',\n",
    "               'movies': 'gs://cloud-samples-data-us-central1/vertex-ai/feature-store/datasets/movies.avro'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_import:movies,avro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Import the feature data into your `Featurestore` resource\n",
    "\n",
    "Next, you import the feature data for your `Featurestore` resource. Once imported, you can use these feature values for online and offline (batch) serving.\n",
    "\n",
    "### Data layout\n",
    "\n",
    "Each imported `EntityType` resource data must have an ID; also, each `EntityType` resource data item can optionally have a timestamp, sepecifying when the feature values were generated.\n",
    "\n",
    "When importing, specify the following in your request:\n",
    "\n",
    "- Data source format: BigQuery Table/Avro/CSV/Pandas Dataframe\n",
    "- Data source URL\n",
    "- Destination: featurestore/entity types/features to be imported\n",
    "\n",
    "The feature values for the movies dataset are in Avro format. The Avro schemas are as follows:\n",
    "\n",
    "**Users entity**:\n",
    "\n",
    "```\n",
    "schema = {\n",
    "  \"type\": \"record\",\n",
    "  \"name\": \"User\",\n",
    "  \"fields\": [\n",
    "      {\n",
    "       \"name\":\"user_id\",\n",
    "       \"type\":[\"null\",\"string\"]\n",
    "      },\n",
    "      {\n",
    "       \"name\":\"age\",\n",
    "       \"type\":[\"null\",\"long\"]\n",
    "      },\n",
    "      {\n",
    "       \"name\":\"gender\",\n",
    "       \"type\":[\"null\",\"string\"]\n",
    "      },\n",
    "      {\n",
    "       \"name\":\"liked_genres\",\n",
    "       \"type\":{\"type\":\"array\",\"items\":\"string\"}\n",
    "      },\n",
    "      {\n",
    "       \"name\":\"update_time\",\n",
    "       \"type\":[\"null\",{\"type\":\"long\",\"logicalType\":\"timestamp-micros\"}]\n",
    "      },\n",
    "  ]\n",
    " }\n",
    " ```\n",
    "\n",
    "**Movies entity**:\n",
    "\n",
    "```\n",
    "schema = {\n",
    " \"type\": \"record\",\n",
    " \"name\": \"Movie\",\n",
    " \"fields\": [\n",
    "     {\n",
    "      \"name\":\"movie_id\",\n",
    "      \"type\":[\"null\",\"string\"]\n",
    "     },\n",
    "     {\n",
    "      \"name\":\"average_rating\",\n",
    "      \"type\":[\"null\",\"double\"]\n",
    "     },\n",
    "     {\n",
    "      \"name\":\"title\",\n",
    "      \"type\":[\"null\",\"string\"]\n",
    "     },\n",
    "     {\n",
    "      \"name\":\"genres\",\n",
    "      \"type\":[\"null\",\"string\"]\n",
    "     },\n",
    "     {\n",
    "      \"name\":\"update_time\",\n",
    "      \"type\":[\"null\",{\"type\":\"long\",\"logicalType\":\"timestamp-micros\"}]\n",
    "     },\n",
    " ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Importing the feature values from Cloud Storage\n",
    "\n",
    "You import the feature values for the `EntityType` resources using the `ingest_from_gcs()` method, with the following parameters:\n",
    "\n",
    "- `entity_id_field`: The identifier name for the parent `EntityType` resource.\n",
    "- `feature_ids`: A list of identifier names for `Feature` resources' data to add to the `EntityType` resource.\n",
    "- `feature_time`: The field corresponding to the timestamp for the features being entered.\n",
    "- `gcs_source_type`: The format of the imported data. Must be CSV or Avro.\n",
    "- `gcs_source_uris`: A list of one or more Cloud Storage locations of the imported data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_import:movies,avro",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "entity_type = featurestore.get_entity_type(\"users\")\n",
    "response = entity_type.ingest_from_gcs(\n",
    "    entity_id_field=\"user_id\",\n",
    "    feature_ids=[\"age\", \"gender\", \"liked_genres\"],\n",
    "    feature_time=\"update_time\",\n",
    "    gcs_source_type=\"avro\",\n",
    "    gcs_source_uris=[FS_ENTITIES[\"users\"]]\n",
    ")\n",
    "print(response)\n",
    "\n",
    "entity_type = featurestore.get_entity_type(\"movies\")\n",
    "response = entity_type.ingest_from_gcs(\n",
    "    entity_id_field=\"movie_id\",\n",
    "    feature_ids=[\"title\", \"genres\", \"average_rating\"],\n",
    "    feature_time=\"update_time\",\n",
    "    gcs_source_type=\"avro\",\n",
    "    gcs_source_uris=[FS_ENTITIES[\"movies\"]]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_delete_entities:movies",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Delete the entity types and corresponding features and feature values\n",
    "\n",
    "Next, in preparation to repeat importing feature values from a dataframe, you first delete the existing entity types, and corresponding content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_delete_entities:movies",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "entity_type = featurestore.get_entity_type(\"users\")\n",
    "entity_type.delete(force=True)\n",
    "entity_type = featurestore.get_entity_type(\"movies\")\n",
    "entity_type.delete(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create:entity_type",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Create entity types for your `Featurestore` resource\n",
    "\n",
    "Next, you create the `EntityType` resources for your `Featurestore` resource using the `create_entity_type()` method, with the following parameters:\n",
    "\n",
    "- `entity_type_id`: The name of the `EntityType` resource.\n",
    "- `description`:  A description of the entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create:entity_type",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "for name, description in [(\"users\", \"Users descrip\"), (\"movies\", \"Movies descrip\")]:\n",
    "    entity_type = featurestore.create_entity_type(\n",
    "        entity_type_id=name,\n",
    "        description=description\n",
    "    )\n",
    "    print(entity_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_create:feature",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Add `Feature` resources for your `EntityType` resources\n",
    "\n",
    "Next, you create the `Feature` resources for each of the `EntityType` resources in your `Featurestore` resource using the `create_feature()` method, with the following parameters:\n",
    "\n",
    "- `feature_id`: The name of the `Feature` resource.\n",
    "- `description`: A description of the feature.\n",
    "- `value_type`: The data type for the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_create:feature,movies",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "def create_features(featurestore_name, entity_name, features):\n",
    "    entity_type = aip.EntityType(\n",
    "        entity_type_name=entity_name,\n",
    "        featurestore_id=featurestore_name\n",
    "    )\n",
    "\n",
    "    for feature in features:\n",
    "        feature = entity_type.create_feature(\n",
    "            feature_id=feature[0],\n",
    "            description=feature[1],\n",
    "            value_type=feature[2]\n",
    "        )\n",
    "        print(feature)\n",
    "\n",
    "create_features(\n",
    "    FEATURESTORE_NAME,\n",
    "    \"users\",\n",
    "    [\n",
    "        (\"age\", \"Age descrip\", \"INT64\"),\n",
    "        (\"gender\", \"Gender descrip\", \"STRING\"),\n",
    "        (\"liked_genres\", \"Genres descrip\", \"STRING_ARRAY\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "create_features(\n",
    "    FEATURESTORE_NAME,\n",
    "    \"movies\",\n",
    "    [\n",
    "        (\"title\", \"Title descrip\", \"STRING\"),\n",
    "        (\"genres\", \"Genres descrip\", \"STRING\"),\n",
    "        (\"average_rating\", \"Ave descrip\", \"DOUBLE\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:movies,lbn,df",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "GCS_USERS_AVRO_URI = FS_ENTITIES['users']\n",
    "GCS_MOVIES_AVRO_URI = FS_ENTITIES['movies']\n",
    "\n",
    "USERS_AVRO_FN = \"users.avro\"\n",
    "MOVIES_AVRO_FN = \"movies.avro\"\n",
    "\n",
    "! gsutil cp $GCS_USERS_AVRO_URI $USERS_AVRO_FN\n",
    "! gsutil cp $GCS_MOVIES_AVRO_URI $MOVIES_AVRO_FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_df_from_avro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Load Avro Files into pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_df_from_avro",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from avro.datafile import DataFileReader\n",
    "from avro.io import DatumReader\n",
    "\n",
    "\n",
    "class AvroReader:\n",
    "    def __init__(self, data_file):\n",
    "        self.avro_reader = DataFileReader(open(data_file, \"rb\"), DatumReader())\n",
    "\n",
    "    def to_dataframe(self):\n",
    "        records = [record for record in self.avro_reader]\n",
    "        return pd.DataFrame.from_records(data=records)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "users_avro_reader = AvroReader(data_file=USERS_AVRO_FN)\n",
    "users_source_df = users_avro_reader.to_dataframe()\n",
    "print(users_source_df)\n",
    "\n",
    "movies_avro_reader = AvroReader(data_file=MOVIES_AVRO_FN)\n",
    "movies_source_df = movies_avro_reader.to_dataframe()\n",
    "print(movies_source_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_import:movies,df",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Importing the feature values from DataFrame\n",
    "\n",
    "You import the feature values for the `EntityType` resources using the `ingest_from_df()` method, with the following parameters:\n",
    "\n",
    "- `entity_id_field`: The identifier name for the parent `EntityType` resource.\n",
    "- `feature_ids`: A list of identifier names for `Feature` resources' data to add to the `EntityType` resource.\n",
    "- `feature_time`: The field corresponding to the timestamp for the features being entered.\n",
    "- `df_source`: The DataFrame containing the imported feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_import:movies,df",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "entity_type = featurestore.get_entity_type(\"users\")\n",
    "entity_type.ingest_from_df(\n",
    "    feature_ids=[\"age\", \"gender\", \"liked_genres\"],\n",
    "    feature_time=\"update_time\",\n",
    "    df_source=users_source_df,\n",
    "    entity_id_field=\"user_id\",\n",
    ")\n",
    "\n",
    "entity_type = featurestore.get_entity_type(\"movies\")\n",
    "entity_type.ingest_from_df(\n",
    "    feature_ids=[\"average_rating\", \"title\", \"genres\"],\n",
    "    feature_time=\"update_time\",\n",
    "    df_source=movies_source_df,\n",
    "    entity_id_field=\"movie_id\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Vertex AI Feature Store serving\n",
    "\n",
    "The Vertex AI Feature Store service provides the following two services for serving features from a `Featurestore` resource:\n",
    "\n",
    "- Online serving - low-latency serving of small batches of features (prediction).\n",
    "\n",
    "- Batch serving - high-throughput serving of large batches of features (training and prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_serving:online,movies",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "def serve_features(featurestore, entity_name, features, id):\n",
    "    entity_type = featurestore.get_entity_type(entity_name)\n",
    "    return entity_type.read(\n",
    "        entity_ids=[id],\n",
    "        feature_ids=features\n",
    "    )\n",
    "\n",
    "features = serve_features(featurestore, \"users\", [\"age\", \"gender\", \"liked_genres\"], \"alice\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving:batch",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Batch Serving\n",
    "\n",
    "The Vertex AI Feature Store batch serving service is optimized for serving large batches of features in real-time with high-throughput, typically for training a model or batch prediction.\n",
    "\n",
    "One can batch serve to the following destinations:\n",
    "\n",
    "- BigQuery table\n",
    "- Cloud Storage location\n",
    "- Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving:batch,output,movies",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Output dataset\n",
    "\n",
    "For batch serving, you use a BigQuery table for the output. First, you must create this output destination table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_serving:batch,output,movies",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Output dataset\n",
    "DESTINATION_DATASET = f\"movies_predictions_{TIMESTAMP}\"\n",
    "\n",
    "# Output table.\n",
    "DESTINATION_TABLE = \"training_data\"  # @param {type:\"string\"}\n",
    "\n",
    "DESTINATION_TABLE_URI = f\"bq://{PROJECT_ID}.{DESTINATION_DATASET}.{DESTINATION_TABLE}\"\n",
    "\n",
    "dataset_id = f\"{PROJECT_ID}.{DESTINATION_DATASET}\"\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset = bqclient.create_dataset(dataset)\n",
    "print(\"Created dataset:\", dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_serving:batch,read,movies",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Batch Read Feature Values\n",
    "\n",
    "You batch serve entity data items to a BigQuery table using the `read_serve_to_bq()` method, with the following parameters:\n",
    "\n",
    "- `bq_destination_output_uri`: The destination BigQuery table to serve the features to.\n",
    "- `serving_feature_ids`: A dictionary of entity type and corresponding features to serve.\n",
    "- `read_instances_uri`: A Cloud Storage location to read the entity data items from.\n",
    "\n",
    "The output is stored in a BigQuery table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_serving:batch,read,movies",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "response = featurestore.batch_serve_to_bq(\n",
    "    bq_destination_output_uri=DESTINATION_TABLE_URI,\n",
    "    serving_feature_ids = {\n",
    "                    'users': [\"age\", \"gender\", \"liked_genres\"],\n",
    "                    'movies': [\"average_rating\", \"genres\"],\n",
    "                },\n",
    "    read_instances_uri=IMPORT_FILE\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_bq_dataset",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Delete a BigQuery dataset\n",
    "\n",
    "Use the method `delete_dataset()` to delete a BigQuery dataset along with all its tables, by setting the parameter `delete_contents` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_bq_dataset",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "bqclient.delete_dataset(dataset, delete_contents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "featurestore_delete",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Delete a `Featurestore` resource\n",
    "\n",
    "You can get a delete a specified `Featurestore` resource using the `delete()` method, with the following parameter:\n",
    "\n",
    "- `force`: A flag indicating whether to delete a non-empy `Featurestore` resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "featurestore_delete",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "featurestore.delete(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vars": {
   "AIP": "AI Platform",
   "AUTOML": "AutoML",
   "AUTOML_URL": "https://cloud.google.com/vertex-ai/docs/start/automl-users",
   "BQ": "BigQuery",
   "COLAB": "Colab",
   "DATASET_ALIAS": "movies",
   "DATASET_NAME": "Movie Recommendations",
   "DOCKER": "Docker",
   "EDGE": "Edge",
   "EMAIL": "cdpe@gmail.com",
   "GAPIC": "client library",
   "GAPICP": "SDK",
   "GCONSOLE": "Cloud Console",
   "GCP": "Google Cloud",
   "GCS": "Cloud Storage",
   "GNOTEBOOK": "Google Cloud Notebook",
   "IMPORT_FORMAT": "avro",
   "NOTEBOOK": "stage2/get_started_vertex_feature_store.ipynb",
   "REGION": "us-central1",
   "REPO": "vertex-ai-samples/blob/main/notebooks/community/ml_ops",
   "SDKP": "SDK for Python",
   "STAGE": "2 : experimentation: get started with Feature Store",
   "TENSORFLOW": "TensorFlow",
   "TFLite": "TFLite",
   "TFServing": "TF Serving",
   "TITLE": "E2E ML on GCP: MLOps stage 2 : experimentation: get started with Feature Store",
   "VERTEX": "Vertex AI",
   "VERTEX_AI": "Vertex AI",
   "YEAR": "2022",
   "null": "null",
   "uCAIP": "Vertex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
