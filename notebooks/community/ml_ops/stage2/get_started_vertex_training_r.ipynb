{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "copyright",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 2 : experimentation: get started with Vertex Training for R\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training_r.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage2/get_started_vertex_training_r.ipynb\">\n",
    "      Open in Google Cloud Notebooks\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 2 : experimentation: get started with Vertex Training for R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:r,iris,lcn",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the Iris dataset built into the R package. This dataset does not require any feature engineering. The trained model predicts the type of Iris flower species from a class of three species: setosa, virginica, or versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,get_started_vertex_training_r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `Vertex AI Training` for training a R custom model.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Training`\n",
    "- `Vertex AI Model` resource\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Locally train an R model in a notebook using %%R magic commands\n",
    "- Create a deployment image with trained R model and serving functions.\n",
    "- Test the deployment image locally.\n",
    "- Create a `Vertex AI Model` resource for the deployment image with embedded R model.\n",
    "- Deploy the deployment image with embedded R model to a `Vertex AI Endpoint` resource.\n",
    "- Test the deployment image with embedded R model.\n",
    "- Create a R-to-Python training package.\n",
    "- Create a training image for training the model.\n",
    "- Train a R model using `Vertex AI Trainingh` service with the R-to-Python training package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install *one time* the packages for executing the MLOps notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "ONCE_ONLY = False\n",
    "if ONCE_ONLY:\n",
    "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
    "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
    "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
    "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
    "    ! pip3 install --upgrade kfp $USER_FLAG\n",
    "    ! pip3 install --upgrade torchvision $USER_FLAG\n",
    "    ! pip3 install --upgrade rpy2 $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "set_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "autoset_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: andy-1234-221921\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "set_gcloud_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "autoset_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://andy-1234-221921aip-20220203205833/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "import_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "init_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set hardware accelerators\n",
    "\n",
    "You can set hardware accelerators for training and prediction.\n",
    "\n",
    "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
    "\n",
    "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "\n",
    "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
    "\n",
    "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
    "\n",
    "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "accelerators:training,cpu,prediction,cpu,mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, int(os.getenv(\"IS_TESTING_TRAIN_GPU\")))\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (None, None)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")))\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set machine type\n",
    "\n",
    "Next, set the machine type to use for training and prediction.\n",
    "\n",
    "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: The following is not supported for training:*\n",
    "\n",
    " - `standard`: 2 vCPUs\n",
    " - `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "machine:training,prediction",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = 'n1-standard'\n",
    "\n",
    "VCPU = '4'\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + '-' + VCPU\n",
    "print('Train machine type', TRAIN_COMPUTE)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = 'n1-standard'\n",
    "\n",
    "VCPU = '4'\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + '-' + VCPU\n",
    "print('Deploy machine type', DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_intro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Introduction to R training\n",
    "\n",
    "### Training a R model\n",
    "\n",
    "You can either train your model locally, or use the `Vertex AI Training` service. In the later case, you would install the R-to-Python interpreter `rpy2` into your training instance (e.g., setup.py) and execute the R training script using the R-to-Python interpreter `rpy2`.\n",
    "\n",
    "\n",
    "### Deploying a R model\n",
    "\n",
    "Deploying a R model on `Vertex AI Prediction` service requires to use a custom container that serves online predictions. In this tutorial, you deploy a container running plumber R package to serve predictions from trained model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_ipython",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Load the R notebook interpreter\n",
    "\n",
    "Loading the module `rpy2.ipython` will add support for %R and %RR magic cells in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "r_ipython",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "r_home is None. Try python -m rpy2.situation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-fb23c6edefe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rpy2.ipython'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2346\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2348\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/rpy2/ipython/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrmagic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mload_ipython_extension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmagic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_ipython_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/rpy2/ipython/rmagic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# numpy and rpy2 imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrinterface\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrinterface_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobjects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/rpy2/rinterface.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrinterface_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mopenrlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrinterface_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rinterface_capi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_rinterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrinterface_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedded\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/rpy2/rinterface_lib/openrlib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mrlib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen_rlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR_HOME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/rpy2/rinterface_lib/openrlib.py\u001b[0m in \u001b[0;36m_dlopen_rlib\u001b[0;34m(r_home)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"\"\"Open R's shared C library.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr_home\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         raise ValueError('r_home is None. '\n\u001b[0m\u001b[1;32m     17\u001b[0m                          'Try python -m rpy2.situation')\n\u001b[1;32m     18\u001b[0m     \u001b[0mlib_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msituation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: r_home is None. Try python -m rpy2.situation"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_install:randomForest",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Install and import some additional R packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_install:randomForest",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "install.packages(c(\"randomForest\", \"plumber\"), repos = \"http://cran.us.r-project.org\")\n",
    "\n",
    "library(ggplot2)\n",
    "library(randomForest)\n",
    "library(plumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick_peek:r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Quick peek at your data\n",
    "\n",
    "This tutorial uses a version of the iris dataset that is built into the R package.\n",
    "\n",
    "Start by doing a quick peek at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quick_peek:r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "head(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "custom_dir",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Make folder for R\n",
    "! rm -rf deploy custom\n",
    "! mkdir deploy custom custom/trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "locally_train:r,iris",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Locally train an R model\n",
    "\n",
    "First, you locally train the R model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "locally_train:r,iris",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "# train model\n",
    "model = randomForest(Species ~ ., data = iris)\n",
    "# save model\n",
    "save(model, file = \"deploy/model.RData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "serving_script_r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Serving script\n",
    "\n",
    "You create a R script for serving predictions. This script does the following:\n",
    "\n",
    "- Extracts prediction request values from the HTTP body of the input request.\n",
    "- Construct a prediction request for the R model.\n",
    "- Submit the prediction request to the R model.\n",
    "- Returns the predicted results.\n",
    "\n",
    "*Note*: Plumber makes use of comment “annotations” above functions to define the web service. When you feed the file into Plumber, you’ll get a runnable web service that other systems can interact with over a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "serving_script_r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy/serving.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy/serving.R\n",
    "# serving.R\n",
    "\n",
    "library(\"randomForest\")\n",
    "\n",
    "#* Health check\n",
    "#* @get /ping\n",
    "#* @serializer unboxedJSON\n",
    "function() {\n",
    "    list(status = \"OK\")\n",
    "}\n",
    "\n",
    "#* @apiTitle flower classifier\n",
    "#* @param petal_length\n",
    "#* @param petal_width\n",
    "#* @param sepal_length\n",
    "#* @param sepal_width\n",
    "#* @post /classify\n",
    "function (req)\n",
    "{\n",
    "    instances <- as.data.frame(jsonlite::fromJSON(req$postBody))\n",
    "    results <- list()\n",
    "\n",
    "    load(\"./model.RData\")\n",
    "\n",
    "    for(i in 1:nrow(instances)) {       # for-loop over columns\n",
    "        petal_length <- instances[i, \"instances.petal_length\"]\n",
    "        petal_width <- instances[i, \"instances.petal_width\"]\n",
    "        sepal_length <- instances[i, \"instances.sepal_length\"]\n",
    "        sepal_width <- instances[i, \"instances.sepal_width\"]\n",
    "        test = c(sepal_length, sepal_width, petal_length, petal_width)\n",
    "        test = sapply(test, as.numeric)\n",
    "        test = data.frame(matrix(test, ncol = 4))\n",
    "        colnames(test) = colnames(iris[, 1:4])\n",
    "        results <- append(results, predict(model, test))\n",
    "    }\n",
    "\n",
    "    list(predictions = results)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "start_server_r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Script for running the R server\n",
    "\n",
    "Next, you create a file that runs the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "start_server_r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy/startServer.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy/startServer.R\n",
    "library(plumber)\n",
    "pr <- plumb(\"serving.R\")\n",
    "pr$run(host = \"0.0.0.0\", port = 7080)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "docker_write,prediction,r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Make R container for prediction\n",
    "\n",
    "Currently, Vertex AI does not have a prefined container for making predictions with a deployed R model. No problem, you can assemble your own custom container. For this tutorial, you construct a deployment container from a Docker image as follows:\n",
    "\n",
    "    - Set the base image supplied by RStudio (rstudio/plumber).\n",
    "    - Package the model artifacts and serving scripts into a Docker image.\n",
    "    - Start the serving script on port 7080."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "docker_write,prediction,r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing deploy/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile deploy/Dockerfile\n",
    "\n",
    "FROM rstudio/plumber\n",
    "\n",
    "# install random forest\n",
    "RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
    "\n",
    "# Copy model and script\n",
    "RUN mkdir /app\n",
    "COPY model.RData /app\n",
    "COPY serving.R /app\n",
    "COPY startServer.R /app\n",
    "WORKDIR /app\n",
    "\n",
    "# plumber & run server\n",
    "EXPOSE 7080\n",
    "\n",
    "ENTRYPOINT [\"R\", \"-f\", \"/app/startServer.R\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "docker_push,prediction,r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/andy-1234-221921/r-predict-iris\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/9 : FROM rstudio/plumber\n",
      "latest: Pulling from rstudio/plumber\n",
      "\n",
      "\u001b[1B2f368469: Pulling fs layer \n",
      "\u001b[1B401695eb: Pulling fs layer \n",
      "\u001b[1B33e47f6e: Pulling fs layer \n",
      "\u001b[1B4233d960: Pulling fs layer \n",
      "\u001b[1Bd1dd96ee: Pulling fs layer \n",
      "\u001b[1B84b23979: Pulling fs layer \n",
      "\u001b[1Bfa0e97e9: Pulling fs layer \n",
      "\u001b[1B72b16f6a: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:c4be1a61787fef69f5bc8a086214cc8a10228fabb1f83e27e49611b853dcd6c2[9A\u001b[2K\u001b[7A\u001b[2K\u001b[5A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[3A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[4A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[9A\u001b[2K\u001b[7A\u001b[2K\u001b[1A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for rstudio/plumber:latest\n",
      " ---> 655c8301c1be\n",
      "Step 2/9 : RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
      " ---> Running in eb386ab2ac25\n",
      "\n",
      "R version 4.1.2 (2021-11-01) -- \"Bird Hippie\"\n",
      "Copyright (C) 2021 The R Foundation for Statistical Computing\n",
      "Platform: x86_64-pc-linux-gnu (64-bit)\n",
      "\n",
      "R is free software and comes with ABSOLUTELY NO WARRANTY.\n",
      "You are welcome to redistribute it under certain conditions.\n",
      "Type 'license()' or 'licence()' for distribution details.\n",
      "\n",
      "  Natural language support but running in an English locale\n",
      "\n",
      "R is a collaborative project with many contributors.\n",
      "Type 'contributors()' for more information and\n",
      "'citation()' on how to cite R or R packages in publications.\n",
      "\n",
      "Type 'demo()' for some demos, 'help()' for on-line help, or\n",
      "'help.start()' for an HTML browser interface to help.\n",
      "Type 'q()' to quit R.\n",
      "\n",
      "> install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")\n",
      "\u001b[91mInstalling package into ‘/usr/local/lib/R/site-library’\n",
      "(as ‘lib’ is unspecified)\n",
      "\u001b[0m\u001b[91mtrying URL 'https://cran.rstudio.com/src/contrib/randomForest_4.7-1.tar.gz'\n",
      "\u001b[0m\u001b[91mContent type 'application/x-gzip' length 80852 bytes (78 KB)\n",
      "=========\u001b[0m\u001b[91m===========\u001b[0m\u001b[91m==========\u001b[0m\u001b[91m=========\u001b[0m\u001b[91m===========\n",
      "downloaded 78 KB\n",
      "\n",
      "\u001b[0m\u001b[91m* installing *source* package ‘randomForest’ ...\n",
      "\u001b[0m\u001b[91m** package ‘randomForest’ successfully unpacked and MD5 sums checked\n",
      "\u001b[0m\u001b[91m** using staged installation\n",
      "\u001b[0m\u001b[91m** libs\n",
      "\u001b[0mgcc -I\"/usr/local/lib/R/include\" -DNDEBUG   -I/usr/local/include   -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c classTree.c -o classTree.o\n",
      "gcc -I\"/usr/local/lib/R/include\" -DNDEBUG   -I/usr/local/include   -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c init.c -o init.o\n",
      "gcc -I\"/usr/local/lib/R/include\" -DNDEBUG   -I/usr/local/include   -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c regTree.c -o regTree.o\n",
      "gcc -I\"/usr/local/lib/R/include\" -DNDEBUG   -I/usr/local/include   -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c regrf.c -o regrf.o\n",
      "gcc -I\"/usr/local/lib/R/include\" -DNDEBUG   -I/usr/local/include   -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rf.c -o rf.o\n",
      "gfortran -fno-optimize-sibling-calls  -fpic  -g -O2  -c rfsub.f -o rfsub.o\n",
      "gcc -I\"/usr/local/lib/R/include\" -DNDEBUG   -I/usr/local/include   -fpic  -g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -g  -c rfutils.c -o rfutils.o\n",
      "gcc -shared -L/usr/local/lib/R/lib -L/usr/local/lib -o randomForest.so classTree.o init.o regTree.o regrf.o rf.o rfsub.o rfutils.o -lgfortran -lm -lquadmath -L/usr/local/lib/R/lib -lR\n",
      "\u001b[91minstalling to /usr/local/lib/R/site-library/00LOCK-randomForest/00new/randomForest/libs\n",
      "\u001b[0m\u001b[91m** R\n",
      "\u001b[0m\u001b[91m** data\n",
      "\u001b[0m\u001b[91m** inst\n",
      "\u001b[0m\u001b[91m** byte-compile and prepare package for lazy loading\n",
      "\u001b[0m\u001b[91m** help\n",
      "\u001b[0m\u001b[91m*** installing help indices\n",
      "\u001b[0m\u001b[91m** building package indices\n",
      "\u001b[0m\u001b[91m** testing if installed package can be loaded from temporary location\n",
      "\u001b[0m\u001b[91m** checking absolute paths in shared objects and dynamic libraries\n",
      "\u001b[0m\u001b[91m** testing if installed package can be loaded from final location\n",
      "\u001b[0m\u001b[91m** testing if installed package keeps a record of temporary installation path\n",
      "\u001b[0m\u001b[91m* DONE (randomForest)\n",
      "\u001b[0m\u001b[91m\n",
      "The downloaded source packages are in\n",
      "\t‘/tmp/RtmpzvP3UJ/downloaded_packages’\n",
      "\u001b[0m> \n",
      "> \n",
      "Removing intermediate container eb386ab2ac25\n",
      " ---> ea267999b545\n",
      "Step 3/9 : RUN mkdir /app\n",
      " ---> Running in 1200f81447cc\n",
      "Removing intermediate container 1200f81447cc\n",
      " ---> 62a53af8ee89\n",
      "Step 4/9 : COPY model.RData /app\n",
      "COPY failed: stat /var/lib/docker/tmp/docker-builder305196348/model.RData: no such file or directory\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/andy-1234-221921/r-predict-iris]\n",
      "An image does not exist locally with the tag: gcr.io/andy-1234-221921/r-predict-iris\n"
     ]
    }
   ],
   "source": [
    "DEPLOY_IMAGE = f\"gcr.io/{PROJECT_ID}/r-predict-iris\"\n",
    "print(DEPLOY_IMAGE)\n",
    "\n",
    "! docker build --tag=$DEPLOY_IMAGE ./deploy\n",
    "\n",
    "! docker push $DEPLOY_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_docker_locally:launch,r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Locally test the Docker image\n",
    "\n",
    "Next, you locally test the Docker image you created for serving predictions.\n",
    "\n",
    "#### Launch the serving binary\n",
    "\n",
    "First, you launch the serving binary, listening on port 7080, and then ping it as a health check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "test_docker_locally:launch,r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find image 'gcr.io/andy-1234-221921/r-predict-iris:latest' locally\n",
      "latest: Pulling from andy-1234-221921/r-predict-iris\n",
      "\n",
      "\u001b[1B6ab2e44d: Pulling fs layer \n",
      "\u001b[1B085ace41: Pulling fs layer \n",
      "\u001b[1B80efcbfc: Pulling fs layer \n",
      "\u001b[1Bfdd53aff: Pulling fs layer \n",
      "\u001b[1Bec397a63: Pulling fs layer \n",
      "\u001b[1Bd732e19a: Pulling fs layer \n",
      "\u001b[1B4a477cd5: Pulling fs layer \n",
      "\u001b[1Bed5bfdf8: Pulling fs layer \n",
      "\u001b[1Ba1dc496e: Pulling fs layer \n",
      "\u001b[1Bb949682e: Pulling fs layer \n",
      "\u001b[1B1d733af6: Pulling fs layer \n",
      "\u001b[1B746380c0: Pulling fs layer \n",
      "\u001b[1B484e6c7d: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:96f18819c63442226b1028cd93ddbebb399593b80339830089a92b981ef99e16[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[7A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[9A\u001b[2K\u001b[14A\u001b[2K\u001b[14A\u001b[2K\u001b[3A\u001b[2K\u001b[12A\u001b[2K\u001b[2A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[14A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for gcr.io/andy-1234-221921/r-predict-iris:latest\n",
      "02e9b9caaaffdf27db16ac37cc8832d222cf26980a01286d761afdc0f72f5c9c\n",
      "{\"status\":\"OK\"}"
     ]
    }
   ],
   "source": [
    "! docker stop local_iris 2>/dev/null\n",
    "! docker run -t -d --rm -p 7080:7080 --name=local_iris $DEPLOY_IMAGE\n",
    "! sleep 10\n",
    "! curl http://localhost:7080/ping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_docker_locally:predict,r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Send predicton request\n",
    "\n",
    "Next, you send a prediction request to the serving binary you locally launched. Afterwards, you will shutdown the launched serving binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "test_docker_locally:predict,r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"predictions\":[\"setosa\",\"setosa\"]}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat > ./deploy/instances.json <<END\n",
    "{\n",
    "  \"instances\": [{\n",
    "      \"sepal_width\": 1,\n",
    "      \"sepal_length\": 2,\n",
    "      \"petal_width\": 3,\n",
    "      \"petal_length\": 1\n",
    "    },\n",
    "    {\n",
    "      \"sepal_width\": 4,\n",
    "      \"sepal_length\": 2,\n",
    "      \"petal_width\": 1,\n",
    "      \"petal_length\": 1\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "END\n",
    "\n",
    "curl -s -X POST \\\n",
    "  -H \"Content-Type: application/json; charset=utf-8\" \\\n",
    "  -d @./deploy/instances.json \\\n",
    "  http://localhost:7080/classify\n",
    "\n",
    "docker stop local_iris 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_model,r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Upload a R model to a `Vertex AI Model` resource\n",
    "\n",
    "Next you upload the R model to a Vertex AI Model resource, with the following parameters:\n",
    "\n",
    "- `display_name`: A human readable name for the model resource.\n",
    "- `serving_container_image_uri`: The deployment image that contains the serving binary and R model.\n",
    "- `serving_container_predict_route`: The URI endpoint for prediction requests.\n",
    "- `serving_container_health_route`: The URI endpoint for health ping.\n",
    "- `serving_container_ports`: A list of ports to listen on for prediction/health requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "upload_model,r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/759209241365/locations/us-central1/models/4683079507442139136/operations/1134432735549456384\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/759209241365/locations/us-central1/models/4683079507442139136\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/759209241365/locations/us-central1/models/4683079507442139136')\n"
     ]
    }
   ],
   "source": [
    "DISPLAY_NAME = \"iris_\" + TIMESTAMP\n",
    "health_route = \"/ping\"\n",
    "predict_route = \"/classify\"\n",
    "serving_container_ports = [7080]\n",
    "\n",
    "model = aip.Model.upload(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    serving_container_predict_route=predict_route,\n",
    "    serving_container_health_route=health_route,\n",
    "    serving_container_ports=serving_container_ports,\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:mbsdk,all",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "## Deploy the model\n",
    "\n",
    "Next, deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "If there are existing models on the endpoint, for which the traffic will be split, then use model_id to specify as { \"0\": percent, model_id: percent, ... }, where model_id is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "deploy_model:mbsdk,all",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/759209241365/locations/us-central1/endpoints/8869214943501615104/operations/6296402333446176768\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/759209241365/locations/us-central1/endpoints/8869214943501615104\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/759209241365/locations/us-central1/endpoints/8869214943501615104')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/759209241365/locations/us-central1/endpoints/8869214943501615104\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/759209241365/locations/us-central1/endpoints/8869214943501615104/operations/3540199361495433216\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/759209241365/locations/us-central1/endpoints/8869214943501615104\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"iris-\" + TIMESTAMP\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "if DEPLOY_GPU:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_GPU.name,\n",
    "        accelerator_count=DEPLOY_NGPU,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )\n",
    "else:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_deployed_model:iris",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Make test prediction\n",
    "\n",
    "Next, you test the deployed model by sending synthentic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "test_deployed_model:iris",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "ename": "Unknown",
     "evalue": "None Stream removed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Stream removed\"\n\tdebug_error_string = \"{\"created\":\"@1643923354.098666405\",\"description\":\"Error received from peer ipv4:74.125.195.95:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1074,\"grpc_message\":\"Stream removed\",\"grpc_status\":2}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mUnknown\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-98306713cbec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINSTANCES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, instances, parameters)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m         prediction_response = self._prediction_client.predict(\n\u001b[0;32m-> 1164\u001b[0;31m             \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gca_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1165\u001b[0m         )\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# Done; return the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnknown\u001b[0m: None Stream removed"
     ]
    }
   ],
   "source": [
    "INSTANCES = [\n",
    "    {\n",
    "        \"sepal_width\": 1,\n",
    "        \"sepal_length\": 2,\n",
    "        \"petal_width\": 3,\n",
    "        \"petal_length\": 1\n",
    "    },\n",
    "    {\n",
    "        \"sepal_width\": 4,\n",
    "        \"sepal_length\": 2,\n",
    "        \"petal_width\": 1,\n",
    "        \"petal_length\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "prediction = endpoint.predict(instances=INSTANCES)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "## Undeploy the model\n",
    "\n",
    "When you are done doing predictions, you undeploy the model from the `Endpoint` resouce. This deprovisions all compute resources and ends billing for the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "undeploy_model:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Undeploying Endpoint model: projects/759209241365/locations/us-central1/endpoints/8869214943501615104\n",
      "INFO:google.cloud.aiplatform.models:Undeploy Endpoint model backing LRO: projects/759209241365/locations/us-central1/endpoints/8869214943501615104/operations/7066517869726531584\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model undeployed. Resource name: projects/759209241365/locations/us-central1/endpoints/8869214943501615104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f1b79b12e50> \n",
       "resource name: projects/759209241365/locations/us-central1/endpoints/8869214943501615104"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "endpoint_delete:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "#### Delete the endpoint\n",
    "\n",
    "The method 'delete()' will delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "endpoint_delete:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting Endpoint : projects/759209241365/locations/us-central1/endpoints/8869214943501615104\n",
      "INFO:google.cloud.aiplatform.base:Delete Endpoint  backing LRO: projects/759209241365/locations/us-central1/operations/6478798118354681856\n",
      "INFO:google.cloud.aiplatform.base:Endpoint deleted. . Resource name: projects/759209241365/locations/us-central1/endpoints/8869214943501615104\n"
     ]
    }
   ],
   "source": [
    "endpoint.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_delete:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "#### Delete the model\n",
    "\n",
    "The method 'delete()' will delete the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "model_delete:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting Model : projects/759209241365/locations/us-central1/models/4683079507442139136\n",
      "INFO:google.cloud.aiplatform.base:Delete Model  backing LRO: projects/759209241365/locations/us-central1/operations/6989956676061233152\n",
      "INFO:google.cloud.aiplatform.base:Model deleted. . Resource name: projects/759209241365/locations/us-central1/models/4683079507442139136\n"
     ]
    }
   ],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents:iris,r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create the task script for the Python/R training package\n",
    "\n",
    "Next, you create the `task.py` script for driving the training package. Some noteable steps include:\n",
    "\n",
    "\n",
    "- Command-line arguments:\n",
    "    - `model-dir`: The location to save the trained model. When using Vertex AI custom training, the location will be specified in the environment variable: `AIP_MODEL_DIR`.\n",
    "\n",
    "\n",
    "- Training:\n",
    "    - Uses R-to-Python interpreter to run the R training script.\n",
    "\n",
    "\n",
    "- Model artifact saving:\n",
    "    - Saves the model artifacts at the Cloud Storage location specified by `model-dir`.\n",
    "    - *Note*: GCSFuse (`/gcs`) is used to do filesystem operations on Cloud Storage buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "taskpy_contents:iris,r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import rpy2\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "# import rpy2's package module\n",
    "import rpy2.robjects.packages as rpackages\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# import R's utility package\n",
    "randomForest = rpackages.importr('randomForest')\n",
    "\n",
    "# train the model\n",
    "logging.info(\"Model training started ...\")\n",
    "rpy2.robjects.r('''\n",
    "    # train model\n",
    "    model = randomForest(Species ~ ., data = iris)\n",
    "    # save model\n",
    "    save(model, file = \"model.RData\")\n",
    "'''\n",
    ")\n",
    "logging.info(\"Model training completed ...\")\n",
    "\n",
    "# GCSFuse conversion\n",
    "gs_prefix = 'gs://'\n",
    "gcsfuse_prefix = '/gcs/'\n",
    "if args.model_dir.startswith(gs_prefix):\n",
    "    args.model_dir = args.model_dir.replace(gs_prefix, gcsfuse_prefix)\n",
    "    dirpath = os.path.split(args.model_dir)[0]\n",
    "    if not os.path.isdir(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "# Upload the saved model file to Cloud Storage\n",
    "gcs_model_path = os.path.join(args.model_dir, 'model.RData')\n",
    "logging.info(\"Saving model artifacts to {}\". format(gcs_model_path))\n",
    "with open(\"model.RData\", \"rb\") as f:\n",
    "    data = f.read()\n",
    "with open(gcs_model_path, \"wb\") as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "docker_write,training,r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Make R container for training\n",
    "\n",
    "Currently, Vertex AI does not have a prefined container for training an R model. No problem, you can assemble your own custom container. For this tutorial, you construct a deployment container from a Docker image as follows:\n",
    "\n",
    "    - Set the base image to a TensorFlow Deep Learning image\n",
    "    - Install R-to-Python package and other R tools\n",
    "    - Install cloud storage package\n",
    "    - Package the model artifacts and serving scripts into a Docker image.\n",
    "    - Set the entry point in the container to run the training package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "docker_write,training,r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/Dockerfile\n",
    "\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3\n",
    "\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends build-essential r-base r-cran-randomforest python3.6 python3-pip python3-setuptools python3-dev\n",
    "RUN pip install google-cloud-storage\n",
    "RUN pip install rpy2\n",
    "\n",
    "# install random forest\n",
    "RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
    "\n",
    "WORKDIR /\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY trainer /trainer\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "docker_push,training,r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcr.io/andy-1234-221921/r-train-iris\n",
      "Sending build context to Docker daemon   5.12kB\n",
      "Step 1/8 : FROM gcr.io/deeplearning-platform-release/tf2-cpu.2-3\n",
      " ---> 06b564b6e868\n",
      "Step 2/8 : RUN apt-get update && apt-get install -y --no-install-recommends build-essential r-base r-cran-randomforest python3.6 python3-pip python3-setuptools python3-dev\n",
      " ---> Running in fb3bfd0b7281\n",
      "Get:1 http://packages.cloud.google.com/apt gcsfuse-bionic InRelease [5388 B]\n",
      "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:3 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6786 B]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:5 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 Packages [816 B]\n",
      "Get:6 http://packages.cloud.google.com/apt cloud-sdk-bionic/main amd64 Packages [232 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.8 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [760 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2544 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1466 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2986 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [796 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2244 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.5 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [12.6 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.6 kB]\n",
      "Fetched 24.5 MB in 4s (6411 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "build-essential is already the newest version (12.4ubuntu1).\n",
      "python3-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.5).\n",
      "The following additional packages will be installed:\n",
      "  dh-python libexpat1-dev libpython3-dev libpython3.6 libpython3.6-dev\n",
      "  libpython3.6-minimal libpython3.6-stdlib python3-pkg-resources python3.6-dev\n",
      "  python3.6-minimal r-base-core r-cran-boot r-cran-class r-cran-cluster\n",
      "  r-cran-codetools r-cran-foreign r-cran-kernsmooth r-cran-lattice r-cran-mass\n",
      "  r-cran-matrix r-cran-mgcv r-cran-nlme r-cran-nnet r-cran-rpart\n",
      "  r-cran-spatial r-cran-survival r-recommended\n",
      "Suggested packages:\n",
      "  python-setuptools-doc python3.6-venv python3.6-doc binfmt-support ess\n",
      "  r-doc-info | r-doc-pdf r-mathlib r-base-html r-cran-rcolorbrewer\n",
      "Recommended packages:\n",
      "  r-base-html r-doc-html r-base-dev\n",
      "The following NEW packages will be installed:\n",
      "  dh-python libexpat1-dev libpython3-dev libpython3.6-dev python3-dev\n",
      "  python3-pkg-resources python3-setuptools python3.6-dev r-base r-base-core\n",
      "  r-cran-boot r-cran-class r-cran-cluster r-cran-codetools r-cran-foreign\n",
      "  r-cran-kernsmooth r-cran-lattice r-cran-mass r-cran-matrix r-cran-mgcv\n",
      "  r-cran-nlme r-cran-nnet r-cran-randomforest r-cran-rpart r-cran-spatial\n",
      "  r-cran-survival r-recommended\n",
      "The following packages will be upgraded:\n",
      "  libpython3.6 libpython3.6-minimal libpython3.6-stdlib python3.6\n",
      "  python3.6-minimal\n",
      "5 upgraded, 27 newly installed, 0 to remove and 44 not upgraded.\n",
      "Need to get 91.4 MB of archives.\n",
      "After this operation, 140 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6 amd64 3.6.9-1~18.04ubuntu1.6 [1414 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3.6 amd64 3.6.9-1~18.04ubuntu1.6 [203 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-stdlib amd64 3.6.9-1~18.04ubuntu1.6 [1712 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3.6-minimal amd64 3.6.9-1~18.04ubuntu1.6 [1609 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-minimal amd64 3.6.9-1~18.04ubuntu1.6 [534 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-python all 3.20180325ubuntu2 [89.2 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libexpat1-dev amd64 2.2.5-3ubuntu0.2 [122 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3.6-dev amd64 3.6.9-1~18.04ubuntu1.6 [44.9 MB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpython3-dev amd64 3.6.7-1~18.04 [7328 B]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3.6-dev amd64 3.6.9-1~18.04ubuntu1.6 [508 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-dev amd64 3.6.7-1~18.04 [1288 B]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-base-core amd64 3.4.4-1ubuntu1 [23.2 MB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-boot all 1.3-20-1.1 [613 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-cluster amd64 2.0.6-2build1 [502 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-foreign amd64 0.8.69-1build1 [228 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-mass amd64 7.3-49-1 [1100 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-kernsmooth amd64 2.23-15-3build1 [89.4 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-lattice amd64 0.20-35-1build1 [713 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-nlme amd64 3.1.131-3build1 [2186 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-matrix amd64 1.2-12-1 [2334 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-mgcv amd64 1.8-23-1 [2496 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-survival amd64 2.41-3-2build1 [5156 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-rpart amd64 4.1-13-1 [878 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-class amd64 7.3-14-2build1 [85.9 kB]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-nnet amd64 7.3-12-2build1 [110 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-spatial amd64 7.3-11-2build1 [127 kB]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-codetools all 0.2-15-1.1 [46.1 kB]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-recommended all 3.4.4-1ubuntu1 [2820 B]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-base all 3.4.4-1ubuntu1 [9312 B]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu bionic/universe amd64 r-cran-randomforest amd64 4.6-12-2 [142 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 91.4 MB in 6s (15.9 MB/s)\n",
      "(Reading database ... 88042 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libpython3.6_3.6.9-1~18.04ubuntu1.6_amd64.deb ...\n",
      "Unpacking libpython3.6:amd64 (3.6.9-1~18.04ubuntu1.6) over (3.6.9-1~18.04ubuntu1.4) ...\n",
      "Preparing to unpack .../01-python3.6_3.6.9-1~18.04ubuntu1.6_amd64.deb ...\n",
      "Unpacking python3.6 (3.6.9-1~18.04ubuntu1.6) over (3.6.9-1~18.04ubuntu1.4) ...\n",
      "Preparing to unpack .../02-libpython3.6-stdlib_3.6.9-1~18.04ubuntu1.6_amd64.deb ...\n",
      "Unpacking libpython3.6-stdlib:amd64 (3.6.9-1~18.04ubuntu1.6) over (3.6.9-1~18.04ubuntu1.4) ...\n",
      "Preparing to unpack .../03-python3.6-minimal_3.6.9-1~18.04ubuntu1.6_amd64.deb ...\n",
      "Unpacking python3.6-minimal (3.6.9-1~18.04ubuntu1.6) over (3.6.9-1~18.04ubuntu1.4) ...\n",
      "Preparing to unpack .../04-libpython3.6-minimal_3.6.9-1~18.04ubuntu1.6_amd64.deb ...\n",
      "Unpacking libpython3.6-minimal:amd64 (3.6.9-1~18.04ubuntu1.6) over (3.6.9-1~18.04ubuntu1.4) ...\n",
      "Selecting previously unselected package dh-python.\n",
      "Preparing to unpack .../05-dh-python_3.20180325ubuntu2_all.deb ...\n",
      "Unpacking dh-python (3.20180325ubuntu2) ...\n",
      "Selecting previously unselected package libexpat1-dev:amd64.\n",
      "Preparing to unpack .../06-libexpat1-dev_2.2.5-3ubuntu0.2_amd64.deb ...\n",
      "Unpacking libexpat1-dev:amd64 (2.2.5-3ubuntu0.2) ...\n",
      "Selecting previously unselected package libpython3.6-dev:amd64.\n",
      "Preparing to unpack .../07-libpython3.6-dev_3.6.9-1~18.04ubuntu1.6_amd64.deb ...\n",
      "Unpacking libpython3.6-dev:amd64 (3.6.9-1~18.04ubuntu1.6) ...\n",
      "Selecting previously unselected package libpython3-dev:amd64.\n",
      "Preparing to unpack .../08-libpython3-dev_3.6.7-1~18.04_amd64.deb ...\n",
      "Unpacking libpython3-dev:amd64 (3.6.7-1~18.04) ...\n",
      "Selecting previously unselected package python3.6-dev.\n",
      "Preparing to unpack .../09-python3.6-dev_3.6.9-1~18.04ubuntu1.6_amd64.deb ...\n",
      "Unpacking python3.6-dev (3.6.9-1~18.04ubuntu1.6) ...\n",
      "Selecting previously unselected package python3-dev.\n",
      "Preparing to unpack .../10-python3-dev_3.6.7-1~18.04_amd64.deb ...\n",
      "Unpacking python3-dev (3.6.7-1~18.04) ...\n",
      "Selecting previously unselected package python3-pkg-resources.\n",
      "Preparing to unpack .../11-python3-pkg-resources_39.0.1-2_all.deb ...\n",
      "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
      "Selecting previously unselected package python3-setuptools.\n",
      "Preparing to unpack .../12-python3-setuptools_39.0.1-2_all.deb ...\n",
      "Unpacking python3-setuptools (39.0.1-2) ...\n",
      "Selecting previously unselected package r-base-core.\n",
      "Preparing to unpack .../13-r-base-core_3.4.4-1ubuntu1_amd64.deb ...\n",
      "Unpacking r-base-core (3.4.4-1ubuntu1) ...\n",
      "Selecting previously unselected package r-cran-boot.\n",
      "Preparing to unpack .../14-r-cran-boot_1.3-20-1.1_all.deb ...\n",
      "Unpacking r-cran-boot (1.3-20-1.1) ...\n",
      "Selecting previously unselected package r-cran-cluster.\n",
      "Preparing to unpack .../15-r-cran-cluster_2.0.6-2build1_amd64.deb ...\n",
      "Unpacking r-cran-cluster (2.0.6-2build1) ...\n",
      "Selecting previously unselected package r-cran-foreign.\n",
      "Preparing to unpack .../16-r-cran-foreign_0.8.69-1build1_amd64.deb ...\n",
      "Unpacking r-cran-foreign (0.8.69-1build1) ...\n",
      "Selecting previously unselected package r-cran-mass.\n",
      "Preparing to unpack .../17-r-cran-mass_7.3-49-1_amd64.deb ...\n",
      "Unpacking r-cran-mass (7.3-49-1) ...\n",
      "Selecting previously unselected package r-cran-kernsmooth.\n",
      "Preparing to unpack .../18-r-cran-kernsmooth_2.23-15-3build1_amd64.deb ...\n",
      "Unpacking r-cran-kernsmooth (2.23-15-3build1) ...\n",
      "Selecting previously unselected package r-cran-lattice.\n",
      "Preparing to unpack .../19-r-cran-lattice_0.20-35-1build1_amd64.deb ...\n",
      "Unpacking r-cran-lattice (0.20-35-1build1) ...\n",
      "Selecting previously unselected package r-cran-nlme.\n",
      "Preparing to unpack .../20-r-cran-nlme_3.1.131-3build1_amd64.deb ...\n",
      "Unpacking r-cran-nlme (3.1.131-3build1) ...\n",
      "Selecting previously unselected package r-cran-matrix.\n",
      "Preparing to unpack .../21-r-cran-matrix_1.2-12-1_amd64.deb ...\n",
      "Unpacking r-cran-matrix (1.2-12-1) ...\n",
      "Selecting previously unselected package r-cran-mgcv.\n",
      "Preparing to unpack .../22-r-cran-mgcv_1.8-23-1_amd64.deb ...\n",
      "Unpacking r-cran-mgcv (1.8-23-1) ...\n",
      "Selecting previously unselected package r-cran-survival.\n",
      "Preparing to unpack .../23-r-cran-survival_2.41-3-2build1_amd64.deb ...\n",
      "Unpacking r-cran-survival (2.41-3-2build1) ...\n",
      "Selecting previously unselected package r-cran-rpart.\n",
      "Preparing to unpack .../24-r-cran-rpart_4.1-13-1_amd64.deb ...\n",
      "Unpacking r-cran-rpart (4.1-13-1) ...\n",
      "Selecting previously unselected package r-cran-class.\n",
      "Preparing to unpack .../25-r-cran-class_7.3-14-2build1_amd64.deb ...\n",
      "Unpacking r-cran-class (7.3-14-2build1) ...\n",
      "Selecting previously unselected package r-cran-nnet.\n",
      "Preparing to unpack .../26-r-cran-nnet_7.3-12-2build1_amd64.deb ...\n",
      "Unpacking r-cran-nnet (7.3-12-2build1) ...\n",
      "Selecting previously unselected package r-cran-spatial.\n",
      "Preparing to unpack .../27-r-cran-spatial_7.3-11-2build1_amd64.deb ...\n",
      "Unpacking r-cran-spatial (7.3-11-2build1) ...\n",
      "Selecting previously unselected package r-cran-codetools.\n",
      "Preparing to unpack .../28-r-cran-codetools_0.2-15-1.1_all.deb ...\n",
      "Unpacking r-cran-codetools (0.2-15-1.1) ...\n",
      "Selecting previously unselected package r-recommended.\n",
      "Preparing to unpack .../29-r-recommended_3.4.4-1ubuntu1_all.deb ...\n",
      "Unpacking r-recommended (3.4.4-1ubuntu1) ...\n",
      "Selecting previously unselected package r-base.\n",
      "Preparing to unpack .../30-r-base_3.4.4-1ubuntu1_all.deb ...\n",
      "Unpacking r-base (3.4.4-1ubuntu1) ...\n",
      "Selecting previously unselected package r-cran-randomforest.\n",
      "Preparing to unpack .../31-r-cran-randomforest_4.6-12-2_amd64.deb ...\n",
      "Unpacking r-cran-randomforest (4.6-12-2) ...\n",
      "Setting up r-base-core (3.4.4-1ubuntu1) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "\n",
      "Creating config file /etc/R/Renviron with new version\n",
      "Setting up r-cran-nnet (7.3-12-2build1) ...\n",
      "Setting up python3-pkg-resources (39.0.1-2) ...\n",
      "Setting up libpython3.6-minimal:amd64 (3.6.9-1~18.04ubuntu1.6) ...\n",
      "Setting up r-cran-spatial (7.3-11-2build1) ...\n",
      "Setting up libexpat1-dev:amd64 (2.2.5-3ubuntu0.2) ...\n",
      "Setting up r-cran-mass (7.3-49-1) ...\n",
      "Setting up r-cran-randomforest (4.6-12-2) ...\n",
      "Setting up python3-setuptools (39.0.1-2) ...\n",
      "Setting up dh-python (3.20180325ubuntu2) ...\n",
      "Setting up r-cran-cluster (2.0.6-2build1) ...\n",
      "Setting up libpython3.6-stdlib:amd64 (3.6.9-1~18.04ubuntu1.6) ...\n",
      "Setting up r-cran-boot (1.3-20-1.1) ...\n",
      "Setting up r-cran-codetools (0.2-15-1.1) ...\n",
      "Setting up r-cran-lattice (0.20-35-1build1) ...\n",
      "Setting up r-cran-nlme (3.1.131-3build1) ...\n",
      "Setting up r-cran-foreign (0.8.69-1build1) ...\n",
      "Setting up r-cran-class (7.3-14-2build1) ...\n",
      "Setting up python3.6-minimal (3.6.9-1~18.04ubuntu1.6) ...\n",
      "Setting up r-cran-kernsmooth (2.23-15-3build1) ...\n",
      "Setting up libpython3.6:amd64 (3.6.9-1~18.04ubuntu1.6) ...\n",
      "Setting up python3.6 (3.6.9-1~18.04ubuntu1.6) ...\n",
      "Setting up r-cran-matrix (1.2-12-1) ...\n",
      "Setting up libpython3.6-dev:amd64 (3.6.9-1~18.04ubuntu1.6) ...\n",
      "Setting up r-cran-mgcv (1.8-23-1) ...\n",
      "Setting up r-cran-survival (2.41-3-2build1) ...\n",
      "Setting up python3.6-dev (3.6.9-1~18.04ubuntu1.6) ...\n",
      "Setting up libpython3-dev:amd64 (3.6.7-1~18.04) ...\n",
      "Setting up python3-dev (3.6.7-1~18.04) ...\n",
      "Setting up r-cran-rpart (4.1-13-1) ...\n",
      "Setting up r-recommended (3.4.4-1ubuntu1) ...\n",
      "Setting up r-base (3.4.4-1ubuntu1) ...\n",
      "Processing triggers for tex-common (6.09) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Running mktexlsr. This may take some time... done.\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n",
      "Processing triggers for install-info (6.5.0.dfsg.1-2) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
      "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
      "Removing intermediate container fb3bfd0b7281\n",
      " ---> c9a7da9cc168\n",
      "Step 3/8 : RUN pip install google-cloud-storage\n",
      " ---> Running in 4ff4386fecde\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (1.42.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.34.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.7.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.3.3)\n",
      "Requirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.31.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.25.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (21.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (2021.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (3.16.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (57.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (1.53.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-api-core<3.0dev,>=1.29.0->google-cloud-storage) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 4ff4386fecde\n",
      " ---> 125c54eefd30\n",
      "Step 4/8 : RUN pip install rpy2\n",
      " ---> Running in a0054f6578f4\n",
      "Collecting rpy2\n",
      "  Downloading rpy2-3.4.5.tar.gz (194 kB)\n",
      "Requirement already satisfied: cffi>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from rpy2) (1.14.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from rpy2) (2.11.3)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from rpy2) (2021.1)\n",
      "Collecting tzlocal\n",
      "  Downloading tzlocal-4.1-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.10.0->rpy2) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->rpy2) (1.1.1)\n",
      "Collecting backports.zoneinfo\n",
      "  Downloading backports.zoneinfo-0.2.1-cp37-cp37m-manylinux1_x86_64.whl (70 kB)\n",
      "Collecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2021.5-py2.py3-none-any.whl (339 kB)\n",
      "Building wheels for collected packages: rpy2\n",
      "  Building wheel for rpy2 (setup.py): started\n",
      "  Building wheel for rpy2 (setup.py): finished with status 'done'\n",
      "  Created wheel for rpy2: filename=rpy2-3.4.5-py3-none-any.whl size=198819 sha256=40a8dded09e7655bc7a83b2bf43b2ac124e572db0eedcd796ef38d00e7c4b577\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/00/c5/a43320afe86e7540d16d7f07cf4d29547d98921e76ea9f2f7a\n",
      "Successfully built rpy2\n",
      "Installing collected packages: tzdata, backports.zoneinfo, pytz-deprecation-shim, tzlocal, rpy2\n",
      "Successfully installed backports.zoneinfo-0.2.1 pytz-deprecation-shim-0.1.0.post0 rpy2-3.4.5 tzdata-2021.5 tzlocal-4.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container a0054f6578f4\n",
      " ---> 3aa5d6ebef7f\n",
      "Step 5/8 : RUN R -e 'install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")'\n",
      " ---> Running in e962a566df28\n",
      "\n",
      "R version 3.4.4 (2018-03-15) -- \"Someone to Lean On\"\n",
      "Copyright (C) 2018 The R Foundation for Statistical Computing\n",
      "Platform: x86_64-pc-linux-gnu (64-bit)\n",
      "\n",
      "R is free software and comes with ABSOLUTELY NO WARRANTY.\n",
      "You are welcome to redistribute it under certain conditions.\n",
      "Type 'license()' or 'licence()' for distribution details.\n",
      "\n",
      "R is a collaborative project with many contributors.\n",
      "Type 'contributors()' for more information and\n",
      "'citation()' on how to cite R or R packages in publications.\n",
      "\n",
      "Type 'demo()' for some demos, 'help()' for on-line help, or\n",
      "'help.start()' for an HTML browser interface to help.\n",
      "Type 'q()' to quit R.\n",
      "\n",
      "> install.packages(c(\"randomForest\"), repos = \"https://cran.rstudio.com/\")\n",
      "\u001b[91mInstalling package into ‘/usr/local/lib/R/site-library’\n",
      "(as ‘lib’ is unspecified)\n",
      "\u001b[0m\u001b[91mWarning message:\n",
      "package ‘randomForest’ is not available (for R version 3.4.4) \n",
      "\u001b[0m> \n",
      "> \n",
      "Removing intermediate container e962a566df28\n",
      " ---> 384a90fdc500\n",
      "Step 6/8 : WORKDIR /\n",
      " ---> Running in a771272f4be3\n",
      "Removing intermediate container a771272f4be3\n",
      " ---> 0a2f9a2cbc10\n",
      "Step 7/8 : COPY trainer /trainer\n",
      " ---> c036115de50e\n",
      "Step 8/8 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in 40cdf9c8214e\n",
      "Removing intermediate container 40cdf9c8214e\n",
      " ---> 3d7c325aa9cd\n",
      "Successfully built 3d7c325aa9cd\n",
      "Successfully tagged gcr.io/andy-1234-221921/r-train-iris:latest\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/andy-1234-221921/r-train-iris]\n",
      "\n",
      "\u001b[1Bf1a50768: Preparing \n",
      "\u001b[1B90a382df: Preparing \n",
      "\u001b[1B32eefe00: Preparing \n",
      "\u001b[1B99c1517d: Preparing \n",
      "\u001b[1Bca4f55cc: Preparing \n",
      "\u001b[1B7a187b52: Preparing \n",
      "\u001b[1B12393cf7: Preparing \n",
      "\u001b[1Beea30d04: Preparing \n",
      "\u001b[1Bcb08ebd3: Preparing \n",
      "\u001b[1B43a0b8a6: Preparing \n",
      "\u001b[1Be5e33f1e: Preparing \n",
      "\u001b[1B8e8f4996: Preparing \n",
      "\u001b[1Bd3afbbd2: Preparing \n",
      "\u001b[1B3a0d9161: Preparing \n",
      "\u001b[1B27f9bc83: Preparing \n",
      "\u001b[1B839b703f: Preparing \n",
      "\u001b[1B038361c3: Preparing \n",
      "\u001b[1B72ab02bc: Preparing \n",
      "\u001b[1Bf59ba4e4: Preparing \n",
      "\u001b[1Bbf18a086: Preparing \n",
      "\u001b[1B178bb296: Preparing \n",
      "\u001b[1B82d95108: Preparing \n",
      "\u001b[1B97c8aec4: Preparing \n",
      "\u001b[20Ba4f55cc: Pushed   209.8MB/207.1MB\u001b[24A\u001b[2K\u001b[20A\u001b[2K\u001b[22A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[21A\u001b[2K\u001b[24A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[18A\u001b[2K\u001b[15A\u001b[2K\u001b[20A\u001b[2K\u001b[12A\u001b[2K\u001b[20A\u001b[2K\u001b[9A\u001b[2K\u001b[20A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2K\u001b[20A\u001b[2Klatest: digest: sha256:f2fbcb1295ecd3b7e84f2694f3bf421d4d3ddb38601d5517a0b344198fe94e12 size: 5344\n"
     ]
    }
   ],
   "source": [
    "TRAIN_IMAGE = f\"gcr.io/{PROJECT_ID}/r-train-iris\"\n",
    "print(TRAIN_IMAGE)\n",
    "\n",
    "! docker build --tag=$TRAIN_IMAGE ./custom\n",
    "\n",
    "! docker push $TRAIN_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_container_training_job:mbsdk,no_model",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Create and run custom training job\n",
    "\n",
    "\n",
    "To train a custom model, you perform two steps: 1) create a custom training job, and 2) run the job.\n",
    "\n",
    "#### Create custom training job\n",
    "\n",
    "A custom training job is created with the `CustomTrainingJob` class, with the following parameters:\n",
    "\n",
    "- `display_name`: The human readable name for the custom training job.\n",
    "- `container_uri`: The training container image.\n",
    "\n",
    "- `command`: The command (e.g., interpreter) and script to invokee within the container.\n",
    "\n",
    "*Note:* The interpreter and script to invoke is overridable within the container (i.e., ENTRYPOINT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "create_custom_container_training_job:mbsdk,no_model",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.training_jobs.CustomContainerTrainingJob object at 0x7f1b31fbbb10>\n"
     ]
    }
   ],
   "source": [
    "job = aip.CustomContainerTrainingJob(\n",
    "    display_name=\"iris_\" + TIMESTAMP,\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    command=['python3', 'trainer/task.py']\n",
    ")\n",
    "\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_custom_container_training_job:r",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Run the custom container training job\n",
    "\n",
    "Next, you run the custom job to start the training job by invoking the method `run()`. The parameters are the same as when running a CustomTrainingJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "run_custom_container_training_job:r",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://andy-1234-221921aip-20220203205833/aiplatform-custom-training-2022-02-03-21:24:57.374 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/7598789730745974784?project=759209241365\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/759209241365/locations/us-central1/trainingPipelines/7598789730745974784 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/759209241365/locations/us-central1/trainingPipelines/7598789730745974784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/3744834349623672832?project=759209241365\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/759209241365/locations/us-central1/trainingPipelines/7598789730745974784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/759209241365/locations/us-central1/trainingPipelines/7598789730745974784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/759209241365/locations/us-central1/trainingPipelines/7598789730745974784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob projects/759209241365/locations/us-central1/trainingPipelines/7598789730745974784 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomContainerTrainingJob run completed. Resource name: projects/759209241365/locations/us-central1/trainingPipelines/7598789730745974784\n",
      "WARNING:google.cloud.aiplatform.training_jobs:Training did not produce a Managed Model returning None. Training Pipeline projects/759209241365/locations/us-central1/trainingPipelines/7598789730745974784 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
     ]
    }
   ],
   "source": [
    "CMDARGS = [ \"--model-dir=\" + BUCKET_NAME\n",
    "]\n",
    "\n",
    "job.run(\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    sync=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_job",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a custom training job\n",
    "\n",
    "After a training job is completed, you can delete the training job with the method `delete()`.  Prior to completion, a training job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_job",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "job.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- AutoML Training Job\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if 'dataset' in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if 'model' in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if 'endpoint' in globals():\n",
    "            endpoint.undeploy_all()\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline training job\n",
    "    try:\n",
    "        if 'dag' in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom training job\n",
    "    try:\n",
    "        if 'job' in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if 'batch_predict_job' in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if 'hpt_job' in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if 'BUCKET_NAME' in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vars": {
   "AIP": "AI Platform",
   "AUTOML": "AutoML",
   "AUTOML_URL": "https://cloud.google.com/vertex-ai/docs/start/automl-users",
   "BQ": "BigQuery",
   "COLAB": "Colab",
   "DATASET_ALIAS": "iris",
   "DATASET_NAME": "Iris",
   "DIRECT": "True",
   "DOCKER": "Docker",
   "EDGE": "Edge",
   "EMAIL": "cdpe@gmail.com",
   "GAPIC": "client library",
   "GAPICP": "client library for Python",
   "GCONSOLE": "Cloud Console",
   "GCP": "Google Cloud",
   "GCS": "Cloud Storage",
   "GNOTEBOOK": "Google Cloud Notebook",
   "LABEL_COLUMN": "species",
   "MODEL_TYPE": "tabular classification",
   "NOTEBOOK": "stage2/get_started_vertex_training_r.ipynb",
   "REGION": "us-central1",
   "REPO": "vertex-ai-samples/blob/main/notebooks/community/ml_ops",
   "SDKP": "SDK for Python",
   "STAGE": "2 : experimentation: get started with Vertex Training for R",
   "TENSORFLOW": "TensorFlow",
   "TFLite": "TFLite",
   "TFServing": "TF Serving",
   "TFV": "2.5",
   "TITLE": "E2E ML on GCP: MLOps stage 2 : experimentation: get started with Vertex Training for R",
   "TRAINING": "training",
   "VERTEX": "Vertex AI",
   "VERTEX_AI": "Vertex AI",
   "YEAR": "2022",
   "null": "null",
   "uCAIP": "Vertex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
