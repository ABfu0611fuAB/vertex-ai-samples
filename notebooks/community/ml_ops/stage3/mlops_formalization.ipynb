{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "copyright",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 3 : formalization\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage3/mlops_formalization.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/tree/master/notebooks/official/automl/ml_ops_stage3/mlops_formalization.ipynb\">\n",
    "      Open in Google Cloud Notebooks\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:bq,chicago,lbn",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the [Chicago Taxi](https://www.kaggle.com/chicago/chicago-taxi-trips-bq). The version of the dataset you will use in this tutorial is stored in a public BigQuery table. The trained model predicts whether someone would leave a tip for a taxi fare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage3,tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you create a MLOps stage 3: formalization process.\n",
    "\n",
    "This tutorial uses the following Vertex AI:\n",
    "\n",
    "- `Vertex AI Pipelines`\n",
    "- `Vertex AI Training`\n",
    "- `Google Cloud Pipeline Components`\n",
    "- `Vertex AI Dataset, and Model resources\n",
    "- `Dataflow`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Obtain resources from experimention stage.\n",
    "    - Baseline model.\n",
    "    - Dataset schema/statstics for baseline model.\n",
    "- Formalize a data preprocessing pipeline.\n",
    "    - Extract columns/rows from BigQuery table to local BigQuery table.\n",
    "    - Use Tensorflow Data Validation library to determine statistics, schema, and features.\n",
    "    - Use Dataflow to preprocess the data.\n",
    "    - Create a Vertex AI Dataset.\n",
    "- Formalize a build model architecture pipeline.\n",
    "    - Create the Vertex AI Model base model.\n",
    "- Formalize a training pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install *one time* the packages for executing the MLOps notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "ONCE_ONLY = False\n",
    "if ONCE_ONLY:\n",
    "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
    "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
    "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
    "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
    "    ! pip3 install --upgrade kfp $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "set_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "autoset_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: andy-1234-221921\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "set_gcloud_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "autoset_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://andy-1234-221921aip-20211210003020/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "set_service_account",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "autoset_service_account",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 759209241365-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].strip()\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines",
    "repo": "snippets_pipelines.ipynb"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "set_service_account:pipelines",
    "repo": "snippets_pipelines.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "import_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tf",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import TensorFlow\n",
    "\n",
    "Import the TensorFlow package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "import_tf",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "import_kfp:namedtuple",
    "repo": "snippets_pipelines.ipynb"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "import_gcpc:dataflow",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.experimental.dataflow import DataflowPythonJobOp\n",
    "from google_cloud_pipeline_components.experimental.wait_gcp_resources import WaitGcpResourcesOp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "init_aip:mbsdk,all",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:training,prediction,ngpu,mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set hardware accelerators\n",
    "\n",
    "You can set hardware accelerators for training and prediction.\n",
    "\n",
    "Set the variables `TRAIN_GPU/TRAIN_NGPU` and `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
    "\n",
    "    (aip.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "\n",
    "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
    "\n",
    "Learn more about [hardware accelerator support for your region](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators).\n",
    "\n",
    "*Note*: TF releases before 2.3 for GPU support will fail to load the custom model in this tutorial. It is a known issue and fixed in TF 2.3. This is caused by static graph ops that are generated in the serving function. If you encounter this issue on your own custom models, use a container image for TF 2.3 with GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "accelerators:training,prediction,ngpu,mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_GPU\"):\n",
    "    TRAIN_GPU, TRAIN_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, int(os.getenv(\"IS_TESTING_TRAIN_GPU\")))\n",
    "else:\n",
    "    TRAIN_GPU, TRAIN_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80, int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")))\n",
    "else:\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set pre-built containers\n",
    "\n",
    "Set the pre-built Docker container image for training and prediction.\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for training](https://cloud.google.com/ai-platform-unified/docs/training/pre-built-containers).\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "container:training,prediction",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-5:latest AcceleratorType.NVIDIA_TESLA_K80 1\n",
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest None None\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TF\"):\n",
    "    TF = os.getenv(\"IS_TESTING_TF\")\n",
    "else:\n",
    "    TF = '2.5'.replace('.', '-')\n",
    "\n",
    "if TF[0] == '2':\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = 'tf-gpu.{}'.format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = 'tf-cpu.{}'.format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = 'tf2-gpu.{}'.format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = 'tf2-cpu.{}'.format(TF)\n",
    "else:\n",
    "    if TRAIN_GPU:\n",
    "        TRAIN_VERSION = 'tf-gpu.{}'.format(TF)\n",
    "    else:\n",
    "        TRAIN_VERSION = 'tf-cpu.{}'.format(TF)\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = 'tf-gpu.{}'.format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = 'tf-cpu.{}'.format(TF)\n",
    "\n",
    "TRAIN_IMAGE = \"{0}-docker.pkg.dev/vertex-ai/training/{1}:latest\".format(REGION.split('-')[0],TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"{0}-docker.pkg.dev/vertex-ai/prediction/{1}:latest\".format(REGION.split('-')[0],DEPLOY_VERSION)\n",
    "\n",
    "print(\"Training:\", TRAIN_IMAGE, TRAIN_GPU, TRAIN_NGPU)\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:training,prediction",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set machine type\n",
    "\n",
    "Next, set the machine type to use for training and prediction.\n",
    "\n",
    "- Set the variables `TRAIN_COMPUTE` and `DEPLOY_COMPUTE` to configure  the compute resources for the VMs you will use for for training and prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: The following is not supported for training:*\n",
    "\n",
    " - `standard`: 2 vCPUs\n",
    " - `highcpu`: 2, 4 and 8 vCPUs\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "machine:training,prediction",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train machine type n1-standard-4\n",
      "Deploy machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TRAIN_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_TRAIN_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = 'n1-standard'\n",
    "\n",
    "VCPU = '4'\n",
    "TRAIN_COMPUTE = MACHINE_TYPE + '-' + VCPU\n",
    "print('Train machine type', TRAIN_COMPUTE)\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = 'n1-standard'\n",
    "\n",
    "VCPU = '4'\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + '-' + VCPU\n",
    "print('Deploy machine type', DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,bq",
    "repo": "snippets_common.ipynb"
   },
   "source": [
    "#### Location of BigQuery training data.\n",
    "\n",
    "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "import_file:chicago,bq,lbn",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = 'bq://bigquery-public-data.chicago_taxi_trips.taxi_trips'\n",
    "BQ_TABLE = 'bigquery-public-data.chicago_taxi_trips.taxi_trips'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "find_dataset:bq",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Retrieve the dataset from stage 1\n",
    "\n",
    "Next, retrieve the dataset you created during stage 1 with the helper function `find_dataset()`. This helper function finds all the datasets whose display name matches the specified prefix and import format (e.g., bq). Finally it sorts the matches by create time and returns the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "find_dataset:bq",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.datasets.tabular_dataset.TabularDataset object at 0x7f869b3f7a90> \n",
      "resource name: projects/759209241365/locations/us-central1/datasets/9072774128221552640\n"
     ]
    }
   ],
   "source": [
    "def find_dataset(display_name_prefix, import_format):\n",
    "    matches=[]\n",
    "    datasets = aip.TabularDataset.list()\n",
    "    for dataset in datasets:\n",
    "        if dataset.display_name.startswith(display_name_prefix):\n",
    "            try:\n",
    "                if \"bq\" == import_format and dataset.to_dict()['metadata']['inputConfig']['bigquerySource']:\n",
    "                    matches.append(dataset)\n",
    "                if \"csv\" == import_format and dataset.to_dict()['metadata']['inputConfig']['gcsSource']:\n",
    "                    matches.append(dataset)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    create_time = None\n",
    "    for match in matches:\n",
    "        if (create_time is None or\n",
    "            match.create_time > create_time):\n",
    "            create_time = match.create_time\n",
    "            dataset = match\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "dataset = find_dataset(\"Chicago Taxi\", \"bq\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_dataset_user_metadata",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Load dataset's user metadata\n",
    "\n",
    "Load the user metadata for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "load_dataset_user_metadata",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error executing an HTTP request: HTTP response code 404 with body '{\n  \"error\": {\n    \"code\": 404,\n    \"message\": \"The specified bucket does not exist.\",\n    \"errors\": [\n      {\n        \"message\": \"The specified bucket does not exist.\",\n        \"domain\": \"global\",\n        \"reason\": \"notFound\"\n      }\n    ]\n  }\n}\n'\n\t when reading gs://andy-1234-221921aip-20211209234821/metadata.jsonl/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-b5256d8f218a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user_metadata'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/metadata.jsonl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preread_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;34m\"\"\"Returns the size of the file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mstat\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m   \"\"\"\n\u001b[0;32m--> 871\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mstat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mstat_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moperation\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m   \"\"\"\n\u001b[0;32m--> 887\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pywrap_file_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_to_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Error executing an HTTP request: HTTP response code 404 with body '{\n  \"error\": {\n    \"code\": 404,\n    \"message\": \"The specified bucket does not exist.\",\n    \"errors\": [\n      {\n        \"message\": \"The specified bucket does not exist.\",\n        \"domain\": \"global\",\n        \"reason\": \"notFound\"\n      }\n    ]\n  }\n}\n'\n\t when reading gs://andy-1234-221921aip-20211209234821/metadata.jsonl/"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with tf.io.gfile.GFile('gs://' + dataset.labels['user_metadata'] + \"/metadata.jsonl\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "find_model",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Retrieve the model architecture and baseline model from stage 2\n",
    "\n",
    "Next, retrieve the model architecture and baseline trained model you created during stage 2 with the helper function `find_model()`. This helper function finds all the models whose display name matches the specified prefix and contains the specified label. Finally it sorts the matches by create time and returns the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "find_model",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.models.Model object at 0x7f869b486c90> \n",
      "resource name: projects/759209241365/locations/us-central1/models/8723265770030628864\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def find_model(display_name_prefix, label=None):\n",
    "    matches = []\n",
    "    models = aip.Model.list()\n",
    "    for model in models:\n",
    "        if model.display_name.startswith(display_name_prefix):\n",
    "            try:\n",
    "                if label in model.to_dict()['labels'].keys():\n",
    "                    matches.append(model)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    model = None\n",
    "    create_time = None\n",
    "    for match in matches:\n",
    "        if (create_time is None or\n",
    "            match.create_time > create_time):\n",
    "            create_time = match.create_time\n",
    "            model = match\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "base_model = find_model(\"chicago\", 'base_model')\n",
    "baseline_model = find_model(\"chicago\", 'user_metadata')\n",
    "\n",
    "print(base_model)\n",
    "print(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "formalize_pipeline_intro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Formalizing pipelines introduction\n",
    "\n",
    "A primary reason for formalizing the training and deployment of a model into a pipeline, is that overtime things will change and you will want to rebuild/retrain your model. A pipeline provides the ability to integrate these tasks as an automated process within a CI/CD process.\n",
    "\n",
    "While one generally represents a formalized pipeline as a single e2e pipeline, in practice you decompose the e2e pipeline into the following sub-pipelines:\n",
    "\n",
    "- data pipeline\n",
    "    - data analysis\n",
    "    - data preprocessing\n",
    "- model pipeline\n",
    "    - model architecture construction\n",
    "    - base model storage\n",
    "- training pipeline\n",
    "    - model training\n",
    "    - model evaluation\n",
    "- deployment pipeline\n",
    "    - model candidate\n",
    "    - pre-production deployment evaluations\n",
    "    - deployment to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "formalize_data_pipeline_intro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Formalizing data pipeline introduction\n",
    "\n",
    "The data pipeline consists of data analysis and data preprocessing tasks.\n",
    "\n",
    "### Data analysis task\n",
    "\n",
    "This task performs an analysis of the dataset to determine it's statistical distribution. This distribution is then used to build a dataset schema. The schema is then used by the data preprocessing task. Additionally for tabular data, the default feature types per feature are determined -- i.e., categorical, numeric.\n",
    "\n",
    "### Data preprocessing task\n",
    "\n",
    "This task performs a conversion of the raw dataset data into one or more machine learning ready formats. The dataset schema is used to determine how to preprocess the data. Other tasks include: splitting the dataset into training, test and validation, and encoding and storing the preprocessed data to disk.\n",
    "\n",
    "### Triggers\n",
    "\n",
    "Within the CI/CD process, the data pipeline is triggered for one or more of the following example reasons, while not exhaustive:\n",
    "\n",
    "- New data added to the dataset.\n",
    "- Addition or subtraction of features.\n",
    "- Code changes to the preprocessing tasks.\n",
    "- Code changes to the feature engineering tasks.\n",
    "- Input layer changes that invalidate the stored preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_bq_dataset_component:chicago",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create component for creating a local BigQuery dataset\n",
    "\n",
    "Next, you create a component which makes a local copy, -- i.e., in your project, of the BigQuery Chicago Taxi dataset, where:\n",
    "\n",
    "- Select features to include\n",
    "- Select criteria for including rows\n",
    "- Perform feature engineering.\n",
    "\n",
    "This component returns as an artifact the BigQuery path to the local dataset copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "make_bq_dataset_component:chicago",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"bigquery\"])\n",
    "def make_chicago_bq_dataset(bq_table: str,\n",
    "                            year: int,\n",
    "                            limit: int,\n",
    "                            project: str) -> str:\n",
    "    from google.cloud import bigquery\n",
    "    bqclient = bigquery.Client()\n",
    "\n",
    "    BQ_DATASET = bq_table.split('.')[1]\n",
    "    BQ_TABLE_COPY = f\"{project}.{BQ_DATASET}.taxi_trips\"\n",
    "\n",
    "    query = f'''\n",
    "    CREATE OR REPLACE TABLE `{BQ_TABLE_COPY}`\n",
    "    AS (\n",
    "        WITH\n",
    "          taxitrips AS (\n",
    "          SELECT\n",
    "            trip_start_timestamp,\n",
    "            trip_seconds,\n",
    "            trip_miles,\n",
    "            payment_type,\n",
    "            pickup_longitude,\n",
    "            pickup_latitude,\n",
    "            dropoff_longitude,\n",
    "            dropoff_latitude,\n",
    "            tips,\n",
    "            fare\n",
    "          FROM\n",
    "            `{bq_table}`\n",
    "          WHERE pickup_longitude IS NOT NULL\n",
    "          AND pickup_latitude IS NOT NULL\n",
    "          AND dropoff_longitude IS NOT NULL\n",
    "          AND dropoff_latitude IS NOT NULL\n",
    "          AND trip_miles > 0\n",
    "          AND trip_seconds > 0\n",
    "          AND fare > 0\n",
    "          AND EXTRACT(YEAR FROM trip_start_timestamp) = {year}\n",
    "        )\n",
    "\n",
    "        SELECT\n",
    "          EXTRACT(MONTH from trip_start_timestamp) as trip_month,\n",
    "          EXTRACT(DAY from trip_start_timestamp) as trip_day,\n",
    "          EXTRACT(DAYOFWEEK from trip_start_timestamp) as trip_day_of_week,\n",
    "          EXTRACT(HOUR from trip_start_timestamp) as trip_hour,\n",
    "          CAST(trip_seconds AS FLOAT64) as trip_seconds,\n",
    "          trip_miles,\n",
    "          payment_type,\n",
    "          ST_AsText(\n",
    "              ST_SnapToGrid(ST_GeogPoint(pickup_longitude, pickup_latitude), 0.1)\n",
    "          ) AS pickup_grid,\n",
    "          ST_AsText(\n",
    "              ST_SnapToGrid(ST_GeogPoint(dropoff_longitude, dropoff_latitude), 0.1)\n",
    "          ) AS dropoff_grid,\n",
    "          ST_Distance(\n",
    "              ST_GeogPoint(pickup_longitude, pickup_latitude),\n",
    "              ST_GeogPoint(dropoff_longitude, dropoff_latitude)\n",
    "          ) AS euclidean,\n",
    "          CONCAT(\n",
    "              ST_AsText(ST_SnapToGrid(ST_GeogPoint(pickup_longitude,\n",
    "                  pickup_latitude), 0.1)),\n",
    "              ST_AsText(ST_SnapToGrid(ST_GeogPoint(dropoff_longitude,\n",
    "                  dropoff_latitude), 0.1))\n",
    "          ) AS loc_cross,\n",
    "          IF((tips/fare >= 0.2), 1, 0) AS tip_bin,\n",
    "        FROM\n",
    "          taxitrips\n",
    "        LIMIT {limit}\n",
    "    )\n",
    "    '''\n",
    "\n",
    "    return BQ_TABLE_COPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_data_analysis_component",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create component for performing data analysis on a BigQuery dataset table\n",
    "\n",
    "Next, you create a component which performs data analysis on a BigQuery dataset table using Tensorflow Data Validation library, where:\n",
    "\n",
    "- Create a client connection to BigQuery.\n",
    "- Extract the BigQuery table to a pandas dataframe.\n",
    "- Use Tensorflow Data Validation library (TFDV) to generate the dataset statistics\n",
    "- Use Tensorflow Data Validation library (TFDV) to generate the dataset schema\n",
    "- Determine feature types (numeric vs categorical) from the statistics.\n",
    "- Write statistics and data to Cloud Storage bucket.\n",
    "\n",
    "This component returns as an artifact a dictionary representing the dataset metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "make_data_analysis_component",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=['tensorflow', 'tensorflow-data-validation==1.2', 'google-cloud-bigquery'])\n",
    "def data_analysis(bq_table: str,\n",
    "                  label_column: str,\n",
    "                  data_bucket: str,\n",
    "                  project: str\n",
    "                 ) -> dict:\n",
    "\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from google.cloud import bigquery\n",
    "    import json\n",
    "\n",
    "    bqclient = bigquery.Client(project=project)\n",
    "\n",
    "    table = bigquery.TableReference.from_string(\n",
    "        bq_table\n",
    "    )\n",
    "\n",
    "    rows = bqclient.list_rows(\n",
    "        table\n",
    "\n",
    "    )\n",
    "\n",
    "    dataframe = rows.to_dataframe()\n",
    "\n",
    "    stats = tfdv.generate_statistics_from_dataframe(\n",
    "        dataframe=dataframe,\n",
    "        stats_options=tfdv.StatsOptions(\n",
    "            label_feature=label_column,\n",
    "            sample_rate=1,\n",
    "            num_top_values=50\n",
    "        )\n",
    "    )\n",
    "\n",
    "    tfdv.write_stats_text(stats, data_bucket + \"/statistics.jsonl\")\n",
    "\n",
    "    NUMERIC_FEATURES = []\n",
    "    CATEGORICAL_FEATURES = []\n",
    "    for _ in range(len(stats.datasets[0].features)):\n",
    "        if stats.datasets[0].features[_].path.step[0] == label_column:\n",
    "            continue\n",
    "        if stats.datasets[0].features[_].type == 0:  # int\n",
    "            CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
    "        elif stats.datasets[0].features[_].type == 1:  # float\n",
    "            NUMERIC_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
    "        elif stats.datasets[0].features[_].type == 2:  # string\n",
    "            CATEGORICAL_FEATURES.append(stats.datasets[0].features[_].path.step[0])\n",
    "\n",
    "    schema = tfdv.infer_schema(statistics=stats)\n",
    "\n",
    "    tfdv.write_schema_text(output_path=data_bucket + \"/schema.txt\", schema=schema)\n",
    "\n",
    "    metadata = { \"label_column\" : label_column,\n",
    "                 \"statistics\": data_bucket + \"/statistics.jsonl\",\n",
    "                 \"schema\": data_bucket + \"/schema.txt\",\n",
    "                 \"numeric_features\": NUMERIC_FEATURES,\n",
    "                 \"categorical_features\": CATEGORICAL_FEATURES\n",
    "    }\n",
    "\n",
    "    with tf.io.gfile.GFile(data_bucket + '/metadata.jsonl', \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_dataflow_args:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create constructing the run arguments for Dataflow component\n",
    "\n",
    "Next, you create a component for constructing the run arguments for the subsequent Dataflow component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "make_dataflow_args:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component()\n",
    "def make_dataflow_args(bucket: str,\n",
    "                       bq_table: str,\n",
    "                       setup_file: str,\n",
    "                       metadata: dict,\n",
    "                       transformed_data_prefix: str,\n",
    "                       transform_artifacts_dir: str,\n",
    "                       exported_tfrec_prefix: str,\n",
    "                       exported_jsonl_prefix: str,\n",
    "                       label: str\n",
    "                      ) -> list:\n",
    "    return ['--bucket', bucket,\n",
    "            '--bq-table', bq_table,\n",
    "            '--runner', 'DataflowRunner',\n",
    "            '--setup_file', setup_file,\n",
    "            '--metadata', str(metadata),\n",
    "            '--transformed-data-prefix', transformed_data_prefix,\n",
    "            '--transform-artifacts-dir', transform_artifacts_dir,\n",
    "            '--exported-tfrec-prefix', exported_tfrec_prefix,\n",
    "            '--exported-jsonl-prefix', exported_jsonl_prefix,\n",
    "            '--label', label\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "write_dataflow_script:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Write the Dataflow Python module for preprocessing the data.\n",
    "\n",
    "Next, you write the Python script for preprocessing the data. This script will be used by the subsequent Dataflow component.\n",
    "\n",
    "#### Dataset splitting\n",
    "\n",
    "- Query the BigQuery table for all examples (parse_bq_record).\n",
    "- Split the examples into training, evaluation and test datasets (split_dataset).\n",
    "- Preprocess each example (preprocessing_fn).\n",
    "- Write the preprocessed data to a Cloud Storage bucket as TFRecords.\n",
    "- Write the transformation function artifacts to a Cloud Storage bucket.\n",
    "- Write the raw (unprocessed) examples to a Cloud Storage bucket as TFRecords.\n",
    "- Write the raw (unprocessed) examples to a Cloud Storage bucket as JSONL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "write_dataflow_script:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "\n",
    "def run(argv=None):\n",
    "    \"\"\" Main entry: data management\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--bq-table', dest='bq_table', type=str)\n",
    "    parser.add_argument('--bucket', dest='bucket', type=str)\n",
    "    parser.add_argument('--metadata', dest='metadata', type=str)\n",
    "    parser.add_argument('--transformed-data-prefix', dest='transformed_data_prefix', type=str)\n",
    "    parser.add_argument('--transform-artifacts-dir', dest='transform_artifacts_dir', type=str)\n",
    "    parser.add_argument('--exported-tfrec-prefix', dest='exported_tfrec_prefix', type=str)\n",
    "    parser.add_argument('--exported-jsonl-prefix', dest='exported_jsonl_prefix', type=str)\n",
    "    parser.add_argument('--label', dest='label', type=str)\n",
    "\n",
    "    args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "    logging.info(\"ARGS\")\n",
    "    logging.info(args)\n",
    "    logging.info(\"PIPELINE ARGS\")\n",
    "    logging.info(pipeline_args)\n",
    "\n",
    "    metadata = json.loads(args.metadata.replace(\"'\", '\"'))\n",
    "\n",
    "    numeric_features = metadata['numeric_features']\n",
    "    categorical_features = metadata['categorical_features']\n",
    "    schema_location = metadata['schema']\n",
    "\n",
    "    for i in range(0, len(pipeline_args), 2):\n",
    "        if \"--temp_location\" == pipeline_args[i]:\n",
    "            temp_location = pipeline_args[i+1]\n",
    "        elif \"--project\" == pipeline_args[i]:\n",
    "            project = pipeline_args[i+1]\n",
    "\n",
    "    exported_train = args.bucket + '/exported_data/train'\n",
    "    exported_eval  = args.bucket + '/exported_data/eval'\n",
    "\n",
    "    logging.info(\"Get schema\")\n",
    "    schema = tfdv.load_schema_text(schema_location)\n",
    "    feature_spec = tft.tf_metadata.schema_utils.schema_as_feature_spec(\n",
    "        schema\n",
    "    ).feature_spec\n",
    "\n",
    "    raw_metadata = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n",
    "        tft.tf_metadata.schema_utils.schema_from_feature_spec(feature_spec)\n",
    "    )\n",
    "    query = f\"SELECT * FROM {args.bq_table}\"\n",
    "\n",
    "    logging.info(\"Preprocess the data\")\n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    with beam.Pipeline(options=pipeline_options) as pipeline:\n",
    "        with tft_beam.Context(temp_location):\n",
    "\n",
    "            def parse_bq_record(bq_record):\n",
    "                \"\"\"Parses a bq_record to a dictionary.\"\"\"\n",
    "                output = {}\n",
    "                for key in bq_record:\n",
    "                    output[key] = [bq_record[key]]\n",
    "                return output\n",
    "\n",
    "            def split_dataset(bq_row, num_partitions, ratio):\n",
    "                \"\"\"Returns a partition number for a given bq_row.\"\"\"\n",
    "                import json\n",
    "\n",
    "                assert num_partitions == len(ratio)\n",
    "                bucket = sum(map(ord, json.dumps(bq_row))) % sum(ratio)\n",
    "                total = 0\n",
    "                for i, part in enumerate(ratio):\n",
    "                    total += part\n",
    "                    if bucket < total:\n",
    "                        return i\n",
    "                return len(ratio) - 1\n",
    "\n",
    "            def convert_to_jsonl(data, label=None):\n",
    "                ''' Converts a parsed record to JSON '''\n",
    "                if label:\n",
    "                    del data[label]\n",
    "                return json.dumps(data)\n",
    "\n",
    "            def preprocessing_fn(inputs):\n",
    "                outputs = {}\n",
    "                for key in inputs.keys():\n",
    "                    if key in numeric_features:\n",
    "                        outputs[key] = tft.scale_to_z_score(inputs[key])\n",
    "                    elif key in categorical_features:\n",
    "                        outputs[key] = tft.compute_and_apply_vocabulary(\n",
    "                                            inputs[key],\n",
    "                                            num_oov_buckets=1,\n",
    "                                            vocab_filename=key,\n",
    "                                        )\n",
    "                    else:\n",
    "                        outputs[key] = inputs[key]\n",
    "                    outputs[key] = tf.squeeze(outputs[key], -1)\n",
    "                return outputs\n",
    "\n",
    "\n",
    "            # Read raw BigQuery data.\n",
    "            raw_train_data, raw_val_data, raw_test_data = (\n",
    "                pipeline\n",
    "                | \"Read Raw Data\"\n",
    "                >> beam.io.ReadFromBigQuery(\n",
    "                    query=query,\n",
    "                    project=project,\n",
    "                    use_standard_sql=True,\n",
    "                )\n",
    "                | \"Parse Data\" >> beam.Map(parse_bq_record)\n",
    "                | \"Split\" >> beam.Partition(split_dataset, 3, ratio=[8, 1, 1])\n",
    "            )\n",
    "\n",
    "            # Create a train_dataset from the data and schema.\n",
    "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "\n",
    "            # Analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn.\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset\n",
    "                | \"Analyze & Transform\"\n",
    "                >> tft_beam.AnalyzeAndTransformDataset(preprocessing_fn)\n",
    "            )\n",
    "\n",
    "            # Get data and schema separately from the transformed_dataset.\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # Get data and schema separately from the transformed_dataset.\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # Write transformed train data.\n",
    "            _ = (\n",
    "                transformed_train_data\n",
    "                | \"Write Transformed Train Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(\n",
    "                        args.transformed_data_prefix, \"train/data\"\n",
    "                    ),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Create a val_dataset from the data and schema.\n",
    "            raw_val_dataset = (raw_val_data, raw_metadata)\n",
    "\n",
    "            # Transform raw_val_dataset to produced transformed_val_dataset using transform_fn.\n",
    "            transformed_val_dataset = (\n",
    "                raw_val_dataset,\n",
    "                transform_fn,\n",
    "            ) | \"Transform Validation Data\" >> tft_beam.TransformDataset()\n",
    "\n",
    "            # Get data from the transformed_val_dataset.\n",
    "            transformed_val_data, _ = transformed_val_dataset\n",
    "\n",
    "            # Write transformed val data.\n",
    "            _ = (\n",
    "                transformed_val_data\n",
    "                | \"Write Transformed Validation Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(args.transformed_data_prefix, \"val/data\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Create a test_dataset from the data and schema.\n",
    "            raw_test_dataset = (raw_test_data, raw_metadata)\n",
    "\n",
    "            # Transform raw_test_dataset to produced transformed_test_dataset using transform_fn.\n",
    "            transformed_test_dataset = (\n",
    "                raw_test_dataset,\n",
    "                transform_fn,\n",
    "            ) | \"Transform Test Data\" >> tft_beam.TransformDataset()\n",
    "\n",
    "\n",
    "            # Get data from the transformed_test_dataset.\n",
    "            transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "            # write transformed test data.\n",
    "            _ = (\n",
    "                transformed_test_data\n",
    "                | \"Write Transformed Test Data\"\n",
    "                >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=os.path.join(args.transformed_data_prefix, \"test/data\"),\n",
    "                    file_name_suffix=\".gz\",\n",
    "                    coder=tft.coders.ExampleProtoCoder(transformed_metadata.schema),\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Write transform_fn.\n",
    "            _ = transform_fn | \"Write Transform Artifacts\" >> tft_beam.WriteTransformFn(\n",
    "                args.transform_artifacts_dir\n",
    "            )\n",
    "\n",
    "            # Write raw test data to GCS as TF Records\n",
    "            _ = raw_test_data | \"Write TF Test Data\" >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                file_path_prefix=os.path.join(args.exported_tfrec_prefix, \"data\"),\n",
    "                file_name_suffix=\".tfrecord\",\n",
    "                coder=tft.coders.ExampleProtoCoder(raw_metadata.schema),\n",
    "            )\n",
    "\n",
    "            # Convert raw test data to JSON (for batch prediction)\n",
    "            json_test_data = (\n",
    "                raw_test_data\n",
    "            ) | \"Convert Batch Test Data\" >> beam.Map(convert_to_jsonl, label=args.label)\n",
    "\n",
    "            # Write raw test data to GCS as JSONL files.\n",
    "            _ = json_test_data | \"Write JSONL Test Data\" >> beam.io.WriteToText(\n",
    "                file_path_prefix=args.exported_jsonl_prefix, file_name_suffix=\".jsonl\"\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "write_dataflow_requirements:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Write the requirements (installs) for the Dataflow (Apache Beam) pipeline module\n",
    "\n",
    "Next, create the `requirements.txt` file to specify Python modules that are required to be installed for executing the Apache Beam pipeline module -- in this case, `apache-beam`, `tensorflow-transform` and `tensorflow-data-validation` are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "write_dataflow_requirements:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "apache-beam\n",
    "tensorflow-transform==1.2.0\n",
    "tensorflow-data-validation==1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "write_dataflow_setup:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Prepare package requirements for Dataflow job.\n",
    "\n",
    "Before you can run a Dataflow job, you need to specify the package requirements for the worker pool that will execute the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "write_dataflow_setup:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "import setuptools\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'tensorflow-transform==1.2.0',\n",
    "    'tensorflow-data-validation==1.2'\n",
    "]\n",
    "PACKAGE_NAME = 'my_package'\n",
    "PACKAGE_VERSION = '0.0.1'\n",
    "setuptools.setup(\n",
    "    name=PACKAGE_NAME,\n",
    "    version=PACKAGE_VERSION,\n",
    "    description='preprocessing transformation',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    author=\"cdpe@google.com\",\n",
    "    packages=setuptools.find_packages()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "copy_to_gcs:preprocess",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "#### Copy python module and requirements file to Cloud Storage\n",
    "\n",
    "Next, you copy the Python module, requirements and setup file to your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "copy_to_gcs:preprocess",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://preprocess.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  8.9 KiB/  8.9 KiB]                                                \n",
      "Operation completed over 1 objects/8.9 KiB.                                      \n",
      "Copying file://requirements.txt [Content-Type=text/plain]...\n",
      "/ [1 files][   72.0 B/   72.0 B]                                                \n",
      "Operation completed over 1 objects/72.0 B.                                       \n",
      "Copying file://setup.py [Content-Type=text/x-python]...\n",
      "/ [1 files][  400.0 B/  400.0 B]                                                \n",
      "Operation completed over 1 objects/400.0 B.                                      \n"
     ]
    }
   ],
   "source": [
    "GCS_PREPROCESS_PY = BUCKET_NAME + '/preprocess.py'\n",
    "! gsutil cp preprocess.py $GCS_PREPROCESS_PY\n",
    "GCS_REQUIREMENTS_TXT = BUCKET_NAME + '/requirements.txt'\n",
    "! gsutil cp requirements.txt $GCS_REQUIREMENTS_TXT\n",
    "GCS_SETUP_PY = BUCKET_NAME + '/setup.py'\n",
    "! gsutil cp setup.py $GCS_SETUP_PY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_transform_analysis_component",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create transformed data analysis component\n",
    "\n",
    "Next, you create a component which performs data analysis on the transformed training data using Tensorflow Transform, where:\n",
    "\n",
    "- Load the transformation function artifacts output as `TFTTransformOutput`.\n",
    "- Using the transformed output, determine the number of unique instances per categorical feature.\n",
    "- If the number of unique instances > 10, convert from categorical to embedding feature type.\n",
    "- Update the metadata file for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "make_transform_analysis_component",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"tensorflow\", \"tensorflow-transform\"])\n",
    "def transformed_data_analysis(metadata_location: str,\n",
    "                              transformed_data_prefix: str,\n",
    "                              transform_artifacts_dir: str,\n",
    "                              exported_jsonl_prefix: str,\n",
    "                              exported_tfrec_prefix: str\n",
    "                             ) -> dict:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_transform as tft\n",
    "    import json\n",
    "\n",
    "    tft_output = tft.TFTransformOutput(transform_artifacts_dir)\n",
    "\n",
    "    with tf.io.gfile.GFile(metadata_location, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "    categorical_features = metadata['categorical_features']\n",
    "\n",
    "\n",
    "    CATEGORICAL_FEATURES = []\n",
    "    EMBEDDING_FEATURES = []\n",
    "    for feature in categorical_features:\n",
    "        unique = tft_output.vocabulary_size_by_name(feature)\n",
    "        if unique > 10:\n",
    "            EMBEDDING_FEATURES.append(feature)\n",
    "            print(\"Convert to embedding\", feature, unique)\n",
    "        else:\n",
    "            CATEGORICAL_FEATURES.append(feature)\n",
    "\n",
    "    metadata['categorical_features'] = CATEGORICAL_FEATURES\n",
    "    metadata['embedding_features'] = EMBEDDING_FEATURES\n",
    "\n",
    "    metadata[\"transformed_data_prefix\"] = transformed_data_prefix\n",
    "    metadata[\"transform_artifacts_dir\"] = transform_artifacts_dir\n",
    "    metadata[\"exported_jsonl_prefix\"] = exported_jsonl_prefix\n",
    "    metadata[\"exported_tfrec_prefix\"] = exported_tfrec_prefix\n",
    "\n",
    "    with tf.io.gfile.GFile(metadata_location, \"w\") as f:\n",
    "        json.dump(metadata, f)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_data_pipeline:data_preprocess",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Construct pipeline for data analysis and preprocessing\n",
    "\n",
    "Next, construct the pipeline with the following tasks:\n",
    "\n",
    "- Create the local copy BigQuery dataset.\n",
    "- Perform data analysis on the dataset.\n",
    "- Prepare run arguments for Dataflow script.\n",
    "- Execute the Dataflow script.\n",
    "- Create a Vertex AI Dataset resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "create_data_pipeline:data_preprocess",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"data-preprocessing\",\n",
    "              description=\"Prepare the dataset\"\n",
    "             )\n",
    "def pipeline(bq_table: str,\n",
    "             display_name: str,\n",
    "             transformed_data_prefix: str,\n",
    "             transform_artifacts_dir: str,\n",
    "             exported_tfrec_prefix: str,\n",
    "             exported_jsonl_prefix: str,\n",
    "             label_column: str,\n",
    "             python_file_path: str,\n",
    "             requirements_file_path: str,\n",
    "             setup_file: str,\n",
    "             staging_dir: str,\n",
    "             data_bucket: str,\n",
    "             metadata_location : str,\n",
    "             dataset_labels: str,\n",
    "             year: int,\n",
    "             limit: int,\n",
    "             project: str = PROJECT_ID,\n",
    "             region: str = REGION):\n",
    "    from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "    bq_op = make_chicago_bq_dataset(\n",
    "        bq_table=bq_table,\n",
    "        year=year,\n",
    "        limit=limit,\n",
    "        project=project\n",
    "    )\n",
    "\n",
    "    analysis_op = data_analysis(\n",
    "        bq_table=bq_op.output,\n",
    "        label_column=label_column,\n",
    "        data_bucket=data_bucket,\n",
    "        project=project\n",
    "    )\n",
    "\n",
    "    args_op = make_dataflow_args(\n",
    "        bucket=data_bucket,\n",
    "        setup_file=setup_file,\n",
    "        bq_table=bq_op.output,\n",
    "        metadata=analysis_op.output,\n",
    "        transformed_data_prefix=transformed_data_prefix,\n",
    "        transform_artifacts_dir=transform_artifacts_dir,\n",
    "        exported_tfrec_prefix=exported_tfrec_prefix,\n",
    "        exported_jsonl_prefix=exported_jsonl_prefix,\n",
    "        label=label_column\n",
    "    )\n",
    "\n",
    "    DataflowPythonJobOp.component_spec.implementation.container.image = \"gcr.io/ml-pipeline/google-cloud-pipeline-components:v0.2.0_dataflow_logs_fix\"\n",
    "    dataflow_python_op = DataflowPythonJobOp(\n",
    "        project=project,\n",
    "        location=region,\n",
    "        python_module_path=python_file_path,\n",
    "        temp_location=staging_dir,\n",
    "        requirements_file_path=requirements_file_path,\n",
    "        args=args_op.output,\n",
    "    ).after(bq_op)\n",
    "\n",
    "    dataflow_wait_op = WaitGcpResourcesOp(\n",
    "      gcp_resources = dataflow_python_op.outputs[\"gcp_resources\"])\n",
    "\n",
    "    transformed_analysis_op = transformed_data_analysis(\n",
    "        metadata_location=metadata_location,\n",
    "        transformed_data_prefix=transformed_data_prefix,\n",
    "        transform_artifacts_dir=transform_artifacts_dir,\n",
    "        exported_jsonl_prefix=exported_jsonl_prefix,\n",
    "        exported_tfrec_prefix=exported_tfrec_prefix\n",
    "    ).after(dataflow_wait_op)\n",
    "\n",
    "    dataset_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        bq_source=bq_table,\n",
    "        labels=dataset_labels\n",
    "    ).after(transformed_analysis_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:data_preprocess",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Compile and execute the data analysis and preprocessing pipeline\n",
    "\n",
    "Next, you compile the pipeline and then exeute it. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
    "\n",
    "- `bq_table`: The BigQuery table used for training the model.\n",
    "- `display_name`: The display name for the generated Vertex AI resources.\n",
    "- `transformed_data_prefix`: The Cloud Storage location of the preprocessed training, test and validation data.\n",
    "- `transform_artifacts_dir`: The Cloud Storage location of the transform function artifacts.\n",
    "- `exported_tfrec_prefix`: The Cloud Storage location of the debug/test data for the serving model as TFRecords.\n",
    "- `exported_jsonl_prefix`: The Cloud Storage location of the debug/test data for the serving model in JSONL format.\n",
    "- `label_column`: The name of the label column.\n",
    "- `python_file_path`: The Cloud Storage location of the Dataflow Python script for preprocessing the data.\n",
    "- `requirements_file_path`: The Cloud Storage location of the requirements.txt file for the Dataflow component.\n",
    "- `setup_file`: The Cloud Storage location of the setup.py script for the Dataflow component.\n",
    "- `staging_dir`: The Cloud Storage location for the temporary location for the Apache Beam pipeline.\n",
    "- `data_bucket`: The Cloud Storage location for data analysis artifacts.\n",
    "- `metadata_location`: The Cloud Storage location for the Vertex AI Dataset user metadata.\n",
    "- `dataset_labels`: User defined labels to add to the Vertex AI Dataset -- i.e., metadata location\n",
    "- `project`: The project ID.\n",
    "- `region`: The region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "run_pipeline:data_preprocess",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/759209241365/locations/us-central1/pipelineJobs/data-preprocessing-20211210022853\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/759209241365/locations/us-central1/pipelineJobs/data-preprocessing-20211210022853')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/data-preprocessing-20211210022853?project=759209241365\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/759209241365/locations/us-central1/pipelineJobs/data-preprocessing-20211210022853 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/759209241365/locations/us-central1/pipelineJobs/data-preprocessing-20211210022853 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/759209241365/locations/us-central1/pipelineJobs/data-preprocessing-20211210022853 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/759209241365/locations/us-central1/pipelineJobs/data-preprocessing-20211210022853 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/759209241365/locations/us-central1/pipelineJobs/data-preprocessing-20211210022853 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/759209241365/locations/us-central1/pipelineJobs/data-preprocessing-20211210022853\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/data_preprocess\".format(BUCKET_NAME)\n",
    "\n",
    "EXPORTED_JSONL_PREFIX = os.path.join(BUCKET_NAME, 'exported_data/jsonl')\n",
    "EXPORTED_TFREC_PREFIX = os.path.join(BUCKET_NAME, 'exported_data/tfrec')\n",
    "TRANSFORMED_DATA_PREFIX = os.path.join(BUCKET_NAME, 'transformed_data')\n",
    "TRANSFORM_ARTIFACTS_DIR = os.path.join(BUCKET_NAME, 'transformed_artifacts')\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"data_preprocess.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"data_preprocess\",\n",
    "    template_path=\"data_preprocess.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values = { 'bq_table': IMPORT_FILE,\n",
    "                         'display_name': \"Chicago Taxi\" + TIMESTAMP,\n",
    "                         'transformed_data_prefix': TRANSFORMED_DATA_PREFIX,\n",
    "                         'transform_artifacts_dir': TRANSFORM_ARTIFACTS_DIR,\n",
    "                         'exported_tfrec_prefix': EXPORTED_TFREC_PREFIX,\n",
    "                         'exported_jsonl_prefix': EXPORTED_JSONL_PREFIX,\n",
    "                         'label_column': \"tip_bin\",\n",
    "                         'python_file_path' : GCS_PREPROCESS_PY,\n",
    "                         'requirements_file_path': GCS_REQUIREMENTS_TXT,\n",
    "                         'setup_file': GCS_SETUP_PY,\n",
    "                         'staging_dir': PIPELINE_ROOT,\n",
    "                         'data_bucket': BUCKET_NAME,\n",
    "                         'metadata_location': BUCKET_NAME + '/metadata.jsonl',\n",
    "                         'dataset_labels': str({ 'user_metadata': BUCKET_NAME[5:]}).replace(\"'\", '\"'),\n",
    "                         'year': 2020,\n",
    "                         'limit': 300000,\n",
    "                         'project': PROJECT_ID,\n",
    "                         'region': REGION\n",
    "                       }\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "! rm -f data_preprocess.json requirements.txt setup.py preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipleline_results:data_preprocess,chicago",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### View the data pipeline execution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "view_pipleline_results:data_preprocess,chicago",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759209241365\n",
      "make-chicago-bq-dataset\n",
      "data-preprocessing-20211210022853\n",
      "\n",
      "\n",
      "data-analysis\n",
      "data-preprocessing-20211210022853\n",
      "\n",
      "\n",
      "make-dataflow-args\n",
      "data-preprocessing-20211210022853\n",
      "\n",
      "\n",
      "dataflow-python\n",
      "data-preprocessing-20211210022853\n",
      "\n",
      "\n",
      "wait-gcp-resources\n",
      "data-preprocessing-20211210022853\n",
      "\n",
      "\n",
      "transformed-data-analysis\n",
      "data-preprocessing-20211210022853\n",
      "{\"parameters\": {\"Output\": {\"stringValue\": \"{\\\"label_column\\\": \\\"tip_bin\\\", \\\"statistics\\\": \\\"gs://andy-1234-221921aip-20211210003020/statistics.jsonl\\\", \\\"schema\\\": \\\"gs://andy-1234-221921aip-20211210003020/schema.txt\\\", \\\"numeric_features\\\": [\\\"trip_seconds\\\", \\\"trip_miles\\\", \\\"euclidean\\\"], \\\"categorical_features\\\": [\\\"trip_month\\\", \\\"trip_day_of_week\\\", \\\"payment_type\\\"], \\\"embedding_features\\\": [], \\\"transformed_data_prefix\\\": \\\"gs://andy-1234-221921aip-20211210003020/transformed_data\\\", \\\"transform_artifacts_dir\\\": \\\"gs://andy-1234-221921aip-20211210003020/transformed_artifacts\\\", \\\"exported_jsonl_prefix\\\": \\\"gs://andy-1234-221921aip-20211210003020/exported_data/jsonl\\\", \\\"exported_tfrec_prefix\\\": \\\"gs://andy-1234-221921aip-20211210003020/exported_data/tfrec\\\"}\"}}}\n",
      "\n",
      "tabulardataset-create\n",
      "data-preprocessing-20211210022853\n",
      "{\"artifacts\": {\"dataset\": {\"artifacts\": [{\"name\": \"projects/759209241365/locations/us-central1/metadataStores/default/artifacts/13487403990849440218\", \"uri\": \"https://us-central1-aiplatform.googleapis.com/v1/projects/759209241365/locations/us-central1/datasets/4560167301596315648\", \"metadata\": {\"resourceName\": \"projects/759209241365/locations/us-central1/datasets/4560167301596315648\"}}]}}}\n",
      "\n",
      "projects/759209241365/locations/us-central1/datasets/4560167301596315648\n"
     ]
    }
   ],
   "source": [
    "PROJECT_NUMBER = pipeline.gca_resource.name.split('/')[1]\n",
    "print(PROJECT_NUMBER)\n",
    "\n",
    "def print_pipeline_output(job, output_task_name):\n",
    "    JOB_ID = job.name\n",
    "    print(JOB_ID)\n",
    "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "        EXECUTE_OUTPUT = PIPELINE_ROOT + '/' + PROJECT_NUMBER + '/' + JOB_ID + '/' + output_task_name + '_' + str(TASK_ID) + '/executor_output.json'\n",
    "        GCP_RESOURCES = PIPELINE_ROOT + '/' + PROJECT_NUMBER + '/' + JOB_ID + '/' + output_task_name + '_' + str(TASK_ID) + '/gcp_resources'\n",
    "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
    "            ! gsutil cat $EXECUTE_OUTPUT\n",
    "            break\n",
    "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
    "            ! gsutil cat $GCP_RESOURCES\n",
    "            break\n",
    "\n",
    "    return EXECUTE_OUTPUT\n",
    "\n",
    "print(\"make-chicago-bq-dataset\")\n",
    "artifacts = print_pipeline_output(pipeline, 'make-chicago-bq-dataset')\n",
    "print('\\n')\n",
    "print(\"data-analysis\")\n",
    "artifacts = print_pipeline_output(pipeline, 'data-analysis')\n",
    "print('\\n')\n",
    "print(\"make-dataflow-args\")\n",
    "artifacts = print_pipeline_output(pipeline, 'make-dataflow-args')\n",
    "print('\\n')\n",
    "print(\"dataflow-python\")\n",
    "artifacts = print_pipeline_output(pipeline, 'dataflow-python')\n",
    "print('\\n')\n",
    "print(\"wait-gcp-resources\")\n",
    "artifacts = print_pipeline_output(pipeline, 'wait-gcp-resources')\n",
    "print('\\n')\n",
    "print(\"transformed-data-analysis\")\n",
    "artifacts = print_pipeline_output(pipeline, 'transformed-data-analysis')\n",
    "print('\\n')\n",
    "print(\"tabulardataset-create\")\n",
    "artifacts = print_pipeline_output(pipeline, 'tabulardataset-create')\n",
    "print('\\n')\n",
    "\n",
    "output = !gsutil cat $artifacts\n",
    "output = json.loads(output[0])\n",
    "dataset_id = output['artifacts']['dataset']['artifacts'][0]['metadata']['resourceName']\n",
    "print(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "formalize_model_pipeline_intro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Formalizing model pipeline introduction\n",
    "\n",
    "The model pipeline consists of building the model architecture task.\n",
    "\n",
    "### Build model architecture task\n",
    "\n",
    "BLAH\n",
    "\n",
    "### Triggers\n",
    "\n",
    "Within the CI/CD process, the model pipeline is triggered for one or more of the following example reasons, while not exhaustive:\n",
    "\n",
    "- BLAH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_model_build_component",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create build model architecture component\n",
    "\n",
    "Next, you create a component which creates the base model architecture. Note, the base model is untrained. In this example, the model architecture is for a tabular model, where:\n",
    "\n",
    "- Load the corresonding Vertex AI Dataset,\n",
    "- Load the dataset metadata.\n",
    "- Use the metadata information on the feature types to build the input layer.\n",
    "- Build the DNN body of the model.\n",
    "- Save the base model artifacts to the Cloud Storage location.\n",
    "- Create a Vertex AI Model resource for the base model.\n",
    "\n",
    "The component returns the full resource name of the generated Vertex AI Model resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "create_model_build_component",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"tensorflow==2.5\", \"tensorflow-transform\"])\n",
    "def build_model(dataset_id: str,\n",
    "                display_name: str,\n",
    "                deploy_image: str,\n",
    "                bucket: str,\n",
    "                project: str\n",
    "               ) -> str:\n",
    "\n",
    "    import subprocess\n",
    "    subprocess.call([\"pip3\", \"install\", \"google-cloud-aiplatform\"])\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.layers import Input\n",
    "    from tensorflow.keras import Sequential, Model\n",
    "    from tensorflow.keras.layers import Input, Dense, Concatenate, Activation, Embedding, experimental\n",
    "    import tensorflow_transform as tft\n",
    "    import google.cloud.aiplatform as aip\n",
    "    import logging\n",
    "    from math import sqrt\n",
    "    import json\n",
    "    \n",
    "    logging.info('Tensorflow version: ' + tf.__version__)\n",
    "    \n",
    "    aip.init(project=project, staging_bucket=bucket, experiment=display_name)\n",
    "    aip.start_run(run='retrain')\n",
    "\n",
    "    # Load the dataset resource from the dataset resource ID.\n",
    "    dataset = aip.TabularDataset(dataset_id)\n",
    "\n",
    "    # Load the metadata for this dataset\n",
    "    with tf.io.gfile.GFile('gs://' + dataset.labels['user_metadata'] + \"/metadata.jsonl\", \"r\") as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "    def create_model_inputs(numeric_features=None, categorical_features=None, embedding_features=None):\n",
    "        inputs = {}\n",
    "        for feature_name in numeric_features:\n",
    "            inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.float32)\n",
    "        for feature_name in categorical_features:\n",
    "            inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.int64)\n",
    "        for feature_name in embedding_features:\n",
    "            inputs[feature_name] = Input(name=feature_name, shape=[], dtype=tf.int64)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    input_layers = create_model_inputs(\n",
    "        numeric_features=metadata['numeric_features'],\n",
    "        categorical_features=metadata['categorical_features'],\n",
    "        embedding_features=metadata['embedding_features']\n",
    "     )\n",
    "\n",
    "    logging.info(\"Created input layers for model\")\n",
    "    logging.info(input_layers)\n",
    "\n",
    "    def create_binary_classifier(input_layers, tft_output, metaparams, numeric_features, categorical_features, embedding_features):\n",
    "        layers = []\n",
    "        for feature_name in input_layers:\n",
    "            if feature_name in embedding_features:\n",
    "                vocab_size = tft_output.vocabulary_size_by_name(feature_name)\n",
    "                embedding_size = int(sqrt(vocab_size))\n",
    "                embedding_output = Embedding(\n",
    "                    input_dim=vocab_size + 1,\n",
    "                    output_dim=embedding_size,\n",
    "                    name=f\"{feature_name}_embedding\",\n",
    "                )(input_layers[feature_name])\n",
    "                layers.append(embedding_output)\n",
    "            elif feature_name in categorical_features:\n",
    "                vocab_size = tft_output.vocabulary_size_by_name(feature_name)\n",
    "                onehot_layer = experimental.preprocessing.CategoryEncoding(\n",
    "                    num_tokens=vocab_size,\n",
    "                    output_mode=\"binary\",\n",
    "                    name=f\"{feature_name}_onehot\",\n",
    "                )(input_layers[feature_name])\n",
    "                layers.append(onehot_layer)\n",
    "            elif feature_name in numeric_features:\n",
    "                numeric_layer = tf.expand_dims(input_layers[feature_name], -1)\n",
    "                layers.append(numeric_layer)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        logging.info(\"Created layers for model\")\n",
    "        logging.info(layers)\n",
    "\n",
    "        joined = Concatenate(name=\"combines_inputs\")(layers)\n",
    "        feedforward_output = Sequential(\n",
    "            [\n",
    "                Dense(units, activation=\"relu\")\n",
    "                for units in metaparams[\"hidden_units\"]\n",
    "            ],\n",
    "            name=\"feedforward_network\",\n",
    "        )(joined)\n",
    "        logits = Dense(units=1, name=\"logits\")(feedforward_output)\n",
    "        pred = Activation(\"sigmoid\")(logits)\n",
    "\n",
    "        model = Model(inputs=input_layers, outputs=[pred])\n",
    "        return model\n",
    "\n",
    "    TRANSFORM_ARTIFACTS_DIR = metadata['transform_artifacts_dir']\n",
    "    tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "\n",
    "    metaparams = {'hidden_units': [ 128, 64 ] }\n",
    "    aip.log_params(metaparams)\n",
    "\n",
    "    model = create_binary_classifier(\n",
    "        input_layers,\n",
    "        tft_output, metaparams,\n",
    "        numeric_features=metadata['numeric_features'],\n",
    "        categorical_features=metadata['categorical_features'],\n",
    "        embedding_features=metadata['embedding_features']\n",
    "    )\n",
    "\n",
    "    logging.info(\"Created binary classifier model\")\n",
    "    logging.info(model.summary)\n",
    "\n",
    "    logging.info(\"Save base model architecture\")\n",
    "    MODEL_DIR = f\"{bucket}/base_model\"\n",
    "    model.save(MODEL_DIR)\n",
    "\n",
    "    return MODEL_DIR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_model_pipeline:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Construct pipeline for building the model architecture\n",
    "\n",
    "Next, construct the pipeline with the following tasks:\n",
    "\n",
    "- Build the base model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "create_model_pipeline:tabular",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"build-model\",\n",
    "              description=\"Build the base model architecture\"\n",
    "             )\n",
    "def pipeline(dataset_id: str,\n",
    "             display_name: str,\n",
    "             deploy_image: str,\n",
    "             bucket: str,\n",
    "             project: str = PROJECT_ID,\n",
    "             region: str = REGION,\n",
    "             labels: list = [{'base_model': 1}]\n",
    "            ):\n",
    "    from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "    \n",
    "    model_build_op = build_model(\n",
    "        dataset_id=dataset_id,\n",
    "        display_name=display_name,\n",
    "        deploy_image=deploy_image,\n",
    "        bucket=bucket,\n",
    "        project=project\n",
    "    )\n",
    "    \n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=model_build_op.output,\n",
    "        serving_container_image_uri=deploy_image,\n",
    "        # BUG\n",
    "        #labels=labels,\n",
    "        project=project,\n",
    "        location=region\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run_pipeline:model_build",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Compile and execute the build model architecture pipeline\n",
    "\n",
    "Next, you compile the pipeline and then execute it. The pipeline takes the following parameters, which are passed as the dictionary `parameter_values`:\n",
    "\n",
    "- `dataset_id`: The full resource name of the corresponding Vertex AI Dataset.\n",
    "- `display_name`: The display name for the generated Vertex AI Model resource.\n",
    "- `deploy_image`: The associated deployment container image.\n",
    "- `bucket`: The Cloud Storage location to store the model artifacts.\n",
    "- `project`: The project ID.\n",
    "- `region`: The region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "run_pipeline:model_build",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/759209241365/locations/us-central1/pipelineJobs/build-model-20211210181728\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/759209241365/locations/us-central1/pipelineJobs/build-model-20211210181728')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/build-model-20211210181728?project=759209241365\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/759209241365/locations/us-central1/pipelineJobs/build-model-20211210181728 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/759209241365/locations/us-central1/pipelineJobs/build-model-20211210181728 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob projects/759209241365/locations/us-central1/pipelineJobs/build-model-20211210181728 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob run completed. Resource name: projects/759209241365/locations/us-central1/pipelineJobs/build-model-20211210181728\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_ROOT = \"{}/pipeline_root/model_build\".format(BUCKET_NAME)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"model_build.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"model-build\",\n",
    "    template_path=\"model_build.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values = { 'dataset_id': dataset_id,\n",
    "                         'display_name': \"chicago\" + TIMESTAMP,\n",
    "                         'deploy_image': DEPLOY_IMAGE,\n",
    "                         'bucket': BUCKET_NAME,\n",
    "                         'project': PROJECT_ID,\n",
    "                         'region': REGION\n",
    "                       }\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "! rm -f model_build.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipleline_results:model_build",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### View BLAH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "view_pipleline_results:model_build",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-build\n",
      "build-model-20211210181728\n",
      "\n",
      "\n",
      "model-upload\n",
      "build-model-20211210181728\n",
      "{\"artifacts\": {\"model\": {\"artifacts\": [{\"metadata\": {\"resourceName\": \"projects/759209241365/locations/us-central1/models/3745521544391032832\"}, \"name\": \"projects/759209241365/locations/us-central1/metadataStores/default/artifacts/11241523048246947948\", \"type\": {\"schemaTitle\": \"google.VertexModel\"}, \"uri\": \"https://us-central1-aiplatform.googleapis.com/v1/projects/759209241365/locations/us-central1/models/3745521544391032832\"}]}}}\n",
      "\n",
      "projects/759209241365/locations/us-central1/models/3745521544391032832\n"
     ]
    }
   ],
   "source": [
    "print(\"model-build\")\n",
    "artifacts = print_pipeline_output(pipeline, 'model-build')\n",
    "print('\\n')\n",
    "print(\"model-upload\")\n",
    "artifacts = print_pipeline_output(pipeline, 'model-upload')\n",
    "print('\\n')\n",
    "\n",
    "output = !gsutil cat $artifacts\n",
    "output = json.loads(output[0])\n",
    "model_id = output['artifacts']['model']['artifacts'][0]['metadata']['resourceName']\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "construct_training_package",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Construct the training package\n",
    "\n",
    "#### Package layout\n",
    "\n",
    "Before you start training, you will look at how a Python package is assembled for a custom training job. When unarchived, the package contains the following directory/file layout.\n",
    "\n",
    "- PKG-INFO\n",
    "- README.md\n",
    "- setup.cfg\n",
    "- setup.py\n",
    "- trainer\n",
    "  - \\_\\_init\\_\\_.py\n",
    "  - task.py\n",
    "  - other Python scripts\n",
    "\n",
    "The files `setup.cfg` and `setup.py` are the instructions for installing the package into the operating environment of the Docker image.\n",
    "\n",
    "The file `trainer/task.py` is the Python script for executing the custom training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "construct_training_package",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'google-cloud-aiplatform',\\n\\n        'cloudml-hypertune',\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Chicago Taxi tabular binary classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: cdpe@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex AI\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "read_tfrecords_func",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Load the transformed data into a tf.data.Dataset\n",
    "\n",
    "Next, you load the gzip TFRecords on Cloud Storage storage into a `tf.data.Dataset` generator. These functions are re-used when training the custom model using `Vertex Training`, so you save them to the python training package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "read_tfrecords_func",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/data.py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "    return tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, feature_spec, label_column, batch_size=200):\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "    Args:\n",
    "      file_pattern: input tfrecord file pattern.\n",
    "      feature_spec: a dictionary of feature specifications.\n",
    "      batch_size: representing the number of consecutive elements of returned\n",
    "        dataset to combine in a single batch\n",
    "    Returns:\n",
    "      A dataset that contains (features, indices) tuple where features is a\n",
    "        dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        batch_size=batch_size,\n",
    "        features=feature_spec,\n",
    "        label_key=label_column,\n",
    "        reader=_gzip_reader_fn,\n",
    "        num_epochs=1,\n",
    "        drop_final_batch=True,\n",
    "    )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_model_func",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Develop and test the training scripts\n",
    "\n",
    "When experimenting, one typically develops and tests the training package locally, before moving to training in the cloud.\n",
    "\n",
    "### Create training script\n",
    "\n",
    "Next, you write the Python script for compiling and training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "train_model_func",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/train.py\n",
    "\n",
    "from trainer import data\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from hypertune import HyperTune\n",
    "\n",
    "def compile(model, hyperparams):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=hyperparams[\"learning_rate\"])\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    metrics = [tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")]\n",
    "\n",
    "    model.compile(optimizer=optimizer,loss=loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    hyperparams,\n",
    "    train_data_dir,\n",
    "    val_data_dir,\n",
    "    label_column,\n",
    "    transformed_feature_spec,\n",
    "    log_dir,\n",
    "    tuning=False\n",
    "):\n",
    "\n",
    "    train_dataset = data.get_dataset(\n",
    "        train_data_dir,\n",
    "        transformed_feature_spec,\n",
    "        label_column,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    val_dataset = data.get_dataset(\n",
    "        val_data_dir,\n",
    "        transformed_feature_spec,\n",
    "        label_column,\n",
    "        batch_size=hyperparams[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=hyperparams[\"early_stop\"][\"monitor\"], patience=hyperparams[\"early_stop\"][\"patience\"], restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    callbacks=[tensorboard, early_stop]\n",
    "\n",
    "    if tuning:\n",
    "        # Instantiate the HyperTune reporting object\n",
    "        hpt = HyperTune()\n",
    "\n",
    "        # Reporting callback\n",
    "        class HPTCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "            def on_epoch_end(self, epoch, logs=None):\n",
    "                hpt.report_hyperparameter_tuning_metric(\n",
    "                    hyperparameter_metric_tag='val_loss',\n",
    "                    metric_value=logs['val_loss'],\n",
    "                    global_step=epoch\n",
    "                )\n",
    "\n",
    "        callbacks.append(HPTCallback())\n",
    "\n",
    "    logging.info(\"Model training started...\")\n",
    "    history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=hyperparams[\"num_epochs\"],\n",
    "            validation_data=val_dataset,\n",
    "            callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    logging.info(\"Model training completed.\")\n",
    "    return history\n",
    "\n",
    "def evaluate(\n",
    "    model,\n",
    "    hyperparams,\n",
    "    test_data_dir,\n",
    "    label_column,\n",
    "    transformed_feature_spec\n",
    "):\n",
    "    logging.info(\"Model evaluation started...\")\n",
    "    test_dataset = data.get_dataset(\n",
    "        test_data_dir,\n",
    "        transformed_feature_spec,\n",
    "        label_column,\n",
    "        hyperparams[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    evaluation_metrics = model.evaluate(test_dataset)\n",
    "    logging.info(\"Model evaluation completed.\")\n",
    "\n",
    "    return evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_model_get",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Retrieve model from Vertex AI\n",
    "\n",
    "Next, create the Python script to retrieve your experimental model from Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "create_model_get",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/model.py\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "def get(model_id):\n",
    "    model = aip.Model(model_id)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_task_py",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Create the task script for the Python training package\n",
    "\n",
    "Next, you create the `task.py` script for driving the training package. Some noteable steps include:\n",
    "\n",
    "- Command-line arguments:\n",
    "    - `model-id`: The resource ID of the `Model` resource you built during experimenting. This is the untrained model architecture.\n",
    "    - `dataset-id`: The resource ID of the `Dataset` resource to use for training.\n",
    "    - `experiment`: The name of the experiment.\n",
    "    - `run`: The name of the run within this experiment.\n",
    "    - `tensorboard-logdir`: The logging directory for Vertex AI Tensorboard.\n",
    "\n",
    "\n",
    "- `get_data()`:\n",
    "    - Loads the Dataset resource into memory.\n",
    "    - Obtains the user metadata from the Dataset resource.\n",
    "    - From the metadata, obtain location of transformed data, transformation function and name of label column\n",
    "\n",
    "\n",
    "- `get_model()`:\n",
    "    - Loads the Model resource into memory.\n",
    "    - Obtains location of model artifacts of the model architecture.\n",
    "    - Loads the model architecture.\n",
    "    - Compiles the model.\n",
    "\n",
    "\n",
    "- `train_model()`:\n",
    "    - Train the model.\n",
    "\n",
    "\n",
    "- `evaluate_model()`:\n",
    "    - Evaluates the model.\n",
    "    - Saves evaluation metrics to Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "create_task_py",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "import os\n",
    "import argparse\n",
    "import logging\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "from trainer import data\n",
    "from trainer import model as model_\n",
    "from trainer import train\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "parser.add_argument('--model-id', dest='model_id',\n",
    "                    default=None, type=str, help='Vertex Model ID.')\n",
    "parser.add_argument('--dataset-id', dest='dataset_id',\n",
    "                    default=None, type=str, help='Vertex Dataset ID.')\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.001, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=20, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=200, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--batch_size', dest='batch_size',\n",
    "                    default=16, type=int,\n",
    "                    help='Batch size.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "parser.add_argument('--tensorboard-log-dir', dest='tensorboard_log_dir',\n",
    "                    default='/tmp/logs', type=str,\n",
    "                    help='Output file for tensorboard logs')\n",
    "parser.add_argument('--experiment', dest='experiment',\n",
    "                    default=None, type=str,\n",
    "                    help='Name of experiment')\n",
    "parser.add_argument('--project', dest='project',\n",
    "                    default=None, type=str,\n",
    "                    help='Name of project')\n",
    "parser.add_argument('--run', dest='run',\n",
    "                    default=None, type=str,\n",
    "                    help='Name of run in experiment')\n",
    "parser.add_argument('--evaluate', dest='evaluate',\n",
    "                    default=False, type=bool,\n",
    "                    help='Whether to perform evaluation')\n",
    "parser.add_argument('--tuning', dest='tuning',\n",
    "                    default=False, type=bool,\n",
    "                    help='Whether to perform hyperparameter tuning')\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info('DEVICES'  + str(device_lib.list_local_devices()))\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "    logging.info(\"Single device training\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirrored':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    logging.info(\"Mirrored Strategy distributed training\")\n",
    "# Multi Machine, multiple compute device\n",
    "elif args.distribute == 'multiworker':\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    logging.info(\"Multi-worker Strategy distributed training\")\n",
    "    logging.info('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "logging.info('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Initialize the run for this experiment\n",
    "if args.experiment:\n",
    "    logging.info(\"Initialize experiment: {}\".format(args.experiment))\n",
    "    aip.init(experiment=args.experiment, project=args.project)\n",
    "    aip.start_run(args.run)\n",
    "\n",
    "def get_data():\n",
    "    ''' Get the preprocessed training data '''\n",
    "    global train_data_file_pattern, val_data_file_pattern, test_data_file_pattern\n",
    "    global label_column, transform_feature_spec\n",
    "\n",
    "    dataset = aip.TabularDataset(args.dataset_id)\n",
    "    METADATA = 'gs://' + dataset.labels['user_metadata'] + \"/metadata.jsonl\"\n",
    "\n",
    "    with tf.io.gfile.GFile(METADATA, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    TRANSFORMED_DATA_PREFIX = metadata['transformed_data_prefix']\n",
    "    label_column = metadata['label_column']\n",
    "\n",
    "    train_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/train/data-*.gz'\n",
    "    val_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/val/data-*.gz'\n",
    "    test_data_file_pattern = TRANSFORMED_DATA_PREFIX + '/test/data-*.gz'\n",
    "\n",
    "    TRANSFORM_ARTIFACTS_DIR = metadata['transform_artifacts_dir']\n",
    "    tft_output = tft.TFTransformOutput(TRANSFORM_ARTIFACTS_DIR)\n",
    "    transform_feature_spec = tft_output.transformed_feature_spec()\n",
    "\n",
    "def get_model():\n",
    "    ''' Get the untrained model architecture '''\n",
    "    vertex_model = model_.get(args.model_id)\n",
    "    model_artifacts = vertex_model.gca_resource.artifact_uri\n",
    "    model = tf.keras.models.load_model(model_artifacts)\n",
    "\n",
    "    # Compile the model\n",
    "    hyperparams = {}\n",
    "    hyperparams[\"learning_rate\"] = args.lr\n",
    "    if args.experiment:\n",
    "        aip.log_params(hyperparams)\n",
    "\n",
    "    train.compile(model, hyperparams)\n",
    "    return model\n",
    "\n",
    "def train_model(model):\n",
    "    ''' Train the model '''\n",
    "    trainparams = {}\n",
    "    trainparams[\"num_epochs\"] = args.epochs\n",
    "    trainparams[\"batch_size\"] = args.batch_size\n",
    "    trainparams[\"early_stop\"] = {\"monitor\": \"val_loss\", \"patience\": 5}\n",
    "    if args.experiment:\n",
    "        aip.log_params(trainparams)\n",
    "    train.train(model, trainparams, train_data_file_pattern, val_data_file_pattern, label_column, transform_feature_spec, args.tensorboard_log_dir, args.tuning)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model):\n",
    "    ''' Evaluate the model '''\n",
    "    evalparams = {}\n",
    "    evalparams[\"batch_size\"] = args.batch_size\n",
    "    metrics = train.evaluate(model, evalparams, test_data_file_pattern, label_column, transform_feature_spec)\n",
    "    with tf.io.gfile.GFile(os.path.join(args.model_dir, \"metrics.txt\", \"w\")) as f:\n",
    "        f.write(str(metrics))\n",
    "\n",
    "get_data()\n",
    "with strategy.scope():\n",
    "    model = get_model()\n",
    "model = train_model(model)\n",
    "\n",
    "if args.evaluate:\n",
    "    evaluate_model(model)\n",
    "\n",
    "logging.info('Save trained model to: ' + args.model_dir)\n",
    "model.save(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_package_locally",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Test training package locally\n",
    "\n",
    "Next, test your completed training package locally with just a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "test_package_locally",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vertex_custom_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-fe89d36f42b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDATASET_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mMODEL_ID\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvertex_custom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cd custom; python3 -m trainer.task --model-id={MODEL_ID} --dataset-id={DATASET_ID} --experiment='chicago' --run='test' --project={PROJECT_ID} --epochs=5 --model-dir=/tmp --evaluate=True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vertex_custom_model' is not defined"
     ]
    }
   ],
   "source": [
    "DATASET_ID = dataset.resource_name\n",
    "MODEL_ID = vertex_custom_model.resource_name\n",
    "!cd custom; python3 -m trainer.task --model-id={MODEL_ID} --dataset-id={DATASET_ID} --experiment='chicago' --run='test' --project={PROJECT_ID} --epochs=5 --model-dir=/tmp --evaluate=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_custom_pipeline:lbn",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Construct custom training pipeline\n",
    "\n",
    "In the example below, you construct a pipeline for training a custom model using pre-built Google Cloud Pipeline Components for Vertex AI Training, as follows:\n",
    "\n",
    "\n",
    "1. Pipeline arguments, specify the locations of:\n",
    "    - `import_file`: The CSV index file for the dataset.\n",
    "    - `python_package`: The custom training Python package.\n",
    "    - `python_module`: The entry module in the package to execute.\n",
    "\n",
    "2. Use the prebuilt component `TabularDatasetCreateOp` to create a Vertex AI Dataset resource, where:\n",
    "    - The display name for the dataset is passed into the pipeline.\n",
    "    - The import file for the dataset is passed into the pipeline.\n",
    "    - The component returns the dataset resource as `outputs[\"dataset\"]`\n",
    "3. Use the prebuilt component `CustomPythonPackageTrainingJobRunOp` to train a custom model and upload the custom model as a Vertex AI Model resource, where:\n",
    "    - The display name for the dataset is passed into the pipeline.\n",
    "    - The dataset is the output from the `TabularDatasetCreateOp`.\n",
    "    - The python package, command line argument are passed into the pipeline.\n",
    "    - The training and serving containers are specified in the pipeline definition.\n",
    "    - The component returns the model resource as `outputs[\"model\"]`.\n",
    "4. Use the prebuilt component `EndpointCreateOp` to create a Vertex AI Endpoint to deploy the trained model to, where:\n",
    "    - Since the component has no dependencies on other components, by default it would be executed in parallel with the model training.\n",
    "    - The `after(training_op)` is added to serialize its execution, so its only executed if the training operation completes successfully.\n",
    "     - The component returns the endpoint resource as `outputs[\"endpoint\"]`.\n",
    "5. Use the prebuilt component `ModelDeployOp` to deploy the trained Vertex AI model to, where:\n",
    "    - The display name for the dataset is passed into the pipeline.\n",
    "    - The model is the output from the `CustomPythonPackageTrainingJobRunOp`.\n",
    "    - The endpoint is the output from the `EndpointCreateOp`\n",
    "\n",
    "*Note:* Since each component is executed as a graph node in its own execution context, you pass the parameter `project` for each component op, in constrast to doing a `aip.init(project=project)` if this was a Python script calling the SDK methods directly within the same execution context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "create_custom_pipeline:lbn",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/custom_lbn_training\".format(BUCKET_NAME)\n",
    "\n",
    "@dsl.pipeline(name=\"chicago-custom-training\",\n",
    "              description=\"Custom tabular binary classification training\"\n",
    "             )\n",
    "def pipeline(import_file: str,\n",
    "             display_name: str,\n",
    "             python_package: str,\n",
    "             python_module: str,\n",
    "             project: str = PROJECT_ID,\n",
    "             region: str = REGION):\n",
    "\n",
    "    dataset_op = gcc_aip.TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        display_name=display_name,\n",
    "        bq_source=import_file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:stage3",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "delete_all = False\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if 'dataset' in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if 'model' in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # delete the BQ table\n",
    "    # delete the pipeline\n",
    "\n",
    "    if 'BUCKET_NAME' in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://andy-1234-221921aip-20211210003020/metadata.jsonl#1639103474503094...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/preprocess.py#1639096360091002...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/requirements.txt#1639096361848666...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/schema.txt#1639096478320034... \n",
      "/ [4 objects]                                                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Removing gs://andy-1234-221921aip-20211210003020/setup.py#1639096363629851...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/statistics.jsonl#1639096478109361...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/base_model/#1639157525065872...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/base_model/assets/#1639157527847924...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/base_model/keras_metadata.pb#1639157528523283...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/base_model/saved_model.pb#1639157528288609...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/base_model/variables/#1639157525214653...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/base_model/variables/variables.data-00000-of-00001#1639157527019886...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/base_model/variables/variables.index#1639157527278516...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/exported_data/jsonl-00000-of-00003.jsonl#1639097319624222...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/exported_data/jsonl-00001-of-00003.jsonl#1639097319628799...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/exported_data/jsonl-00002-of-00003.jsonl#1639097319614578...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/exported_data/tfrec/data-00000-of-00003.tfrecord#1639097315646190...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/exported_data/tfrec/data-00001-of-00003.tfrecord#1639097315615188...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/exported_data/tfrec/data-00002-of-00003.tfrecord#1639097315623052...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/#1639096479389191...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/#1639096479912081...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/#1639096480366291...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210003245/#1639096480977748...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210003245/data-analysis_8333361354635214848/#1639096481771403...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210003245/data-analysis_8333361354635214848/executor_output.json#1639096482770923...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210003245/dataflow-python_-890010682219560960/#1639096595301717...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210003245/dataflow-python_-890010682219560960/gcp_resources#1639096691303598...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210003245/make-dataflow-args_-8384000462164066304/#1639096536494110...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210003245/make-dataflow-args_-8384000462164066304/executor_output.json#1639096537605433...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210022853/#1639103476054409...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210022853/tabulardataset-create_-7661172721971101696/#1639103563373746...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210022853/tabulardataset-create_-7661172721971101696/executor_output.json#1639103564321594...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210022853/transformed-data-analysis_6173885333311062016/#1639103476670037...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/759209241365/data-preprocessing-20211210022853/transformed-data-analysis_6173885333311062016/executor_output.json#1639103477605493...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/beamapp-root-1210003801-255969.1639096681.256711/apache_beam-2.34.0-cp37-cp37m-manylinux1_x86_64.whl#1639096685982988...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/beamapp-root-1210003801-255969.1639096681.256711/dataflow_python_sdk.tar#1639096685368681...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/beamapp-root-1210003801-255969.1639096681.256711/pickled_main_session#1639096685123691...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/beamapp-root-1210003801-255969.1639096681.256711/pipeline.pb#1639096686117134...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/beamapp-root-1210003801-255969.1639096681.256711/workflow.tar.gz#1639096685410620...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/#1639096650510512...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/#1639096650817328...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/05bf109abdd44d1c96af6ecb7f8ec9f9#1639097374794369...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/0985774f0709428aba8cb4a820c702bd#1639097158571431...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/0e3fd3bd6374408a981f6ace1ebea78d#1639097390061575...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/1d641afa79114180b735cd11bc50dd66#1639096654066059...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/1d9db48a001347c78626026d5a9859d9#1639096651115983...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/1e4d03f9a5af4b3289e975f0428dc987#1639096652555887...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/1efd0a03ca664f9198cfa9d3a0a87dcc#1639097374152118...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/21801a0dd28246a58ec6b4748739266c#1639097373555935...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/240405dfbaab4479b91eda6fd205dd7c#1639096663698327...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/25d8a47f160642609a27026b12f637f1#1639097159248973...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/327b5bb45d5749f5b246ac7b246070b2#1639097391415873...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/32f9bfdce6914d7d836a39ef2ef7dd16#1639097388966828...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/3635911e2dec4470961523c19161db07#1639097388292244...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/379e10e5756f42108dd95dc4477783e4#1639096661029210...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/3915595553d44e8e863debb9de16126e#1639096655413196...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/396e72244af54b41b4b5e2f746be8b7f#1639097372269269...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/42b94a7361c341418f68bc6ccea1498c#1639096654695699...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/4341eb27432645b0a469dbdfe51f9f59#1639097160846108...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/48c071597a5e4c85a641dc98964deb45#1639097155765203...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/4ec962579ec4464d939ae2f4ef7c8d80#1639096665712348...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/64e1eb3509ce4c159dde7cb40f24af33#1639096664946691...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/6570de62f54946b3a15c0e632b83d73d#1639097156618536...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/7a88b9bce41c47d48e4dce64b5bb4468#1639097372933641...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/7d056a3cc9c54724b7f44050cc3ec886#1639096653237215...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/7ddd25351c7a44159f848bb027161224#1639096651874531...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/811efd1018fe4f2fbfe4f02dba8d9c7c#1639097389531982...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/86719537ab264e968544223634904621#1639096664327403...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/8dd3c2a68a43461994d0fa87ba89554d#1639097157915519...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/973f6ecfc398424c95162e470d66e7a3#1639097160002990...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/9ef2fd12830e47de955540508b6a7af8#1639096656595079...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/b95b298ed8e04e31a2023ee3f0b3ac97#1639097376057360...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/bd7a3a22c8c949cba3c36be0ccaf837a#1639097375413675...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/cb3e6dfa17b84563bf82d8197e821edd#1639097392066177...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/e966bc9bc322481d88b7e5eb2933c468#1639096661785992...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/f047f5f76c394c72ba60a53dd446ef0f#1639097390631756...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/f3048ef76c2e4125be197249e4ae43f6#1639097387654670...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/f3ba14618b2540779e6900b8bf088220#1639097157267424...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/f3e7fc9b16ca4b3bbb3e31ed62054457#1639097377176489...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/f678257bcfea4620b18bbbd0e21782a1#1639096662453154...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/analyzer_temporary_assets/feec2f83b0b144f487e7d42968776697#1639096663072849...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/#1639097162871659...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/assets/#1639097166289483...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/assets/0985774f0709428aba8cb4a820c702bd#1639097170034274...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/assets/25d8a47f160642609a27026b12f637f1#1639097170575930...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/assets/4341eb27432645b0a469dbdfe51f9f59#1639097171658256...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/assets/48c071597a5e4c85a641dc98964deb45#1639097166902550...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/assets/6570de62f54946b3a15c0e632b83d73d#1639097167557475...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/assets/8dd3c2a68a43461994d0fa87ba89554d#1639097169478052...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/assets/973f6ecfc398424c95162e470d66e7a3#1639097171130858...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/assets/f3ba14618b2540779e6900b8bf088220#1639097168934722...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/saved_model.pb#1639097172060678...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/variables/#1639097163033741...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/variables/variables.data-00000-of-00001#1639097165001080...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/b681712db0ff470cad09e2dd2f0e4590/variables/variables.index#1639097165273308...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/dropoff_grid#1639097358209995...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/#1639097378789977...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/.tft_metadata/#1639097392899452...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/.tft_metadata/schema.pbtxt#1639097393313635...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/assets/#1639097382203254...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/assets/dropoff_grid#1639097382817503...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/assets/loc_cross#1639097383604384...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/assets/payment_type#1639097384110231...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/assets/pickup_grid#1639097384666938...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/assets/trip_day#1639097385182395...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/assets/trip_day_of_week#1639097385739254...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/assets/trip_hour#1639097386267617...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/assets/trip_month#1639097386790540...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/saved_model.pb#1639097387146974...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/variables/#1639097378968133...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/variables/variables.data-00000-of-00001#1639097380471022...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/f38031c1423043aabb226e5c0eeb7655/variables/variables.index#1639097380648922...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/loc_cross#1639097329899963...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/payment_type#1639097341077650...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/pickup_grid#1639097336614247...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/trip_day#1639097326492607...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/trip_day_of_week#1639097356762849...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/trip_hour#1639097345781901...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/data_preprocess/tftransform_tmp/trip_month#1639097361930684...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/#1639148225049066...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/#1639148225355361...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210145448/#1639148225647698...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210145448/build-model_5073036699395686400/#1639148226110495...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210145448/build-model_5073036699395686400/executor_output.json#1639148226707642...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210173005/#1639157529432300...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210173005/build-model_3116222661303205888/#1639157529824482...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210173005/build-model_3116222661303205888/executor_output.json#1639157530404986...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210173005/model-upload_-6107149375551569920/#1639157621616794...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210174050/#1639158187729286...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210174050/model-upload_7259534318484062208/#1639158188060290...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210180434/#1639159545079206...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210180434/model-upload_7638258899648249856/#1639159545473359...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210181728/#1639160263413384...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210181728/model-upload_6773567771193114624/#1639160263777931...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210181728/model-upload_6773567771193114624/executor_output.json#1639160286636366...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/pipeline_root/model_build/759209241365/build-model-20211210181728/model-upload_6773567771193114624/gcp_resources#1639160265305559...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/#1639097394047786...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/#1639097408322373...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/assets/#1639097408439436...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/assets/dropoff_grid#1639097408561772...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/assets/loc_cross#1639097408676497...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/assets/payment_type#1639097408803405...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/assets/pickup_grid#1639097408958643...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/assets/trip_day#1639097409104615...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/assets/trip_day_of_week#1639097409243362...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/assets/trip_hour#1639097409361618...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/assets/trip_month#1639097409510537...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/saved_model.pb#1639097409668286...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/variables/#1639097409811166...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/variables/variables.data-00000-of-00001#1639097409923149...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transform_fn/variables/variables.index#1639097410114632...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transformed_metadata/#1639097407785094...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transformed_metadata/asset_map#1639097407925740...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_artifacts/transformed_metadata/schema.pbtxt#1639097408096217...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/test/data-00000-of-00004.gz#1639097491338566...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/test/data-00001-of-00004.gz#1639097491307791...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/test/data-00002-of-00004.gz#1639097491296463...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/test/data-00003-of-00004.gz#1639097491299564...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/train/data-00000-of-00003.gz#1639097486653579...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/train/data-00001-of-00003.gz#1639097486641788...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/train/data-00002-of-00003.gz#1639097486630273...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/val/data-00000-of-00004.gz#1639097488153672...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/val/data-00001-of-00004.gz#1639097488167976...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/val/data-00002-of-00004.gz#1639097488160519...\n",
      "Removing gs://andy-1234-221921aip-20211210003020/transformed_data/val/data-00003-of-00004.gz#1639097488157527...\n",
      "/ [165 objects]  13.27 objects/s                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "\n",
      "Operation completed over 165 objects.                                            \n",
      "Removing gs://andy-1234-221921aip-20211210003020/...\n"
     ]
    }
   ],
   "source": [
    "! gsutil rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vars": {
   "AIP": "AI Platform",
   "AUTOML": "AutoML",
   "AUTOML_URL": "https://cloud.google.com/vertex-ai/docs/start/automl-users",
   "BQ": "BigQuery",
   "COLAB": "Colab",
   "DATASET_ALIAS": "chicago",
   "DATASET_NAME": "Chicago Taxi",
   "DIRECT": "False",
   "DOCKER": "Docker",
   "EDGE": "Edge",
   "EMAIL": "cdpe@gmail.com",
   "FEATURE_COLUMNS": "{'id': 'dataset:bq,chicago,lbn', 'parameters': [{'DATASET_ALIAS': 'chicago', 'DATASET_NAME': 'Chicago Taxi', 'FEATURE_COLUMNS': ['trip_month', 'trip_day', 'trip_day_of_week', 'trip_hour', 'trip_seconds', 'trip_miles', 'payment_type', 'pickup_grid', 'dropoff_grid', 'euclidean', 'loc_cross'], 'LABEL_COLUMN': 'tip_bin'}], 'type': 'text'}",
   "GAPIC": "client library",
   "GAPICP": "SDK",
   "GCONSOLE": "Cloud Console",
   "GCP": "Google Cloud",
   "GCS": "Cloud Storage",
   "GNOTEBOOK": "Google Cloud Notebook",
   "LABEL_COLUMN": "tip_bin",
   "MODEL_TYPE": "tabular binary classification",
   "MODEL_TYPE_ABBR": "lbn",
   "NGPU": "1",
   "NOTEBOOK": "ml_ops_stage3/mlops_formalization.ipynb",
   "REGION": "us-central1",
   "REPO": "vertex-ai-samples/tree/master/notebooks/official/automl",
   "SDKP": "SDK for Python",
   "STAGE": "3 : formalization",
   "TENSORFLOW": "TensorFlow",
   "TFLite": "TFLite",
   "TFServing": "TF Serving",
   "TFV": "2.5",
   "TITLE": "E2E ML on GCP: MLOps stage 3 : formalization",
   "TRAINING": "training",
   "VERTEX": "Vertex AI",
   "VERTEX_AI": "Vertex AI",
   "YEAR": "2021",
   "null": "null",
   "uCAIP": "Vertex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
