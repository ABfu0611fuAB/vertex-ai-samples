{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "# E2E ML on GCP: MLOps stage 3 : formalization: get started with BigQuery and TFDV pipeline components\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_bq_tfdv_pipeline_components.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/ai/platform/notebooks/deploy-notebook?download_url=https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/ml_ops/stage3/get_started_with_bq_tfdv_pipeline_components.ipynb\">\n",
    "      Open in Google Cloud Notebooks\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI for E2E MLOps on Google Cloud in production. This tutorial covers stage 3 : formalization: get started with BigQuery and TFDV pipeline components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:gsod,lrg",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the GSOD dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). The version of the dataset you use only the fields year, month and day to predict the value of mean daily temperature (mean_temp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage3,get_started_bq_tfdv_pipeline_components",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use build lightweight Python components for BigQuery and Tensorflow Data Validation.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `Vertex AI Pipelines`\n",
    "- `Vertex AI Datasets`\n",
    "- `BigQuery`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Build and execute a pipeline component for creating a Vertex AI Tabular Dataset from a BigQuery table.\n",
    "- Build and execute a pipeline component for generating TFDV statistics and schema from a Vertex AI Tabular Dataset.\n",
    "- Execute a Vertex AI pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install *one time* the packages for executing the MLOps notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_mlops",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "ONCE_ONLY = False\n",
    "if ONCE_ONLY:\n",
    "    ! pip3 install -U tensorflow==2.5 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-data-validation==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-transform==1.2 $USER_FLAG\n",
    "    ! pip3 install -U tensorflow-io==0.18 $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-aiplatform[tensorboard] $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-pipeline-components $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-bigquery $USER_FLAG\n",
    "    ! pip3 install --upgrade google-cloud-logging $USER_FLAG\n",
    "    ! pip3 install --upgrade apache-beam[gcp] $USER_FLAG\n",
    "    ! pip3 install --upgrade pyarrow $USER_FLAG\n",
    "    ! pip3 install --upgrade cloudml-hypertune $USER_FLAG\n",
    "    ! pip3 install --upgrade kfp $USER_FLAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "restart",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_gcloud_project_id",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "region",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "timestamp",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "validate_bucket",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "**If you don't know your service account**, try to get your service account using `gcloud` command by executing the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "autoset_service_account",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "if SERVICE_ACCOUNT == \"\" or SERVICE_ACCOUNT is None or SERVICE_ACCOUNT == \"[your-service-account]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud auth list 2>/dev/null\n",
    "    SERVICE_ACCOUNT = shell_output[2].strip()\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account:pipelines",
    "repo": "snippets_pipelines.ipynb"
   },
   "source": [
    "#### Set service account access for Vertex AI Pipelines\n",
    "\n",
    "Run the following commands to grant your service account access to read and write pipeline artifacts in the bucket that you created in the previous step -- you only need to run these once per service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_service_account:pipelines",
    "repo": "snippets_pipelines.ipynb"
   },
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_NAME\n",
    "\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Set up variables\n",
    "\n",
    "Next, set up some variables used throughout the tutorial.\n",
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_tf",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "#### Import TensorFlow\n",
    "\n",
    "Import the TensorFlow package into your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_tf",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_kfp:namedtuple",
    "repo": "snippets_pipelines.ipynb"
   },
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "source": [
    "### Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "init_aip:mbsdk",
    "repo": "snippets_housekeeping.ipynb"
   },
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq_dataflow_components_intro",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "## Pipeline components with BigQuery and Dataflow\n",
    "\n",
    "### An anatomy of a pipeline component\n",
    "\n",
    "Let's dive a bit into how pipeline components are executed. First, each component is containerized. That is, each component has its own:\n",
    "\n",
    "- container image\n",
    "- installation requirements\n",
    "- (optional) hardware requirements\n",
    "\n",
    "The above affects the amount of time/resources required to provision the component. For example, if each component in the pipeline had a different machine requirement, then a machine would have to be provisioned for each component. Even if the machine type is the same, if each component had a different container image, then a new container image would have to be provisioned for each component.\n",
    "\n",
    "In otherwords, the efficiency of the pipeline is affected by the amount of provisioning.\n",
    "\n",
    "Additionally, since each component runs in a container with its own memory space, there are performance issues relating to the amount of data moved across the container boundaries -- i.e., marshalling. To marshall data, the data has to be serialized and written to a volume storage, where the next component can access and de-serialize the data. Simple data types like integers, floats, strings, small dictionaries can be efficiently marshalled. You want to avoid though marshalling large memory objects.\n",
    "\n",
    "### Construction of data pipeline components\n",
    "\n",
    "Both BigQuery and Dataflow deal with data, and more importantly large amounts of data. As a result, you need to carefully consider the construction of the pipeline, so that you are not marshalling large amounts of in-memory data.\n",
    "\n",
    "For example, consider a task that consists of reading a million records into an in-memory pandas dataframe, and then the in-memory data is processed for statistics. You could write this as two components: one component creates the dataframe, and the other processes it. Sounds good, nice and modular and the first component is likely reusable. Bad choice though.\n",
    "\n",
    "If you did construct the components this way, the first component would have to write the dataframe to a disk, and the second component would then have to read it back from disk. Very inefficient. If you need a large in-memory object, one should only create it in the same component where it is used. In this example, one would create a single component to create and process the dataframe.\n",
    "\n",
    "Let's now consider Vertex AI resources like datasets, models and endpoints. These resources have a physical manifestation which may include a combination of data and binary files. The Vertex AI resource object is not the actual files, but a in-memory wrapper. The resource object consists of properties and method, and file data is not read into memory until a property/method needs it.\n",
    "\n",
    "Thus, for efficiency purposes, Vertex AI was designed with reference identifiers. One can load these resource wrappers via the resource identifier. Thus, when creating or otherwise referencing resource objects between components, one passes the resource identifier(s) between components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "import_file:u_dataset,bq",
    "repo": "snippets_common.ipynb"
   },
   "source": [
    "#### Location of BigQuery training data.\n",
    "\n",
    "Now set the variable `IMPORT_FILE` to the location of the data table in BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "import_file:gsod,bq,lrg",
    "repo": "snippets_common.ipynb"
   },
   "outputs": [],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.samples.gsod\"\n",
    "BQ_TABLE = 'bigquery-public-data.samples.gsod'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_dataset_component:bq",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### BigQuery components\n",
    "\n",
    "First, you build a component `create_dataset_bq` to create a Vertex AI dataset from a BigQuery table. The component will return the resource identifier for the created Vertex AI dataset. Next, you build two downstream components:\n",
    "\n",
    "    - `get_dataset_source`: Using the returned resource identifier, load the dataset resource object and get/return the dataset input source.\n",
    "    - `get_column_names`: Using the returned resource identifier, load the dataset resource object and get/return the dataset column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_dataset_component:bq",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def create_dataset_bq(bq_table: str,\n",
    "                      display_name: str,\n",
    "                      project: str) -> str:\n",
    "    import google.cloud.aiplatform as aip\n",
    "\n",
    "    dataset = aip.TabularDataset.create(\n",
    "        display_name=display_name,\n",
    "        bq_source='bq://' + bq_table,\n",
    "        project=project\n",
    "    )\n",
    "\n",
    "    return dataset.resource_name\n",
    "\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def get_dataset_source(dataset_id: str) -> str:\n",
    "    import google.cloud.aiplatform as aip\n",
    "\n",
    "    dataset =  aip.TabularDataset(dataset_id)\n",
    "    if \"gcsSource\" in dataset.gca_resource.metadata['inputConfig'].keys():\n",
    "        files = dataset.gca_resource.metadata['inputConfig']['gcsSource']['uri']\n",
    "        return list(files)\n",
    "    else:\n",
    "        bq = dataset.gca_resource.metadata['inputConfig']['bigquerySource']['uri']\n",
    "        return bq\n",
    "\n",
    "@component(packages_to_install=[\"google-cloud-aiplatform\"])\n",
    "def get_column_names(dataset_id: str) -> list:\n",
    "    import google.cloud.aiplatform as aip\n",
    "\n",
    "    dataset =  aip.TabularDataset(dataset_id)\n",
    "    return dataset.column_names\n",
    "\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/dataset_bq\".format(BUCKET_NAME)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"dataset-bq\",\n",
    "    description=\"Vertex Dataset from BQ Table\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(bq_table: str = BQ_TABLE,\n",
    "             display_name: str = 'example',\n",
    "             project: str = PROJECT_ID\n",
    "            ):\n",
    "    create_op = create_dataset_bq(bq_table, display_name, project)\n",
    "\n",
    "    source_op = get_dataset_source(create_op.output)\n",
    "\n",
    "    column_names_op = get_column_names(create_op.output)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"dataset_bq.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"dataset_bq\",\n",
    "    template_path=\"dataset_bq.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "! rm dataset_bq.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_results:dataset_bq",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### View the pipeline execution results\n",
    "\n",
    "Next, view the results -- i.e., artifacts that are passed by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_pipeline_results:dataset_bq",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "PROJECT_NUMBER = pipeline.gca_resource.name.split('/')[1]\n",
    "print(PROJECT_NUMBER)\n",
    "\n",
    "def print_pipeline_output(job, output_task_name):\n",
    "    JOB_ID = job.name\n",
    "    print(JOB_ID)\n",
    "    for _ in range(len(job.gca_resource.job_detail.task_details)):\n",
    "        TASK_ID = job.gca_resource.job_detail.task_details[_].task_id\n",
    "        EXECUTE_OUTPUT = PIPELINE_ROOT + '/' + PROJECT_NUMBER + '/' + JOB_ID + '/' + output_task_name + '_' + str(TASK_ID) + '/executor_output.json'\n",
    "        GCP_RESOURCES = PIPELINE_ROOT + '/' + PROJECT_NUMBER + '/' + JOB_ID + '/' + output_task_name + '_' + str(TASK_ID) + '/gcp_resources'\n",
    "        if tf.io.gfile.exists(EXECUTE_OUTPUT):\n",
    "            ! gsutil cat $EXECUTE_OUTPUT\n",
    "            break\n",
    "        elif tf.io.gfile.exists(GCP_RESOURCES):\n",
    "            ! gsutil cat $GCP_RESOURCES\n",
    "            break\n",
    "\n",
    "    return EXECUTE_OUTPUT\n",
    "\n",
    "print(\"create_dataset_bq\")\n",
    "artifacts = print_pipeline_output(pipeline, 'create-dataset-bq')\n",
    "output = !gsutil cat $artifacts\n",
    "val = json.loads(output[0])\n",
    "dataset_id = val['parameters']['Output']['stringValue']\n",
    "print('\\n\\n')\n",
    "\n",
    "print(\"get_dataset_source\")\n",
    "artifacts = print_pipeline_output(pipeline, 'get-dataset-source')\n",
    "print('\\n\\n')\n",
    "\n",
    "print(\"get_column_names\")\n",
    "artifacts = print_pipeline_output(pipeline, 'get-column-names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_statistics_pipeline",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### Build TFDV component for dataset statistics\n",
    "\n",
    "Next, you build a component that will use the Tensorflow Data Validation package to produce dataset statistics and schema from the Vertex AI dataset you created, with the following parameters:\n",
    "\n",
    "- `dataset_id`: The resource ID of the Vertex AI dataset.\n",
    "- `label`: The label column for the dataset.\n",
    "- `bucket`: The bucket to write the statistics and schema data\n",
    "\n",
    "The statistics and schema are large memory objects that may be reused downstream by other components. For this purpose, the component directly writes the data to a Cloud Storage bucket, and then returns the Cloud Storage locations of the statistics and schema file as output artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_statistics_pipeline",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-bigquery\", \"tensorflow-data-validation==1.2\", \"tensorflow==2.5\"])\n",
    "def statistics(dataset_id: str,\n",
    "               label: str,\n",
    "               bucket: str) -> NamedTuple(\n",
    "                    \"Outputs\",\n",
    "                    [\n",
    "                        (\"stats\", str),  # Return parameters\n",
    "                        (\"schema\", str)\n",
    "                    ]):\n",
    "    import google.cloud.aiplatform as aip\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    dataset =  aip.TabularDataset(dataset_id)\n",
    "    if \"gcsSource\" in dataset.gca_resource.metadata['inputConfig'].keys():\n",
    "        files = dataset.gca_resource.metadata['inputConfig']['gcsSource']['uri']\n",
    "        files = list(files)\n",
    "        stats = tfdv.generate_statistics_from_csv(\n",
    "            data_location=files[0],\n",
    "            stats_options=tfdv.StatsOptions(\n",
    "                label_feature=label,\n",
    "                num_top_values=50\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        bq = dataset.gca_resource.metadata['inputConfig']['bigquerySource']['uri']\n",
    "        bq_table = bq[5:]\n",
    "        table = bigquery.TableReference.from_string(\n",
    "            bq_table\n",
    "        )\n",
    "        bqclient = bigquery.Client()\n",
    "        rows = bqclient.list_rows(\n",
    "            table,\n",
    "            selected_fields = [\n",
    "                            bigquery.SchemaField(\"station_number\", \"STRING\"),\n",
    "                            bigquery.SchemaField(\"year\", \"INTEGER\"),\n",
    "                            bigquery.SchemaField(\"month\", \"INTEGER\"),\n",
    "                            bigquery.SchemaField(\"day\", \"INTEGER\"),\n",
    "                            bigquery.SchemaField(\"mean_temp\", \"FLOAT\"),\n",
    "                        ],\n",
    "            max_results=10000\n",
    "\n",
    "        )\n",
    "        dataframe = rows.to_dataframe()\n",
    "        stats = tfdv.generate_statistics_from_dataframe(\n",
    "                dataframe=dataframe,\n",
    "                stats_options=tfdv.StatsOptions(\n",
    "                    label_feature=label,\n",
    "                    num_top_values=50\n",
    "                )\n",
    "            )\n",
    "\n",
    "    stats_file = bucket + '/stats.txt'\n",
    "    tfdv.write_stats_text(output_path=stats_file, stats=stats)\n",
    "\n",
    "    schema = tfdv.infer_schema(statistics=stats)\n",
    "\n",
    "    schema_file = bucket + '/schema.txt'\n",
    "    tfdv.write_schema_text(output_path=schema_file, schema=schema)\n",
    "\n",
    "    return (stats_file, schema_file)\n",
    "\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/dataset_stats\".format(BUCKET_NAME)\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"dataset-stats\",\n",
    "    description=\"Dataset statistics\",\n",
    "    pipeline_root=PIPELINE_ROOT\n",
    ")\n",
    "def pipeline(dataset_id: str,\n",
    "             label: str,\n",
    "             bucket: str):\n",
    "\n",
    "    stats_op = statistics(dataset_id, label, bucket)\n",
    "\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"dataset_stats.json\"\n",
    ")\n",
    "\n",
    "pipeline = aip.PipelineJob(\n",
    "    display_name=\"dataset_stats\",\n",
    "    template_path=\"dataset_stats.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values = { 'dataset_id': dataset_id,\n",
    "                         'label': 'mean_temp',\n",
    "                         'bucket': BUCKET_NAME\n",
    "                       }\n",
    ")\n",
    "\n",
    "pipeline.run()\n",
    "\n",
    "!rm -f dataset_stats.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view_pipeline_results:statistics",
    "repo": "snippets_mlops.ipynb"
   },
   "source": [
    "### View the pipeline execution results\n",
    "\n",
    "Next, view the results -- i.e., the location of the statistics and schema artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_pipeline_results:statistics",
    "repo": "snippets_mlops.ipynb"
   },
   "outputs": [],
   "source": [
    "artifacts = print_pipeline_output(pipeline, 'statistics')\n",
    "output = !gsutil cat $artifacts\n",
    "val = json.loads(output[0])\n",
    "schema_location = val['parameters']['schema']['stringValue']\n",
    "stats_location = val['parameters']['stats']['stringValue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "### Delete a pipeline job\n",
    "\n",
    "After a pipeline job is completed, you can delete the pipeline job with the method `delete()`.  Prior to completion, a pipeline job can be canceled with the method `cancel()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "delete_pipeline",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "pipeline.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "- Dataset\n",
    "- Pipeline\n",
    "- Model\n",
    "- Endpoint\n",
    "- AutoML Training Job\n",
    "- Batch Job\n",
    "- Custom Job\n",
    "- Hyperparameter Tuning Job\n",
    "- Cloud Storage Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cleanup:mbsdk",
    "repo": "snippets_mbsdk.ipynb"
   },
   "outputs": [],
   "source": [
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if 'dataset' in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if 'model' in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if 'endpoint' in globals():\n",
    "            endpoint.undeploy_all()\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline training job\n",
    "    try:\n",
    "        if 'dag' in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom training job\n",
    "    try:\n",
    "        if 'job' in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if 'batch_predict_job' in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if 'hpt_job' in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if 'BUCKET_NAME' in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vars": {
   "AIP": "AI Platform",
   "AUTOML": "AutoML",
   "AUTOML_URL": "https://cloud.google.com/vertex-ai/docs/start/automl-users",
   "BQ": "BigQuery",
   "COLAB": "Colab",
   "DATASET_ALIAS": "gsod",
   "DATASET_NAME": "NOAA historical weather data",
   "DOCKER": "Docker",
   "EDGE": "Edge",
   "EMAIL": "cdpe@gmail.com",
   "FEATURE_COLUMNS": "station_number,year,month,day",
   "GAPIC": "client library",
   "GAPICP": "SDK",
   "GCONSOLE": "Cloud Console",
   "GCP": "Google Cloud",
   "GCS": "Cloud Storage",
   "GNOTEBOOK": "Google Cloud Notebook",
   "LABEL_COLUMN": "mean_temp",
   "NOTEBOOK": "stage3/get_started_with_bq_tfdv_pipeline_components.ipynb",
   "REGION": "us-central1",
   "REPO": "vertex-ai-samples/blob/main/notebooks/community/ml_ops",
   "SDKP": "SDK for Python",
   "STAGE": "3 : formalization: get started with BigQuery and TFDV pipeline components",
   "TENSORFLOW": "TensorFlow",
   "TFLite": "TFLite",
   "TFServing": "TF Serving",
   "TIME_COLUMN": "date",
   "TIME_SERIES_ID_COLUMN": "county",
   "TITLE": "E2E ML on GCP: MLOps stage 3 : formalization: get started with BigQuery and TFDV pipeline components",
   "VERTEX": "Vertex AI",
   "VERTEX_AI": "Vertex AI",
   "YEAR": "2021",
   "null": "null",
   "uCAIP": "Vertex"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
